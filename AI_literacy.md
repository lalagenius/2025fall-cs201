# äººå·¥æ™ºèƒ½

Updated 2253 GMT+8 Jul 27 2025

2025 summer, Complied by Hongfei Yan



https://github.com/GMyhf/2025fall-cs201/





# ç¬¬ä¸€éƒ¨åˆ† äººå·¥æ™ºèƒ½åŸç†æ¦‚è¿°

## 1.1 äººå·¥æ™ºèƒ½çš„æ¦‚å¿µä¸å‘å±•å†å²

### ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ

äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯ä½¿è®¡ç®—æœºç³»ç»Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æŠ€æœ¯ã€‚å¸¸è§çš„äººå·¥æ™ºèƒ½åº”ç”¨åŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€æ¨èç³»ç»Ÿç­‰ã€‚

### å›¾çµæµ‹è¯•

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250727133958507.png" alt="image-20250727133958507" style="zoom:50%;" />

è‰¾ä¼¦Â·å›¾çµäº1950å¹´æå‡ºäº†â€œå›¾çµæµ‹è¯•â€ä½œä¸ºåˆ¤æ–­æœºå™¨æ˜¯å¦å…·æœ‰æ™ºèƒ½çš„æ–¹æ³•ã€‚åœ¨å›¾çµæµ‹è¯•ä¸­ï¼Œæµ‹è¯•è€…é€šè¿‡ä¸æœºå™¨å’Œå¦ä¸€ä¸ªäººè¿›è¡Œå¯¹è¯ï¼Œå¦‚æœæ— æ³•åŒºåˆ†æœºå™¨å’Œäººç±»çš„å›ç­”ï¼Œæœºå™¨åˆ™è¢«è®¤ä¸ºå…·æœ‰äººç±»èˆ¬çš„æ™ºèƒ½ã€‚

```text
å›¾çµæµ‹è¯•åŸç†ï¼šå¦‚æœæœºå™¨ä¸äººç±»å¯¹è¯æ— æ³•åŒºåˆ†ï¼Œåˆ™æœºå™¨å…·æœ‰æ™ºèƒ½ã€‚
```

### è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ä¸äººå·¥æ™ºèƒ½çš„è¯ç”Ÿ

1956å¹´ï¼Œè¾¾ç‰¹èŒ…æ–¯ä¼šè®®æ­£å¼æå‡ºå¹¶ç¡®å®šäº†â€œäººå·¥æ™ºèƒ½â€è¿™ä¸€æœ¯è¯­ï¼Œå¹¶æ˜ç¡®äº†AIç ”ç©¶çš„ç›®æ ‡ï¼šå¼€å‘èƒ½å¤Ÿæ¨¡ä»¿äººç±»å­¦ä¹ ã€æ¨ç†å’Œè§£å†³é—®é¢˜èƒ½åŠ›çš„è®¡ç®—æœºç¨‹åºã€‚è¿™æ ‡å¿—ç€äººå·¥æ™ºèƒ½å­¦ç§‘çš„è¯ç”Ÿã€‚

**å‘å±•å†ç¨‹**

1. **1950s-1960s**ï¼šäººå·¥æ™ºèƒ½çš„æ—©æœŸæ¢ç´¢ï¼Œé‡ç‚¹æ˜¯åŸºäºè§„åˆ™çš„æ™ºèƒ½ç³»ç»Ÿã€‚
2. **1970s-1980s**ï¼šç¬¦å·ä¸»ä¹‰æµæ´¾çš„å…´èµ·ï¼Œä¸“å®¶ç³»ç»Ÿå’Œæ¨ç†æ–¹æ³•å¾—åˆ°åº”ç”¨ã€‚
3. **1990sè‡³ä»Š**ï¼šè¿æ¥ä¸»ä¹‰æµæ´¾çš„å´›èµ·ï¼Œæ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œçš„çªç ´ã€‚



## 1.2 äººå·¥æ™ºèƒ½çš„ä¸‰å¤§å­¦æ´¾

äººå·¥æ™ºèƒ½çš„ç ”ç©¶æœ‰å¤šä¸ªæµæ´¾ï¼Œå…¶ä¸­ä¸‰å¤§å­¦æ´¾åŒ…æ‹¬ç¬¦å·ä¸»ä¹‰ï¼ˆSymbolismï¼‰ã€è¿æ¥ä¸»ä¹‰ï¼ˆConnectionismï¼‰ å’Œ è¡Œä¸ºä¸»ä¹‰ï¼ˆBehaviorismï¼‰ã€‚



> https://zhuanlan.zhihu.com/p/443257981

### 1. ç¬¦å·ä¸»ä¹‰ï¼ˆSymbolism / Symbolic AIï¼‰

- **æ ¸å¿ƒè§‚ç‚¹**ï¼šæ™ºèƒ½æºäºå¯¹çŸ¥è¯†çš„ç¬¦å·è¡¨ç¤ºå’Œæ“ä½œï¼ˆè§„åˆ™æ¨ç†ï¼‰ã€‚

- **ä»£è¡¨æ–¹æ³•**ï¼šä¸“å®¶ç³»ç»Ÿã€æœç´¢ç®—æ³•ã€é€»è¾‘æ¨ç†ï¼ˆå¦‚ä¸€é˜¶é€»è¾‘ï¼‰ã€è§„åˆ’ç³»ç»Ÿã€‚

- **ä»£è¡¨äººç‰©**ï¼šè‰¾ä¼¦Â·çº½å„å°”ï¼ˆAllen Newellï¼‰ã€èµ«ä¼¯ç‰¹Â·è¥¿è’™ï¼ˆHerbert Simonï¼‰ã€çº¦ç¿°Â·éº¦å¡é”¡ï¼ˆJohn McCarthyï¼‰ç­‰ã€‚

  > çº¦ç¿°Â·éº¦å¡é”¡ï¼šæå‡ºäº†LISPç¼–ç¨‹è¯­è¨€ï¼ˆåæ¥æˆä¸ºäººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸»è¦è¯­è¨€ï¼‰å¹¶å€¡å¯¼ç¬¦å·ä¸»ä¹‰AIã€‚

  >  ç¬¦å·ä¸»ä¹‰ä¹Ÿç§°ä¸ºâ€œå¼ºäººå·¥æ™ºèƒ½â€çš„æ—©æœŸä»£è¡¨ï¼Œå¼ºè°ƒå½¢å¼åŒ–çŸ¥è¯†å’Œæ¨ç†ã€‚

**ä¼˜ç‚¹**ï¼š

- é«˜å¯è§£é‡Šæ€§ï¼šç¬¦å·ä¸»ä¹‰ç³»ç»Ÿæœ‰æ˜ç¡®çš„è§„åˆ™ï¼Œæ˜“äºç†è§£ã€‚
- ä¸“ç”¨æ€§ï¼šé€‚ç”¨äºæ•°å­¦æ¨ç†ã€æ£‹ç±»æ¸¸æˆç­‰æ˜ç¡®è§„åˆ™çš„ä»»åŠ¡ã€‚

**ç¼ºç‚¹**ï¼š

- å­¦ä¹ èƒ½åŠ›å¼±ï¼šéš¾ä»¥å¤„ç†æ¨¡ç³Šä¿¡æ¯ï¼Œä¸”éœ€è¦å¤§é‡æ‰‹åŠ¨ç¼–å†™è§„åˆ™ã€‚

**ä»£è¡¨æ€§æˆæœ**

- **ä¸“å®¶ç³»ç»Ÿ**ï¼šå¦‚XCONç³»ç»Ÿï¼Œå¸®åŠ©DECå…¬å¸é…ç½®è®¡ç®—æœºç³»ç»Ÿã€‚

  > 20ä¸–çºª80å¹´ä»£å¡å†…åŸºæ¢…éš†å¤§å­¦ä¸ºæ•°å­—è®¾å¤‡å…¬å¸(DEC)è®¾è®¡çš„åä¸ºXCONçš„ä¸“å®¶ç³»ç»Ÿç”¨äºååŠ©é…ç½®DECå…¬å¸å¤æ‚çš„VAXè®¡ç®—æœºç³»ç»Ÿã€‚å®ƒèƒ½å¤Ÿæ ¹æ®å®¢æˆ·è®¢å•ä¸­çš„éœ€æ±‚ï¼ˆå¦‚å¤„ç†å™¨ã€å­˜å‚¨ã€å¤–å›´è®¾å¤‡ç­‰ï¼‰ï¼Œè‡ªåŠ¨ç”Ÿæˆç¡¬ä»¶ç»„è£…å’Œè¿æ¥æ–¹æ¡ˆï¼Œå¹¶ç¡®ä¿æ‰€æœ‰éƒ¨ä»¶çš„å…¼å®¹æ€§ã€‚åœ¨1986å¹´ä¹‹å‰ï¼Œå®ƒæ¯å¹´ä¸ºå…¬å¸çœä¸‹å››åƒä¸‡ç¾å…ƒï¼ŒXCONçš„æˆåŠŸæ ‡å¿—ç€ä¸“å®¶ç³»ç»Ÿåœ¨ä¼ä¸šå®é™…åº”ç”¨ä¸­çš„çªç ´ï¼Œå¯¹åæ¥çš„å•†ä¸šAIç³»ç»Ÿå¼€å‘æœ‰é‡è¦å½±å“ã€‚

- **æ·±è“ï¼ˆDeep Blueï¼‰**ï¼šIBMå¼€å‘çš„å›½é™…è±¡æ£‹AIï¼Œ1997å¹´æˆ˜èƒœå¡æ–¯å¸•ç½—å¤«ã€‚



### 2. è¿æ¥ä¸»ä¹‰ï¼ˆConnectionismï¼‰

è¿æ¥ä¸»ä¹‰å¼ºè°ƒ<mark>é€šè¿‡ç¥ç»ç½‘ç»œæ¨¡ä»¿äººè„‘çš„ç»“æ„ä¸åŠŸèƒ½</mark>ã€‚æ™ºèƒ½æ¥æºäºç¥ç»å…ƒä¹‹é—´çš„è¿æ¥ä¸ä¿¡å·ä¼ é€’ã€‚

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250727141054606.png" alt="image-20250727141054606" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250727141109558.png" alt="image-20250727141109558" style="zoom:50%;" />

- **æ ¸å¿ƒè§‚ç‚¹**ï¼šæ™ºèƒ½æ˜¯ç¥ç»å…ƒä¹‹é—´è¿æ¥å’Œæ¿€æ´»çš„ç»“æœï¼Œå¼ºè°ƒä»æ•°æ®ä¸­å­¦ä¹ ã€‚

- **ä»£è¡¨æ–¹æ³•**ï¼šäººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰ã€æ·±åº¦å­¦ä¹ ï¼ˆCNNã€RNNã€Transformerç­‰ï¼‰ã€‚

- **ä»£è¡¨äººç‰©**ï¼šæ°å¼—é‡ŒÂ·è¾›é¡¿ï¼ˆGeoffrey Hintonï¼‰ã€çº¦ç¿°Â·éœæ™®è²å°”å¾·ã€ä¼Šåˆ©äºšÂ·è‹èŒ¨åŸºå¼—ç­‰ã€‚Yann LeCunã€David Rumelhartã€‚

  > çº¦ç¿°Â·éœæ™®è²å°”å¾·ï¼ˆJohn Hopfieldï¼‰ï¼Œ2024å¹´ä¸Hintonå…±è·è¯ºè´å°”ç‰©ç†å­¦å¥–ã€‚1982å¹´å’Œ1984å¹´å‘è¡¨ä¸¤ç¯‡é‡è¦è®ºæ–‡ï¼Œæå‡ºæå‡ºHopfieldç½‘ç»œï¼Œä¸ºåæ¥æ·±åº¦å­¦ä¹ çš„å´›èµ·é“ºå¹³äº†é“è·¯ã€‚Hopfieldç½‘ç»œï¼Œè¢«å¹¿æ³›åº”ç”¨äºå›¾åƒä¿®å¤ã€æ¨¡å¼è¯†åˆ«ç­‰é¢†åŸŸã€‚

- **åº”ç”¨**ï¼šå›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚

- **ä»£è¡¨ç³»ç»Ÿ**ï¼šAlphaGoã€GPTã€æ·±åº¦è§†è§‰ç³»ç»Ÿç­‰ã€‚

  > æ˜¯è¿‘å¹´æ¥äººå·¥æ™ºèƒ½çªç ´ï¼ˆå°¤å…¶æ˜¯æ·±åº¦å­¦ä¹ ï¼‰çš„ä¸»å¯¼åŠ›é‡ï¼Œå¼ºè°ƒâ€œå¼±è§„åˆ™ï¼Œå¼ºæ•°æ®â€ã€‚

#### ä»£è¡¨æ€§æˆæœ

- **æ„ŸçŸ¥æœºï¼ˆ1958å¹´ï¼‰**ï¼šä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œèƒ½è¿›è¡ŒäºŒåˆ†ç±»ä»»åŠ¡ã€‚

- **åå‘ä¼ æ’­ç®—æ³•ï¼ˆ1986å¹´ï¼‰**ï¼šä½¿å¤šå±‚ç¥ç»ç½‘ç»œå¾—ä»¥æœ‰æ•ˆè®­ç»ƒï¼Œæˆä¸ºæ·±åº¦å­¦ä¹ çš„åŸºçŸ³ã€‚

  > Geoffrey Hintonã€David Rumelhart å’Œ Ronald Williams æå‡ºäº†åå‘ä¼ æ’­ç®—æ³•ï¼ˆBackpropagationï¼‰ã€‚ä½¿å¾—å¤šå±‚ç¥ç»ç½‘ç»œå¾—ä»¥æœ‰æ•ˆè®­ç»ƒã€‚æˆä¸ºæ·±åº¦å­¦ä¹ çš„åŸºçŸ³ä¹‹ä¸€ï¼Œæå¤§æ¨åŠ¨äº†ç¥ç»ç½‘ç»œçš„ç ”ç©¶ä¸åº”ç”¨ã€‚

#### ç¤ºä¾‹ï¼šç®€å•ç¥ç»ç½‘ç»œåˆ†ç±» Iris æ•°æ®é›†ï¼ˆPyTorchï¼‰

```python
import torch
import torch.nn as nn
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# æ•°æ®å‡†å¤‡
X, y = load_iris(return_X_y=True)
X = StandardScaler().fit_transform(X)
X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.long)
train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2)

# æ„å»ºç¥ç»ç½‘ç»œ
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(4, 10),
            nn.ReLU(),
            nn.Linear(10, 3)
        )
    def forward(self, x):
        return self.fc(x)

model = Net()
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# è®­ç»ƒ
for epoch in range(100):
    out = model(train_x)
    loss = loss_fn(out, train_y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# æµ‹è¯•
acc = (model(test_x).argmax(1) == test_y).float().mean()
print("Test Accuracy:", acc.item())
```



> /Users/hfyan/miniconda3/bin/python /Users/hfyan/Library/Application Support/JetBrains/PyCharm2024.1/scratches/scratch_1894.py 
> Test Accuracy: 0.9666666388511658



### 3. è¡Œä¸ºä¸»ä¹‰ï¼ˆBehaviorism / Actionism / Cyberneticsï¼‰

è¡Œä¸ºä¸»ä¹‰çš„æ ¸å¿ƒæ€æƒ³æ˜¯<mark>æ™ºèƒ½ä½“çš„è¡Œä¸ºæ˜¯å¯¹å¤–ç•Œåˆºæ¿€çš„ååº”</mark>ã€‚æ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’ï¼Œå¹¶æ ¹æ®åé¦ˆè°ƒæ•´è¡Œä¸ºï¼Œä»è€Œå®ç°è‡ªæˆ‘å­¦ä¹ ã€‚

- **æ ¸å¿ƒè§‚ç‚¹**ï¼šæ™ºèƒ½ä½“æ— éœ€å…·å¤‡å†…éƒ¨çŸ¥è¯†ç»“æ„ï¼Œåªéœ€é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ è¡Œä¸ºã€‚

- **ä»£è¡¨äººç‰©**ï¼šç½—å¾·å°¼Â·å¸ƒé²å…‹æ–¯ï¼ˆRodney Brooksï¼‰ã€æˆ´å¯†æ–¯Â·å“ˆè¨æ¯”æ–¯ç­‰ã€‚B.F. Skinnerï¼ˆå¿ƒç†å­¦ï¼‰ã€Norbert Wienerï¼ˆæ§åˆ¶è®ºï¼‰

  > ç½—å¾·å°¼Â·å¸ƒé²å…‹æ–¯(Rodney A. Brooks)ï¼Œæå‡ºäº†æ™ºèƒ½è¡Œä¸ºçš„â€œæ„ŸçŸ¥-åŠ¨ä½œâ€æ¨¡å¼ï¼Œå‘æ˜å…­è¶³è¡Œèµ°æœºå™¨äººã€‚è‹±å›½äººæˆ´å¯†æ–¯Â·å“ˆè¨æ¯”æ–¯ï¼ˆDemis Hassabisï¼‰çš„AlphaGoå‡»è´¥äººç±»ã€‚

- **åº”ç”¨**ï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆä¾‹å¦‚AlphaGoã€è‡ªåŠ¨é©¾é©¶ç­‰ï¼‰ã€‚

  > å¼ºè°ƒâ€œè¡ŒåŠ¨ä¼˜äºæ€è€ƒâ€ï¼Œå¯¹æœºå™¨äººå­¦ã€æ™ºèƒ½ä½“ç ”ç©¶æœ‰é‡è¦å½±å“ã€‚

#### å¼ºåŒ–å­¦ä¹ ç¤ºä¾‹

å¼ºåŒ–å­¦ä¹ æ¨¡å‹çš„æ ¸å¿ƒæ˜¯é€šè¿‡å¥–åŠ±å’Œæƒ©ç½šæ¥å¼•å¯¼æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚ä¼ªä»£ç å¦‚ä¸‹ï¼š

```python
# æ›´æ–°Qå€¼å‡½æ•°
Q[state][action] = Q[state][action] + alpha * (reward + gamma * max(Q[next_state]) - Q[state][action])
```



## 1.3 äººå·¥æ™ºèƒ½ä¸‰è¦ç´ ï¼šç®—æ³•ã€ç®—åŠ›ã€æ•°æ®

### 1. ç®—æ³•ï¼šäººå·¥æ™ºèƒ½çš„â€œçµé­‚â€

AIç®—æ³•æ ¹æ®ä»»åŠ¡ç±»å‹å’Œæ•°æ®ç‰¹ç‚¹åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š

- **ç›‘ç£å­¦ä¹ **ï¼ˆSupervised Learningï¼‰é€šè¿‡å¸¦æ ‡ç­¾çš„æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œè¿›è¡Œåˆ†ç±»æˆ–å›å½’ã€‚
- **æ— ç›‘ç£å­¦ä¹ **ï¼ˆUnsupervised Learningï¼‰ä»æ— æ ‡ç­¾æ•°æ®ä¸­æŒ–æ˜æ•°æ®ç»“æ„ï¼Œå¸¸ç”¨äºèšç±»ã€‚
- **å¼ºåŒ–å­¦ä¹ **ï¼šé€šè¿‡å¥–æƒ©æœºåˆ¶ä¼˜åŒ–è¡Œä¸ºç­–ç•¥ï¼Œå¸¸ç”¨äºæ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚

#### ç›‘ç£å­¦ä¹  vs æ— ç›‘ç£å­¦ä¹ 

```python
from sklearn.cluster import KMeans
from sklearn.linear_model import LogisticRegression

# ç›‘ç£å­¦ä¹ ï¼šåˆ†ç±»
clf = LogisticRegression().fit(train_x, train_y)

# æ— ç›‘ç£å­¦ä¹ ï¼šèšç±»
kmeans = KMeans(n_clusters=3).fit(X)
```



å®Œæ•´ç¤ºä¾‹ï¼šç›‘ç£å­¦ä¹  + æ— ç›‘ç£å­¦ä¹ ï¼ˆIris æ•°æ®é›†ï¼‰

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# 1. åŠ è½½æ•°æ®
iris = load_iris()
X = iris.data
y = iris.target

# 2. æ ‡å‡†åŒ–æ•°æ®
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆç”¨äºç›‘ç£å­¦ä¹ ï¼‰
train_x, test_x, train_y, test_y = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# 4. ç›‘ç£å­¦ä¹ ï¼šé€»è¾‘å›å½’åˆ†ç±»
clf = LogisticRegression(max_iter=200)
clf.fit(train_x, train_y)
pred = clf.predict(test_x)
print("Logistic Regression Accuracy:", accuracy_score(test_y, pred))

# 5. æ— ç›‘ç£å­¦ä¹ ï¼šKMeans èšç±»ï¼ˆèšæˆ3ç±»ï¼‰
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# 6. å¯è§†åŒ–èšç±»ï¼ˆé™ç»´åˆ°äºŒç»´ï¼‰
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_2d = pca.fit_transform(X_scaled)

plt.figure(figsize=(10, 5))

# èšç±»ç»“æœ
plt.subplot(1, 2, 1)
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=clusters, cmap='viridis', s=50)
plt.title("KMeans Clustering (unsupervised)")

# åŸå§‹æ ‡ç­¾
plt.subplot(1, 2, 2)
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='Set1', s=50)
plt.title("Ground Truth Labels (supervised)")

plt.show()

```

> ä½¿ç”¨ `LogisticRegression` è®­ç»ƒä¸€ä¸ªæœ‰ç›‘ç£åˆ†ç±»å™¨ï¼Œå¹¶è¾“å‡ºæµ‹è¯•é›†å‡†ç¡®ç‡ï¼›
>
> ä½¿ç”¨ `KMeans` è¿›è¡Œæ— ç›‘ç£èšç±»ï¼›
>
> ä½¿ç”¨ PCA å°† 4 ç»´æ•°æ®é™ç»´ä¸º 2 ç»´ï¼Œä»¥ä¾¿å¯è§†åŒ–èšç±»ç»“æœå’ŒçœŸå®æ ‡ç­¾ï¼›
>
> å¯æ¸…æ™°çœ‹åˆ°èšç±»ä¸çœŸå®åˆ†ç±»çš„å¯¹æ¯”æ•ˆæœã€‚



/Users/hfyan/miniconda3/bin/python /Users/hfyan/Library/Application Support/JetBrains/PyCharm2024.1/scratches/scratch_1895.py 
Logistic Regression Accuracy: 1.0



<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250727143806445.png" alt="image-20250727143806445" style="zoom: 67%;" />



### 2. ç®—åŠ›ï¼šäººå·¥æ™ºèƒ½çš„â€œå¼•æ“â€

ç®—åŠ›æ˜¯AIå‘å±•çš„å…³é”®å› ç´ ã€‚ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹éœ€è¦å¼ºå¤§çš„è®¡ç®—èµ„æºæ”¯æŒï¼Œå°¤å…¶æ˜¯GPUå’ŒTPUçš„åº”ç”¨å¤§å¹…åŠ é€Ÿäº†AIè®­ç»ƒè¿‡ç¨‹ã€‚

#### æµ‹è¯• GPU å’Œ MPSï¼ˆé€‚ç”¨äº Mac ç”¨æˆ·ï¼‰

```python
import torch
print("CUDA:", torch.cuda.is_available())  # æ£€æµ‹GPUæ˜¯å¦å¯ç”¨
print("MPS:", torch.backends.mps.is_available())  # æ£€æµ‹MPSæ˜¯å¦å¯ç”¨
```

### 3. æ•°æ®ï¼šäººå·¥æ™ºèƒ½çš„â€œç‡ƒæ–™â€

æ•°æ®æ˜¯AIæ¨¡å‹å­¦ä¹ çš„åŸºç¡€ã€‚è¶³å¤Ÿå¤§é‡ä¸”é«˜è´¨é‡çš„æ•°æ®å¯ä»¥æ¨åŠ¨AIç®—æ³•çš„æˆåŠŸåº”ç”¨ã€‚

- **æ•°æ®æ ‡æ³¨**ï¼šåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸï¼Œæ ‡æ³¨æ•°æ®ç”¨äºç›‘ç£å­¦ä¹ ã€‚

  > 2006å¹´æé£é£æå‡ºImageNetè®¡åˆ’ï¼šåˆ›å»ºæ¶µç›–æ•°åƒä¸ªç‰©ä½“ç±»åˆ«ï¼Œå¹¶ä¸”æ¯ä¸ªç±»åˆ«åŒ…å«è¶³å¤Ÿæ•°é‡çš„å›¾ç‰‡çš„æ•°æ®é›†ï¼Œä»¥ä¾¿è®­ç»ƒå’ŒéªŒè¯å¤æ‚çš„è®¡ç®—æœºè§†è§‰ç®—æ³•ã€‚
  > æ•°æ®æ¥æºï¼šé€šè¿‡äº’è”ç½‘æŠ“å–æ•°åƒä¸‡å¼ å›¾ç‰‡ã€‚
  > æ ‡ç­¾æ ‡æ³¨ï¼šåˆ©ç”¨ä¼—åŒ…å¹³å°ï¼Œé›‡ä½£æ•°åƒåæ ‡æ³¨å‘˜å¯¹æ•°æ®è¿›è¡Œæ ‡æ³¨ã€‚
  > åˆ†ç±»æ ‡å‡†ï¼šåŸºäº WordNet çš„è¯æ±‡å±‚æ¬¡ç»“æ„ï¼Œè®¾è®¡äº†æ•°æ®é›†çš„ç±»åˆ«å±‚çº§ã€‚
  > æœ€ç»ˆæˆæœï¼š2009å‘å¸ƒï¼ŒåŒ…å«1400ä¸‡å¼ æ ‡æ³¨çš„å›¾ç‰‡ã€‚

- **æ— æ ‡ç­¾æ•°æ®**ï¼šæ— ç›‘ç£å­¦ä¹ ä¾èµ–äºæ— æ ‡ç­¾æ•°æ®è¿›è¡Œæ¨¡å¼è¯†åˆ«ã€‚

  > æ— éœ€äººå·¥å¹²é¢„ï¼Œè‡ªç„¶äº§ç”Ÿï¼šé€šè¿‡ä¼ æ„Ÿå™¨ã€æ‘„åƒå¤´ç­‰å„ç§è®¾å¤‡è®°å½•ï¼Œå„ç§ç½‘ç«™å’Œåº”ç”¨ç¨‹åºçš„ç”¨æˆ·ç”Ÿæˆâ€¦â€¦..



## 1.4 äººå·¥æ™ºèƒ½åº”ç”¨å‰æ²¿

### 1. æ™ºèƒ½åšå¼ˆï¼šAlphaGo å’Œ AlphaZero

- **AlphaGo**ï¼šåˆ©ç”¨æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œã€å¼ºåŒ–å­¦ä¹ å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢æŠ€æœ¯ï¼ŒæˆåŠŸæˆ˜èƒœäº†å›´æ£‹é¡¶å°–é€‰æ‰‹æä¸–çŸ³ã€‚

  > DeepMindå…¬å¸çš„AlphaGo 2016å¹´4:1æˆ˜èƒœæä¸–çŸ³ï¼Œ2017å¹´3ï¼š0æˆ˜èƒœæŸ¯æ´
  >
  > AlphaGoä»¥æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸ºåŸºç¡€ï¼Œä½¿ç”¨æ·±åº¦å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ã€è’™ç‰¹å¡æ´›æ ‘æœç´¢æŠ€æœ¯ï¼Œä»¥äººç±»æ£‹è°±è¿›è¡Œç›‘ç£å­¦ä¹ ã€‚è’™ç‰¹å¡æ´›æ ‘æœç´¢: å‰å‡ æ­¥è·¯è¦ä»”ç»†é€‰æ‹©å¥½çš„ï¼Œå‰©ä¸‹çš„å°±éšä¾¿å¿«é€Ÿå¤šæ¬¡ä¹±è¯•ï¼Œå†åé¦ˆç»“æœã€‚å‚æ•°ä¸ªæ•°ç™¾ä¸‡çº§åˆ«ã€‚
  >

- **AlphaZero**ï¼šç»§AlphaGoåï¼Œè¿›ä¸€æ­¥å‘å±•ï¼ŒAlphaZeroå¯ä»¥ä»é›¶å¼€å§‹å­¦ä¹ ï¼Œè‡ªåŠ¨æŒæ¡ä¸åŒæ£‹ç±»çš„ç­–ç•¥ï¼Œä¸”ä¸ä¾èµ–äººç±»æ£‹è°±ã€‚

  > ç”±AlphaGoæ”¹è¿›åå¾—åˆ°æ›´å¼ºçš„AlphaGo Zero ï¼ŒAlpha Zero
  >
  > AlphaGo Zeroå‚æ•°2000ä¸‡
  >
  > AlphaGo Zeroä»å®Œå…¨éšæœºçš„æ£‹å±€å¼€å§‹ï¼Œé€šè¿‡åå¤çš„è‡ªå¯¹å¼ˆè¿›è¡Œå­¦ä¹ ã€‚ä¸éœ€è¦äººç±»æ£‹è°±ï¼Œè‡ªå·±é€ æ•°æ®è‡ªå·±å­¦ã€‚ç»è¿‡40å¤šå¤©å­¦ä¹ æˆ˜èƒœAlphaGoã€‚
  >
  > 2018å¹´çš„Alpha Zero å¹¶ä¸ä¾èµ–å…·ä½“æ£‹ç±»çš„è§„åˆ™ï¼Œå¯ä»¥ä»æ–°ç»™çš„è§„åˆ™ä¸­ï¼Œé€šè¿‡è‡ªæˆ‘å¯¹å¼ˆå­¦ä¹ å‡ºæœ€ä¼˜ç­–ç•¥ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒæ£‹ç±»ã€‚
  >
  > Alpha Zeroé€šè¿‡8å°æ—¶è‡ªå¯¹å¼ˆå­¦ä¹ ï¼Œæˆ˜èƒœAlphaGo Zero ,ç»è¿‡2hå­¦ä¹ ï¼Œæˆ˜èƒœé¡¶å°–å›½é™…è±¡æ£‹AI  Stockfish, ç”¨4hå­¦ä¹ ï¼Œæˆ˜èƒœé¡¶å°–æ—¥æœ¬å°†æ£‹AIã€‚
  >
  > Alpha Zeroæ­ç¤ºä¹Ÿè®¸åœ¨æœ‰äº›é¢†åŸŸæ”¾å¼ƒäººç±»ç»éªŒä¼šæœ‰æ›´å¥½ç»“æœ

#### AlphaZero ç¤ºä¾‹ï¼š

```text
é€šè¿‡è‡ªå¯¹å¼ˆå­¦ä¹ ï¼ŒAlphaZeroè¶…è¶Šäº†ä¼ ç»Ÿçš„æ£‹ç±»ç¨‹åºï¼ŒæŒæ¡äº†è‡ªæˆ‘ä¼˜åŒ–å’Œç­–ç•¥è°ƒæ•´èƒ½åŠ›ã€‚
```

### 2. è‡ªç„¶è¯­è¨€å¤„ç†ï¼šTransformer ä¸ GPT-3

- **Transformeræ¨¡å‹**ï¼šåŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°å¤„ç†é•¿åºåˆ—æ•°æ®ã€‚
- **GPT-3**ï¼šé€šè¿‡1750äº¿ä¸ªå‚æ•°ï¼ŒGPT-3èƒ½å¤Ÿç”Ÿæˆæµç•…çš„è‡ªç„¶è¯­è¨€æ–‡æœ¬ï¼Œåº”ç”¨å¹¿æ³›äºæ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘ç­‰é¢†åŸŸã€‚

#### GPT ç¤ºä¾‹ï¼šHugging Faceè°ƒç”¨



| æ¨¡å‹                                     | é€‚ç”¨è¯­è¨€ | ç”¨é€”     |
| ---------------------------------------- | -------- | -------- |
| `distilbert-base-cased-distilled-squad`  | è‹±æ–‡     | è‹±æ–‡é—®ç­” |
| `uer/roberta-base-chinese-extractive-qa` | ä¸­æ–‡     | ä¸­æ–‡é—®ç­” |

Q: å¦‚ä½•ç”¨**æµè§ˆå™¨æ‰‹åŠ¨ä¸‹è½½** `uer/roberta-base-chinese-extractive-qa` æ¨¡å‹ï¼Œåšåˆ°å®Œå…¨ **ç¦»çº¿éƒ¨ç½²** çš„æ­¥éª¤ï¼š

> ğŸ”— 1. æ‰“å¼€æ¨¡å‹é¡µé¢
>
> æµè§ˆå™¨è®¿é—®ï¼šhttps://huggingface.co/uer/roberta-base-chinese-extractive-qa
>
> ------
>
> ğŸ“ 2. è¿›å…¥ â€œFiles and versionsâ€ é¡µé¢ï¼Œæ‰‹åŠ¨ä¸‹è½½ä»¥ä¸‹å‡ ä¸ªå…³é”®æ–‡ä»¶ï¼š
>
> | æ–‡ä»¶å                    | è¯´æ˜                           |
> | ------------------------- | ------------------------------ |
> | `config.json`             | æ¨¡å‹ç»“æ„é…ç½®                   |
> | `pytorch_model.bin`       | æ¨¡å‹æƒé‡ï¼ˆå¾ˆå¤§ï¼Œ400MB å·¦å³ï¼‰   |
> | `tokenizer_config.json`   | tokenizer é…ç½®                 |
> | `vocab.txt`               | ä¸­æ–‡è¯è¡¨ï¼ˆå¿…éœ€ï¼‰               |
> | `special_tokens_map.json` | ç‰¹æ®Šç¬¦å·å®šä¹‰ï¼ˆå¯é€‰ä½†æ¨èï¼‰     |
> | `tokenizer.json`          | tokenizer çš„äºŒè¿›åˆ¶å½¢å¼ï¼ˆå¯é€‰ï¼‰ |
>
> ä½ å¯ä»¥åœ¨ç½‘é¡µä¸­ä¾æ¬¡ç‚¹å‡»è¿™äº›æ–‡ä»¶ï¼Œç„¶åç‚¹å‡»å³ä¸Šè§’ â€œDownloadâ€ã€‚
>
> ------
>
> ğŸ—‚ï¸ 3. ä¸‹è½½åï¼ŒæŠŠå®ƒä»¬æ”¾å…¥ä¸€ä¸ªæœ¬åœ°æ–‡ä»¶å¤¹ï¼Œä¾‹å¦‚ï¼š
>
> ```
> ./models/roberta-chinese-qa/
> â”œâ”€â”€ config.json
> â”œâ”€â”€ pytorch_model.bin
> â”œâ”€â”€ tokenizer_config.json
> â”œâ”€â”€ vocab.txt
> â”œâ”€â”€ special_tokens_map.json
> â”œâ”€â”€ tokenizer.json
> ```





```python
from transformers import pipeline

qa = pipeline(
    "question-answering",
    #model="./models/distilbert-base-cased-distilled-squad",
    model="./models/roberta-chinese-qa",
    tokenizer="./models/roberta-chinese-qa",
    framework="pt"  # æ˜¾å¼è¦æ±‚ä½¿ç”¨ PyTorch
)

result = qa(
    question="è°æ˜¯äººå·¥æ™ºèƒ½ä¹‹çˆ¶ï¼Ÿ",
    context="è‰¾ä¼¦Â·å›¾çµæ˜¯äººå·¥æ™ºèƒ½ä¹‹çˆ¶ï¼Œè¢«èª‰ä¸ºè®¡ç®—æœºç§‘å­¦çš„å¥ åŸºäººã€‚"
)

print(result)       # çœ‹å®Œæ•´ç»“æœ
print(result["answer"])  # è¾“å‡ºåº”ä¸ºï¼šè‰¾ä¼¦Â·å›¾çµ
```

/Users/hfyan/miniconda3/bin/python /Users/hfyan/Code/tmp/script2.py 
{'score': 0.3183244466781616, 'start': 0, 'end': 5, 'answer': 'è‰¾ä¼¦Â·å›¾çµ'}
è‰¾ä¼¦Â·å›¾çµ



> â€œç”¨ä¸€ä¸ªæœ¬åœ°çš„ä¸­æ–‡é—®ç­”æ¨¡å‹ï¼Œåœ¨ä¸€æ®µæ–‡æœ¬é‡Œæå–é—®é¢˜çš„ç­”æ¡ˆâ€ã€‚
>
> 
>
> âœ… 1. å¼•å…¥ Hugging Face çš„ `pipeline`
>
> ```python
> from transformers import pipeline
> ```
>
> è¿™æ˜¯ Hugging Face `transformers` æä¾›çš„ç®€æ´ APIï¼Œç”¨äºå¿«é€Ÿæ„å»º NLP ä»»åŠ¡ç®¡é“ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ã€ç¿»è¯‘ç­‰ã€‚
>
> 
>
> âœ… 2. æ„é€ é—®ç­”ä»»åŠ¡çš„ pipelineï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰
>
> ```python
> qa = pipeline(
>     "question-answering",
>     #model="./distilbert-base-cased-distilled-squad",
>     model="./roberta-chinese-qa",      # ä½¿ç”¨æœ¬åœ°ä¸­æ–‡æ¨¡å‹ç›®å½•
>     framework="pt"                     # å¼ºåˆ¶ä½¿ç”¨ PyTorch
> )
> ```
>
> å‚æ•°è¯´æ˜ï¼š
>
> | å‚æ•°                           | å«ä¹‰                                                         |
> | ------------------------------ | ------------------------------------------------------------ |
> | `"question-answering"`         | æŒ‡å®šä»»åŠ¡ç±»å‹æ˜¯æŠ½å–å¼é—®ç­”ï¼ˆextractive QAï¼‰                    |
> | `model="./roberta-chinese-qa"` | æŒ‡å‘æœ¬åœ°ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶å¤¹ï¼Œé‡Œé¢åŒ…å« `pytorch_model.bin`ã€`config.json` ç­‰ |
> | `framework="pt"`               | æ˜¾å¼è¦æ±‚ä½¿ç”¨ PyTorchï¼Œè€Œä¸æ˜¯ TensorFlowï¼Œé˜²æ­¢æ„å¤–åŠ è½½ TF æ¨¡å‹å¼•å‘é”™è¯¯ |
>
> ğŸ“ `tokenizer` ä¼šè‡ªåŠ¨ä»æ¨¡å‹ç›®å½•ä¸­åŠ è½½ï¼Œæ— éœ€å•ç‹¬æŒ‡å®šã€‚
>
> ------
>
> âœ… 3. è¿è¡Œé—®ç­”æ¨ç†
>
> ```python
> result = qa(
>     question="è°æ˜¯äººå·¥æ™ºèƒ½ä¹‹çˆ¶ï¼Ÿ",
>     context="è‰¾ä¼¦Â·å›¾çµæ˜¯äººå·¥æ™ºèƒ½ä¹‹çˆ¶ï¼Œè¢«èª‰ä¸ºè®¡ç®—æœºç§‘å­¦çš„å¥ åŸºäººã€‚"
> )
> ```
>
> è¯´æ˜ï¼š
>
> - è¾“å…¥çš„ `question` æ˜¯ç”¨æˆ·è¦é—®çš„é—®é¢˜ï¼›
> - `context` æ˜¯åŒ…å«ç­”æ¡ˆçš„ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼ˆæ¨¡å‹ä¼šåœ¨é‡Œé¢æŸ¥æ‰¾ç­”æ¡ˆï¼‰ï¼›
> - è¿”å›çš„æ˜¯ä¸€ä¸ªåŒ…å« **é¢„æµ‹ç­”æ¡ˆä½ç½®ã€å†…å®¹ã€ç½®ä¿¡åº¦** çš„å­—å…¸ç»“æ„ã€‚
>
> 
>
> ğŸ›  å¸¸è§è¡¥å……å»ºè®®ï¼š
>
> - **æ›´å¤æ‚ context**ï¼šä½ å¯ä»¥ç»™å®ƒæ›´å¤šæ®µè½ï¼Œå®ƒä¼šæ‰¾æœ€æœ‰å¯èƒ½çš„ç­”æ¡ˆï¼›
>
> - **ä¸­æ–‡ç²¾åº¦æé«˜**ï¼šä½ å¯ä»¥å°è¯• `hfl/chinese-roberta-wwm-ext-large` ä¹‹ç±»çš„æ¨¡å‹ï¼›
>
>   



# ç¬¬äºŒéƒ¨åˆ† äººå·¥æ™ºèƒ½åº”ç”¨ç¼–ç¨‹

é˜…è¯»ï¼šPyTorch æ•™ç¨‹ï¼Œhttps://www.runoob.com/pytorch/pytorch-tutorial.html

ä½¿ç”¨PyTorchå®ç°5ä¸ªä»åŸºç¡€æ¨¡å‹åˆ°è¾ƒå¤æ‚æ¨¡å‹çš„è®­ç»ƒä¸åº”ç”¨ã€‚ç›¸å…³ä»£ç åŠè¯´æ˜æ–‡æ¡£å·²æ•´ç†äº Markdown æ–‡ä»¶ä¸­ï¼Œè¯¦è§é¡¹ç›®ä»“åº“ï¼šhttps://github.com/GMyhf/2025spring-cs201/tree/main/LLM

1. `0_xor_bp_neural_net_manual`ï¼šæ‰‹åŠ¨å®ç°åå‘ä¼ æ’­çš„ç®€å•ç¥ç»ç½‘ç»œï¼Œç”¨äºå¼‚æˆ–é—®é¢˜ã€‚
2. `1_iris_neural_network`ï¼šæ„å»ºå¹¶è®­ç»ƒç”¨äºé¸¢å°¾èŠ±åˆ†ç±»çš„æ•°æ®é©±åŠ¨ç¥ç»ç½‘ç»œã€‚
3. `2_mnist_resnet18`ï¼šä½¿ç”¨ ResNet18 æ¨¡å‹å¯¹ MNIST æ‰‹å†™æ•°å­—è¿›è¡Œåˆ†ç±»ã€‚
4. `3_cifar10_resnet18`ï¼šå°† ResNet18 åº”ç”¨äº CIFAR-10 å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚
5. `4_tiny_imagenet_resnet50`ï¼šåŸºäº ResNet50 æ¨¡å‹å¤„ç† Tiny ImageNet å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚



## 2.1 ç¥ç»ç½‘ç»œä¸­çš„åå‘ä¼ æ’­

https://www.geeksforgeeks.org/machine-learning/backpropagation-in-neural-network/



åå‘ä¼ æ’­ï¼ˆBack Propagationï¼‰ï¼Œåˆç§°ä¸ºâ€œè¯¯å·®çš„åå‘ä¼ æ’­â€ï¼Œæ˜¯ä¸€ç§ç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œçš„æ–¹æ³•ã€‚å…¶ç›®æ ‡æ˜¯é€šè¿‡è°ƒæ•´ç½‘ç»œä¸­çš„æƒé‡ï¼ˆweightsï¼‰å’Œåç½®ï¼ˆbiasesï¼‰ï¼Œæ¥å‡å°æ¨¡å‹é¢„æµ‹è¾“å‡ºä¸å®é™…è¾“å‡ºä¹‹é—´çš„å·®å¼‚ã€‚

å®ƒé€šè¿‡è¿­ä»£æ–¹å¼æ›´æ–°æƒé‡å’Œåç½®ï¼Œä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼ˆcost functionï¼‰ã€‚åœ¨æ¯ä¸€ä¸ªè®­ç»ƒå‘¨æœŸï¼ˆepochï¼‰ä¸­ï¼Œæ¨¡å‹ä¼šæ ¹æ®è¯¯å·®æ¢¯åº¦ï¼ˆerror gradientï¼‰æ›´æ–°å‚æ•°ï¼Œå¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•åŒ…æ‹¬æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰æˆ–éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ã€‚è¯¥ç®—æ³•ä½¿ç”¨å¾®ç§¯åˆ†ä¸­çš„é“¾å¼æ³•åˆ™æ¥è®¡ç®—æ¢¯åº¦ï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ç©¿è¶Šå¤æ‚çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œä¼˜åŒ–æŸå¤±å‡½æ•°ã€‚

> Back Propagation is also known as "Backward Propagation of Errors" is a method used to train neural network . Its goal is to reduce the difference between the modelâ€™s predicted output and the actual output by adjusting the weights and biases in the network.
>
> It works iteratively to adjust weights and bias to minimize the cost function. In each epoch the model adapts these parameters by reducing loss by following the error gradient. It often uses optimization algorithms like **gradient descent** or **stochastic gradient descent**. The algorithm computes the gradient using the chain rule from calculus allowing it to effectively navigate complex layers in the neural network to minimize the cost function.

![Backpropagation-in-Neural-Network-1](https://media.geeksforgeeks.org/wp-content/uploads/20250701163824448467/Backpropagation-in-Neural-Network-1.webp)

<center>Fig(a) A simple illustration of how the backpropagation works by adjustments of weights</center>

<center>å›¾(a)ï¼šé€šè¿‡æƒé‡è°ƒæ•´ï¼Œç®€å•å±•ç¤ºåå‘ä¼ æ’­çš„å·¥ä½œæ–¹å¼</center>



**åå‘ä¼ æ’­çš„é‡è¦æ€§ï¼š**

- **é«˜æ•ˆçš„æƒé‡æ›´æ–°**ï¼šåˆ©ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¯ä¸ªæƒé‡çš„æ¢¯åº¦ï¼Œä»è€Œé«˜æ•ˆåœ°æ›´æ–°å‚æ•°ã€‚
- **è‰¯å¥½çš„æ‰©å±•æ€§**ï¼šé€‚ç”¨äºå¤šå±‚ç»“æ„å’Œå¤æ‚æ¶æ„ï¼Œæ˜¯æ·±åº¦å­¦ä¹ å¯è¡Œçš„æ ¸å¿ƒç®—æ³•ã€‚
- **è‡ªåŠ¨å­¦ä¹ èƒ½åŠ›**ï¼šè®­ç»ƒè¿‡ç¨‹è‡ªåŠ¨è¿›è¡Œï¼Œæ¨¡å‹ä¼šä¸æ–­è°ƒæ•´è‡ªèº«æ¥ä¼˜åŒ–æ€§èƒ½ã€‚

> **Back Propagation** plays a critical role in how neural networks improve over time. Here's why:
>
> 1. **Efficient Weight Update**: It computes the gradient of the loss function with respect to each weight using the chain rule making it possible to update weights efficiently.
> 2. **Scalability**: The Back Propagation algorithm scales well to networks with multiple layers and complex architectures making deep learning feasible.
> 3. **Automated Learning**: With Back Propagation the learning process becomes automated and the model can adjust itself to optimize its performance.



### 2.1.1 åå‘ä¼ æ’­ç®—æ³•çš„å·¥ä½œæµç¨‹

åå‘ä¼ æ’­ç®—æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼š**å‰å‘ä¼ æ’­ï¼ˆForward Passï¼‰** å’Œ **åå‘ä¼ æ’­ï¼ˆBackward Passï¼‰**

#### 1. Forward Pass Workå‰å‘ä¼ æ’­

è¾“å…¥æ•°æ®ä»è¾“å…¥å±‚å¼€å§‹ï¼Œç»è¿‡å¸¦æƒé‡çš„è¿æ¥ä¼ é€’åˆ°éšè—å±‚ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªæœ‰ä¸¤ä¸ªéšè—å±‚ h1 å’Œ h2 çš„ç½‘ç»œä¸­ï¼Œh1 çš„è¾“å‡ºä½œä¸º h2 çš„è¾“å…¥ã€‚åœ¨åº”ç”¨æ¿€æ´»å‡½æ•°å‰ï¼Œè¿˜ä¼šåŠ ä¸Šåç½®é¡¹ã€‚

æ¯ä¸€å±‚éƒ½ä¼šè®¡ç®—è¾“å…¥çš„åŠ æƒå’Œï¼ˆè®°ä½œ `a`ï¼‰ï¼Œå†é€šè¿‡å¦‚ ReLU ç­‰æ¿€æ´»å‡½æ•°å¾—åˆ°è¾“å‡º `o`ã€‚æœ€ç»ˆï¼Œè¾“å‡ºå±‚é€šå¸¸ä¼šä½¿ç”¨ softmax æ¿€æ´»å‡½æ•°å°†ç»“æœè½¬æ¢ä¸ºåˆ†ç±»æ¦‚ç‡ã€‚

> ### Working of Back Propagation Algorithm
>
> The Back Propagation algorithm involves two main steps: the **Forward Pass** and the **Backward Pass**.
>
> ### 1. Forward Pass Work
>
> In **forward pass** the input data is fed into the input layer. These inputs combined with their respective weights are passed to hidden layers. For example in a network with two hidden layers (h1 and h2) the output from h1 serves as the input to h2. Before applying an activation function, a bias is added to the weighted inputs.
>
> Each hidden layer computes the weighted sum (`a`) of the inputs then applies an activation function like [**ReLU (Rectified Linear Unit)**](https://www.geeksforgeeks.org/deep-learning/relu-activation-function-in-deep-learning/) to obtain the output (`o`). The output is passed to the next layer where an activation function such as [**softmax**](https://www.geeksforgeeks.org/deep-learning/the-role-of-softmax-in-neural-networks-detailed-explanation-and-applications/) converts the weighted outputs into probabilities for classification.

![Backpropagation-in-Neural-Network-2](https://media.geeksforgeeks.org/wp-content/uploads/20250701163954688803/Backpropagation-in-Neural-Network-2.webp)

<center>The forward pass using weights and biases</center>

> h1,h2ï¼Œè¡¨ç¤ºéšè—å±‚çš„ä¸¤ä¸ªç¥ç»å…ƒ



#### 2. Backward Passåå‘ä¼ æ’­

åå‘ä¼ æ’­é˜¶æ®µä¼šå°†é¢„æµ‹è¾“å‡ºä¸å®é™…è¾“å‡ºçš„è¯¯å·®å‘åä¼ é€’ï¼Œå¹¶è°ƒæ•´æ¯ä¸€å±‚çš„æƒé‡å’Œåç½®ã€‚å¸¸è§çš„è¯¯å·®è®¡ç®—æ–¹æ³•æ˜¯**å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰**ï¼š

$MSE = (\text{Predicted Output} âˆ’ \text{Actual Output})^2$

åœ¨è¯¯å·®è®¡ç®—ä¹‹åï¼Œé€šè¿‡é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦ï¼Œè¿™äº›æ¢¯åº¦ç”¨äºæŒ‡å¯¼æƒé‡å’Œåç½®çš„æ›´æ–°æ–¹å‘å’Œå¹…åº¦ã€‚åå‘ä¼ æ’­è¿‡ç¨‹æ˜¯é€å±‚æ‰§è¡Œçš„ï¼Œ<mark>æ¿€æ´»å‡½æ•°çš„å¯¼æ•°åœ¨æ¢¯åº¦è®¡ç®—ä¸­èµ·ç€å…³é”®ä½œç”¨</mark>ã€‚



**åå‘ä¼ æ’­çš„ç¤ºä¾‹ï¼šæœºå™¨å­¦ä¹ ä¸­çš„æ¡ˆä¾‹**

å‡è®¾æˆ‘ä»¬ä½¿ç”¨ sigmoid æ¿€æ´»å‡½æ•°ï¼Œç›®æ ‡è¾“å‡ºä¸º 0.5ï¼Œå­¦ä¹ ç‡ä¸º 1ã€‚

> ### 2. Backward Pass
>
> In the backward pass the error (the difference between the predicted and actual output) is propagated back through the network to adjust the weights and biases. One common method for error calculation is the [**Mean Squared Error (MSE)**](https://www.geeksforgeeks.org/maths/mean-squared-error/) given by:
>
> $MSE = (\text{Predicted Output} âˆ’ \text{Actual Output})^2$
>
> Once the error is calculated the network adjusts weights using **gradients** which are computed with the chain rule. These gradients indicate how much each weight and bias should be adjusted to minimize the error in the next iteration. The backward pass continues layer by layer ensuring that the network learns and improves its performance. The activation function through its derivative plays a crucial role in computing these gradients during Back Propagation.
>
> 
>
> ## Example of Back Propagation in Machine Learning
>
> Letâ€™s walk through an example of Back Propagation in machine learning. Assume the neurons use the sigmoid activation function for the forward and backward pass. The target output is 0.5 and the learning rate is 1.

![Backpropagation-in-Neural-Network-3](https://media.geeksforgeeks.org/wp-content/uploads/20250701164029130520/Backpropagation-in-Neural-Network-3.webp)

<center>Example (1) of backpropagation sum</center>



### 2.1.2 å‰å‘ä¼ æ’­Forward Propagation

#### 1. Initial Calculationåˆå§‹è®¡ç®—

The weighted sum at each node is calculated using:

> $a_j=\sum(w_{i,j}âˆ—x_i)$

Where,

- $a_j$ is the weighted sum of all the inputs and weights at each node
- $w_{i,j}$ represents the weights between the $i^{th}$ input and the $j^{th}$ neuron
- $x_i$ represents the value of the $i^{th}$ input

`O (output):`After applying the activation function to `a`, we get the output of the neuron:

> $o_j = \text{activation function}(a_j)$

#### 2. Sigmoid Function

The sigmoid function returns a value between 0 and 1, introducing non-linearity into the model.

> $y_j = \frac{1}{1+e^{âˆ’a_j}}$ 

![Backpropagation-in-Neural-Network-4](https://media.geeksforgeeks.org/wp-content/uploads/20250701164114106895/Backpropagation-in-Neural-Network-4.webp)

<center>To find the outputs of y3, y4 and y5</center>



#### 3. Computing Outputsè¾“å‡ºè®¡ç®—

h1 èŠ‚ç‚¹ï¼š
$$
a_1 = (w_{1,1} \times x_1) + (w_{2,1} \times x_2)
$$
$$
a_1 = (0.2 \times 0.35) + (0.2 \times 0.7) = 0.21
$$

è®¡ç®—å®Œ $ a_1 $ åï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­è®¡ç®— $ y_3 $ çš„å€¼ï¼š

$$
y_j = F(a_j) = \frac{1}{1 + e^{-a_1}}
$$
$$
y_3 = F(0.21) = \frac{1}{1 + e^{-0.21}} = 0.56
$$



h2 èŠ‚ç‚¹ï¼š
$$
a_2 = (w_{1,2} \times x_1) + (w_{2,2} \times x_2) = (0.3 \times 0.35) + (0.3 \times 0.7) = 0.315
$$
$$
y_4 = F(0.315) = \frac{1}{1 + e^{-0.315}} = 0.578
$$



è¾“å‡ºèŠ‚ç‚¹ O3ï¼š
$$
a_3 = (w_{1,3} \times y_3) + (w_{2,3} \times y_4) = (0.3 \times 0.56) + (0.9 \times 0.58) = 0.702
$$
$$
y_5 = F(0.702) = \frac{1}{1 + e^{-0.702}} = 0.67
$$



> At h1 node
>
> Once we calculated the a1 value, we can now proceed to find the y3 value:
>
> Similarly find the values of y4 at h2 and y5 at O3



![Backpropagation-in-Neural-Network-5](https://media.geeksforgeeks.org/wp-content/uploads/20250701164956768059/Backpropagation-in-Neural-Network-5.webp)

<center>Values of y3, y4 and y5</center>



#### 4. Error Calculationè¯¯å·®è®¡ç®—

Our actual output is 0.5 but we obtained 0.67**.** To calculate the error we can use the below formula:

> $Error_j=y_{target}âˆ’y_5$ 

=> 0.5âˆ’0.67=âˆ’0.17

Using this error value we will be backpropagating.



### 2.1.3 åå‘ä¼ æ’­Back Propagation

#### 1. Calculating Gradientsè®¡ç®—æ¢¯åº¦

The change in each weight is calculated as:

> $Î”w_{ij}=Î·Ã—Î´_jÃ—O_j$

Where:

- $Î´_j$ is the error term for each unit,
- $Î·$ is the learning rate.

#### 2. Output Unit Errorè¾“å‡ºå±‚è¯¯å·®

For O3:

> $Î´_5=y_5(1âˆ’y_5)(y_{target}âˆ’y_5)$

=0.67(1âˆ’0.67)(âˆ’0.17)=âˆ’0.0376

#### 3. Hidden Unit Erroréšè—å±‚è¯¯å·®

For h1:

> $Î´_3=y_3(1âˆ’y_3)(w_{1,3}Ã—Î´_5)$

=0.56(1âˆ’0.56)(0.3Ã—âˆ’0.0376)=âˆ’0.0027



For h2:

> $Î´_4=y_4(1âˆ’y_4)(w_{2,3}Ã—Î´_5)$

=0.59(1âˆ’0.59)(0.9Ã—âˆ’0.0376)=âˆ’0.0819



#### 4. Weight Updatesæƒé‡æ›´æ–°

For the weights from hidden to output layer:

> $Î”w_{2,3}=1Ã—(âˆ’0.0376)Ã—0.59=âˆ’0.022184$

New weight:

> $w_{2,3}(new)=âˆ’0.022184+0.9=0.877816$

For weights from input to hidden layer:

> $Î”w_{1,1}=1Ã—(âˆ’0.0027)Ã—0.35=0.000945$

New weight:

> $w_{1,1}(new)=0.000945+0.2=0.200945$

Similarly other weights are updated:

- $w_{1,2}(new)=0.273225$
- $w_{1,3}(new)=0.086615$
- $w_{2,1}(new)=0.269445$
- $w_{2,2}(new)=0.18534$

The updated weights are illustrated below

![Backpropagation-in-Neural-Network-5](https://media.geeksforgeeks.org/wp-content/uploads/20250701165041328619/Backpropagation-in-Neural-Network-5.webp)

<center>Through backward pass the weights are updated</center>

> ä¸Šå›¾æƒé‡æ²¡æœ‰æ›´æ–°ï¼Œä¾‹å¦‚ï¼š$w_{2,2}$åº”è¯¥æ›´æ–°ä¸º0.18534



After updating the weights the forward pass is repeated yielding:

- y3=0.57
- y4=0.56
- y5=0.61

ä»æœªè¾¾åˆ°ç›®æ ‡å€¼ 0.5ï¼Œå› æ­¤ç»§ç»­è¿›è¡Œåå‘ä¼ æ’­ï¼Œç›´åˆ°æ”¶æ•›ã€‚

> Since y5=0.61 is still not the target output the process of calculating the error and backpropagating continues until the desired output is reached.



This process demonstrates how Back Propagation iteratively updates weights by minimizing errors until the network accurately predicts the output.

> $Error=y_{target}âˆ’y_5 $

=0.5âˆ’0.61=âˆ’0.11=0.5âˆ’0.61=âˆ’0.11

This process is said to be continued until the actual output is gained by the neural network.



### 2.1.4 Back Propagation Implementation in Python for XOR Problem

This code demonstrates how Back Propagation is used in a neural network to solve the XOR problem. The neural network consists of:

#### 1. Defining Neural Networkå®šä¹‰ç¥ç»ç½‘ç»œç»“æ„

è¾“å…¥å±‚ï¼š2ä¸ªèŠ‚ç‚¹ï¼Œéšè—å±‚ï¼š4ä¸ªç¥ç»å…ƒï¼Œè¾“å‡ºå±‚ï¼š1ä¸ªç¥ç»å…ƒï¼Œæ¿€æ´»å‡½æ•°ï¼šSigmoid

> We define a neural network as Input layer with 2 inputs, Hidden layer with 4 neurons, Output layer with 1 output neuron and use **Sigmoid** function as activation function.

- **self.input_size = input_size**: stores the size of the input layer
- **self.hidden_size = hidden_size:** stores the size of the hidden layer
- **self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)**: initializes weights for input to hidden layer
- **self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)**: initializes weights for hidden to output layer
- **self.bias_hidden = np.zeros((1, self.hidden_size)):** initializes bias for hidden layer
- **self.bias_output = np.zeros((1, self.output_size)):** initializes bias for output layer



```python3

import numpy as np


class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        self.weights_input_hidden = np.random.randn(
            self.input_size, self.hidden_size)
        self.weights_hidden_output = np.random.randn(
            self.hidden_size, self.output_size)

        self.bias_hidden = np.zeros((1, self.hidden_size))
        self.bias_output = np.zeros((1, self.output_size))

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        return x * (1 - x)
```



#### 2. Defining Feed Forward Networkå®šä¹‰å‰å‘ä¼ æ’­

In Forward pass inputs are passed through the network activating the hidden and output layers using the sigmoid function.

- **self.hidden_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden**: calculates activation for hidden layer
- **self.hidden_output= self.sigmoid(self.hidden_activation)**: applies activation function to hidden layer
- **self.output_activation= np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output:** calculates activation for output layer
- **self.predicted_output = self.sigmoid(self.output_activation):** applies activation function to output layer





```python3

def feedforward(self, X):
    self.hidden_activation = np.dot(
        X, self.weights_input_hidden) + self.bias_hidden
    self.hidden_output = self.sigmoid(self.hidden_activation)

    self.output_activation = np.dot(
        self.hidden_output, self.weights_hidden_output) + self.bias_output
    self.predicted_output = self.sigmoid(self.output_activation)

    return self.predicted_output
```



#### 3. Defining Backward Networkå®šä¹‰åå‘ä¼ æ’­

In Backward pass or Back Propagation the errors between the predicted and actual outputs are computed. The gradients are calculated using the derivative of the sigmoid function and weights and biases are updated accordingly.

- **output_error = y - self.predicted_output:** calculates the error at the output layer
- **output_delta = output_error * self.sigmoid_derivative(self.predicted_output):** calculates the delta for the output layer
- **hidden_error = np.dot(output_delta, self.weights_hidden_output.T):** calculates the error at the hidden layer
- **hidden_delta = hidden_error \* self.sigmoid_derivative(self.hidden_output):** calculates the delta for the hidden layer
- **self.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * learning_rate:** updates weights between hidden and output layers
- **self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate:** updates weights between input and hidden layers



```python3

def backward(self, X, y, learning_rate):
    output_error = y - self.predicted_output
    output_delta = output_error * \
        self.sigmoid_derivative(self.predicted_output)

    hidden_error = np.dot(output_delta, self.weights_hidden_output.T)
    hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)

    self.weights_hidden_output += np.dot(self.hidden_output.T,
                                         output_delta) * learning_rate
    self.bias_output += np.sum(output_delta, axis=0,
                               keepdims=True) * learning_rate
    self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate
    self.bias_hidden += np.sum(hidden_delta, axis=0,
                               keepdims=True) * learning_rate
```



#### 4. Training Networkè®­ç»ƒç½‘ç»œ

The network is trained over 10,000 epochs using the Back Propagation algorithm with a learning rate of 0.1 progressively reducing the error.

- **output = self.feedforward(X):** computes the output for the current inputs
- **self.backward(X, y, learning_rate):** updates weights and biases using Back Propagation
- **loss = np.mean(np.square(y - output)):** calculates the mean squared error (MSE) loss



```python3

def train(self, X, y, epochs, learning_rate):
    for epoch in range(epochs):
        output = self.feedforward(X)
        self.backward(X, y, learning_rate)
        if epoch % 4000 == 0:
            loss = np.mean(np.square(y - output))
            print(f"Epoch {epoch}, Loss:{loss}")
```

#### 5. Testing Neural Networkæµ‹è¯•ç¥ç»ç½‘ç»œ

- **X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]):** defines the input data
- **y = np.array([[0], [1], [1], [0]]):** defines the target values
- **nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1):** initializes the neural network
- **nn.train(X, y, epochs=10000, learning_rate=0.1):** trains the network
- **output = nn.feedforward(X):** gets the final predictions after training





```python3

X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)
nn.train(X, y, epochs=10000, learning_rate=0.1)

output = nn.feedforward(X)
print("Predictions after training:")
print(output)
```

**Output:**

![Screenshot-2025-03-07-130223](https://media.geeksforgeeks.org/wp-content/uploads/20250307130234877256/Screenshot-2025-03-07-130223.png)

<center>Trained Model</center>



è®­ç»ƒåˆæœŸæŸå¤±ä¸º 0.2713ï¼Œé€æ­¥ä¸‹é™åˆ° 0.0066ï¼ˆç¬¬8000è½®ï¼‰ã€‚æœ€ç»ˆæ¨¡å‹å¯ä»¥å¾ˆå¥½åœ°é€¼è¿‘ XOR å‡½æ•°çš„è¾“å‡ºï¼Œå³ï¼š

- å¯¹äºè¾“å…¥ [0,0] å’Œ [1,1]ï¼Œè¾“å‡ºæ¥è¿‘ 0

- å¯¹äºè¾“å…¥ [0,1] å’Œ [1,0]ï¼Œè¾“å‡ºæ¥è¿‘ 1

  

> - The output shows the training progress of a neural network over 10,000 epochs. Initially the loss was high (0.2713) but it gradually decreased as the network learned reaching a low value of 0.0066 by epoch 8000.
> - The final predictions are close to the expected XOR outputs: approximately 0 for [0, 0] and [1, 1] and approximately 1 for [0, 1] and [1, 0] indicating that the network successfully learned to approximate the XOR function.



### 2.1.5 åå‘ä¼ æ’­çš„ä¼˜ç‚¹

**æ˜“äºå®ç°**ï¼šé€‚åˆåˆå­¦è€…ï¼Œæ— éœ€å¤ªå¤šç¥ç»ç½‘ç»œèƒŒæ™¯

**ç»“æ„ç®€å•ï¼Œçµæ´»åº”ç”¨**ï¼šä»ç®€å•å‰é¦ˆåˆ°å¤æ‚å·ç§¯/å¾ªç¯ç½‘ç»œéƒ½å¯ä½¿ç”¨

**é«˜æ•ˆ**ï¼šç›´æ¥æ ¹æ®è¯¯å·®æ›´æ–°æƒé‡ï¼Œå­¦ä¹ é€Ÿåº¦å¿«

**è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›**ï¼šæœ‰åŠ©äºæ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°æ›´å¥½

**å¯æ‰©å±•æ€§å¥½**ï¼šé€‚ç”¨äºå¤§å‹æ•°æ®é›†å’Œæ·±å±‚æ¨¡å‹

> **Advantages of Back Propagation for Neural Network Training**
>
> The key benefits of using the Back Propagation algorithm are:
>
> 1. **Ease of Implementation:** Back Propagation is beginner-friendly requiring no prior neural network knowledge and simplifies programming by adjusting weights with error derivatives.
> 2. **Simplicity and Flexibility:** Its straightforward design suits a range of tasks from basic feedforward to complex convolutional or recurrent networks.
> 3. **Efficiency**: Back Propagation accelerates learning by directly updating weights based on error especially in deep networks.
> 4. **Generalization:** It helps models generalize well to new data improving prediction accuracy on unseen examples.
> 5. **Scalability:** The algorithm scales efficiently with larger datasets and more complex networks making it ideal for large-scale tasks.



### 2.1.6 åå‘ä¼ æ’­é¢ä¸´çš„æŒ‘æˆ˜

**æ¢¯åº¦æ¶ˆå¤±**ï¼šåœ¨æ·±å±‚ç½‘ç»œä¸­æ¢¯åº¦å¯èƒ½è¿‡å°ï¼Œå¯¼è‡´å­¦ä¹ å›°éš¾ï¼ˆç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨ sigmoid/tanh æ—¶ï¼‰

**æ¢¯åº¦çˆ†ç‚¸**ï¼šæ¢¯åº¦å¯èƒ½å˜å¾—è¿‡å¤§ï¼Œä½¿è®­ç»ƒä¸ç¨³å®š

**è¿‡æ‹Ÿåˆ**ï¼šæ¨¡å‹ç»“æ„è¿‡äºå¤æ‚æ—¶ï¼Œå¯èƒ½è®°ä½è®­ç»ƒé›†è€Œéå­¦ä¹ ä¸€èˆ¬æ€§è§„å¾‹

> **Challenges with Back Propagation**
>
> While Back Propagation is useful it does face some challenges:
>
> 1. **Vanishing Gradient Problem**: In deep networks the gradients can become very small during Back Propagation making it difficult for the network to learn. This is common when using activation functions like sigmoid or tanh.
> 2. **Exploding Gradients**: The gradients can also become excessively large causing the network to diverge during training.
> 3. **Overfitting:** If the network is too complex it might memorize the training data instead of learning general patterns.



### 2.1.7 å®Œæ•´xor_nnä»£ç 

```python
# å¯¹äºXORé—®é¢˜ï¼ˆè¾“å…¥ä¸º[0,0], [0,1], [1,0], [1,1]ï¼‰ï¼ŒæœŸæœ›è¾“å‡ºä¸º[0,1,1,0]
# æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­ï¼Œæ²¡æœ‰ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¿™æœ‰åŠ©äºç†è§£åº•å±‚åŸç†
# https://www.geeksforgeeks.org/backpropagation-in-neural-network/
import numpy as np


class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size  # è¾“å…¥ç‰¹å¾ç»´åº¦
        self.hidden_size = hidden_size  # éšè—å±‚ç¥ç»å…ƒæ•°é‡
        self.output_size = output_size  # è¾“å‡ºå±‚ç¥ç»å…ƒæ•°é‡

        # è¾“å…¥å±‚åˆ°éšè—å±‚çš„æƒé‡ï¼Œå½¢çŠ¶ä¸º (è¾“å…¥ç»´åº¦, éšè—å±‚ç»´åº¦)
        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)
        # éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡ï¼Œå½¢çŠ¶ä¸º (éšè—å±‚ç»´åº¦, è¾“å‡ºå±‚ç»´åº¦)
        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)

        # éšè—å±‚çš„åç½®ï¼Œå½¢çŠ¶ä¸º (1, éšè—å±‚ç»´åº¦)
        self.bias_hidden = np.zeros((1, self.hidden_size))
        # è¾“å‡ºå±‚çš„åç½®ï¼Œå½¢çŠ¶ä¸º (1, è¾“å‡ºå±‚ç»´åº¦)
        self.bias_output = np.zeros((1, self.output_size))

    def sigmoid(self, x):  # æ¿€æ´»å‡½æ•°ï¼Œå°†è¾“å…¥å‹ç¼©åˆ°(0,1)åŒºé—´
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        return x * (1 - x)  # Sigmoidçš„å¯¼æ•°ï¼Œç”¨äºåå‘ä¼ æ’­ä¸­çš„æ¢¯åº¦è®¡ç®—

    def feedforward(self, X):
        # éšè—å±‚è®¡ç®—
        self.hidden_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden  # çº¿æ€§å˜æ¢
        self.hidden_output = self.sigmoid(self.hidden_activation)  # æ¿€æ´»å‡½æ•°

        # è¾“å‡ºå±‚è®¡ç®—
        self.output_activation = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output
        self.predicted_output = self.sigmoid(self.output_activation)

        return self.predicted_output

    def backward(self, X, y, learning_rate):
        # è®¡ç®—è¾“å‡ºå±‚è¯¯å·®
        output_error = y - self.predicted_output  # è¯¯å·® = çœŸå®å€¼ - é¢„æµ‹å€¼
        # è®¡ç®—è¾“å‡ºå±‚çš„deltaï¼ˆæ¢¯åº¦çš„ä¸€éƒ¨åˆ†ï¼ŒæŸå¤±å¯¹æ¿€æ´»è¾“å…¥çš„æ¢¯åº¦ï¼‰
        output_delta = output_error * self.sigmoid_derivative(self.predicted_output)  # Delta = è¯¯å·® Ã— æ¿€æ´»å‡½æ•°å¯¼æ•°
        # output_delta = (y - Å·) * Ïƒ'(z_output)

        # è®¡ç®—éšè—å±‚è¯¯å·®ï¼ˆåå‘ä¼ æ’­ï¼‰
        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)  # å°†è¯¯å·®ä»è¾“å‡ºå±‚åå‘ä¼ æ’­åˆ°éšè—å±‚
        # hidden_error = output_delta @ W_hidden_output^T
        # è®¡ç®—éšè—å±‚çš„deltaï¼ˆæŸå¤±å¯¹éšè—å±‚æ¿€æ´»è¾“å…¥çš„æ¢¯åº¦ï¼‰
        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)  # Delta = è¯¯å·® Ã— æ¿€æ´»å‡½æ•°å¯¼æ•°
        # hidden_delta = (hidden_error) * Ïƒ'(z_hidden)

        # æ›´æ–°æƒé‡å’Œåç½®ï¼ˆä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ï¼‰
        # è®¡ç®—å¹¶æ›´æ–°éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡
        self.weights_hidden_output += np.dot(self.hidden_output.T,
                                             output_delta) * learning_rate  # æƒé‡æ›´æ–°é‡ = å­¦ä¹ ç‡ Ã— (éšè—å±‚è¾“å‡ºè½¬ç½® Ã— è¾“å‡ºå±‚delta)
        # W_hidden_output += learning_rate * (hidden_output^T @ output_delta)

        # æ›´æ–°è¾“å‡ºå±‚åç½®ï¼ŒåŸºäºæ‰€æœ‰æ ·æœ¬çš„è¾“å‡ºå±‚deltaæ²¿åˆ—æ±‚å’Œ
        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate  # åç½®æ›´æ–°é‡ = å­¦ä¹ ç‡ Ã— (æ²¿åˆ—æ±‚å’Œè¾“å‡ºå±‚delta)
        # b_output += learning_rate * sum(output_delta)

        # è®¡ç®—å¹¶æ›´æ–°ä»è¾“å…¥å±‚åˆ°éšè—å±‚çš„æƒé‡çš„æ¢¯åº¦
        self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate  # æƒé‡æ›´æ–°é‡ = å­¦ä¹ ç‡ Ã— (è¾“å…¥æ•°æ®è½¬ç½® Ã— éšè—å±‚delta)
        # W_input_hidden += learning_rate * (X^T @ hidden_delta)

        # æ›´æ–°éšè—å±‚åç½®ï¼ŒåŸºäºæ‰€æœ‰æ ·æœ¬çš„éšè—å±‚deltaæ²¿åˆ—æ±‚å’Œ
        # axis=0ï¼šæ²¿åˆ—æ±‚å’Œï¼Œèšåˆæ‰€æœ‰æ ·æœ¬çš„æ¢¯åº¦
        # keepdims=Trueï¼šä¿æŒåŸçŸ©é˜µçš„è¡Œæ•°ç»´åº¦ï¼Œç¡®ä¿åç½®æ›´æ–°çš„å½¢çŠ¶å…¼å®¹æ€§
        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate  # åç½®æ›´æ–°é‡ = å­¦ä¹ ç‡ Ã— (æ²¿åˆ—æ±‚å’Œéšè—å±‚delta)
        # b_hidden += learning_rate * sum(hidden_delta)

    def train(self, X, y, epochs, learning_rate):
        for epoch in range(epochs):
            output = self.feedforward(X)  # å‰å‘ä¼ æ’­
            self.backward(X, y, learning_rate)  # åå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–°
            if epoch % 4000 == 0:
                loss = np.mean(np.square(y - output))  # è®¡ç®—å‡æ–¹è¯¯å·®
                print(f"Epoch {epoch}, Loss:{loss}")


X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

# è¾“å…¥ç»´åº¦ 2ï¼ˆäºŒç»´äºŒè¿›åˆ¶ç‰¹å¾ï¼‰ï¼Œéšè—å±‚4ä¸ªç¥ç»å…ƒï¼Œè¾“å‡ºå±‚1ä¸ªç¥ç»å…ƒï¼ˆäºŒåˆ†ç±»é—®é¢˜ï¼‰
nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)
# è®­ç»ƒæ€»è½®æ¬¡, å­¦ä¹ ç‡
nn.train(X, y, epochs=10000, learning_rate=0.1)

output = nn.feedforward(X)
print("Predictions after training:")
print(output)
"""
Epoch 0, Loss:0.2653166263520884
Epoch 4000, Loss:0.007000926683956338
Epoch 8000, Loss:0.001973630232951721
Predictions after training:
[[0.03613239]
 [0.96431351]
 [0.96058291]
 [0.03919372]]
"""
```



## 2.2 Neural Networkå®ç°é¸¢å°¾èŠ±å‰æ•°æ®åˆ†ç±»

Updated 2013 GMT+8 Jul 26 2025

2025 spring, Complied by Hongfei Yan



é¸¢ï¼ˆyuÄnï¼‰å°¾èŠ±æ•°æ®é›†åŒ…å«ä¸‰ä¸ªç±»åˆ«ï¼Œæ¯ä¸ªç±»åˆ«æœ‰50ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰å››ä¸ªç‰¹å¾ï¼šèŠ±è¼é•¿åº¦ã€èŠ±è¼å®½åº¦ã€èŠ±ç“£é•¿åº¦ã€èŠ±ç“£å®½åº¦ã€‚ç›®æ ‡æ˜¯æ ¹æ®è¿™å››ä¸ªç‰¹å¾é¢„æµ‹èŠ±çš„ç§ç±»ï¼Œå±äºå¤šåˆ†ç±»é—®é¢˜ã€‚

### 1.å‡†å¤‡æ•°æ®

é¸¢å°¾èŠ±æ•°æ®é›†é€šå¸¸æ˜¯é€šè¿‡scikit-learnçš„datasetsæ¨¡å—è·å–çš„ï¼Œæ‰€ä»¥å¯èƒ½éœ€è¦ç»“åˆscikit-learnæ¥åŠ è½½æ•°æ®ï¼Œç„¶åè½¬æ¢æˆPyTorchçš„Tensorã€‚

æ•°æ®é¢„å¤„ç†æ–¹é¢ï¼Œéœ€è¦å°†ç‰¹å¾æ•°æ®å’Œæ ‡ç­¾åˆ†å¼€ã€‚ç‰¹å¾æ•°æ®éœ€è¦æ ‡å‡†åŒ–æˆ–å½’ä¸€åŒ–ï¼Œå› ä¸ºä¸åŒç‰¹å¾çš„é‡çº²å¯èƒ½ä¸åŒï¼Œè¿™å¯¹ç¥ç»ç½‘ç»œçš„è®­ç»ƒæœ‰å¸®åŠ©ã€‚æ ‡ç­¾éœ€è¦è½¬æ¢æˆæ•°å€¼å½¢å¼ï¼Œæ¯”å¦‚0ã€1ã€2ï¼Œç„¶åå¯èƒ½è¿˜éœ€è¦è½¬æ¢ä¸ºé•¿æ•´å‹å¼ é‡ï¼Œå› ä¸ºäº¤å‰ç†µæŸå¤±å‡½æ•°åœ¨PyTorchä¸­é€šå¸¸è¦æ±‚è¿™æ ·çš„æ ¼å¼ã€‚

### 2.è®­ç»ƒæ¨¡å‹å’ŒéªŒè¯

ç„¶åï¼Œæ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹ã€‚è€ƒè™‘åˆ°é¸¢å°¾èŠ±æ•°æ®é›†ç›¸å¯¹ç®€å•ï¼Œå¯èƒ½ä¸éœ€è¦å¾ˆæ·±çš„ç½‘ç»œã€‚ä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç½‘ç»œå¯èƒ½å°±è¶³å¤Ÿäº†ã€‚æ¯”å¦‚ï¼Œè¾“å…¥å±‚4ä¸ªèŠ‚ç‚¹ï¼Œéšè—å±‚å¯ä»¥é€‰æ‹©10ä¸ªèŠ‚ç‚¹ï¼Œè¾“å‡ºå±‚3ä¸ªèŠ‚ç‚¹å¯¹åº”ä¸‰ä¸ªç±»åˆ«ã€‚æ¿€æ´»å‡½æ•°å¯ä»¥ç”¨ReLUï¼Œè¾“å‡ºå±‚ç”¨Softmaxï¼Œä¸è¿‡æ›´å¸¸è§çš„åšæ³•æ˜¯ä½¿ç”¨CrossEntropyLossï¼Œå®ƒå†…éƒ¨å·²ç»ç»“åˆäº†Softmaxï¼Œæ‰€ä»¥è¾“å‡ºå±‚ä¸éœ€è¦æ˜¾å¼åº”ç”¨Softmaxã€‚

æ¥ä¸‹æ¥æ˜¯æ•°æ®é›†çš„åˆ’åˆ†ï¼Œé€šå¸¸åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œé¸¢å°¾èŠ±æ•°æ®é›†æ ·æœ¬è¾ƒå°‘ï¼Œå¯èƒ½éœ€è¦è¿›è¡Œåˆ†å±‚æŠ½æ ·ï¼Œç¡®ä¿æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„æ¯”ä¾‹ä¸€è‡´ã€‚æˆ–è€…ä½¿ç”¨äº¤å‰éªŒè¯ï¼Œä½†ç”±äºç”¨æˆ·å¯èƒ½å¸Œæœ›ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œå¯èƒ½ç›´æ¥é‡‡ç”¨80-20çš„åˆ’åˆ†ã€‚

å°†æ•°æ®è½¬æ¢ä¸ºPyTorchçš„DataLoaderï¼Œæ–¹ä¾¿æ‰¹é‡è®­ç»ƒã€‚æ•°æ®é›†è¿›è¡Œè®­ç»ƒå¯ä»¥åˆ†å°æ‰¹é‡å¤„ç†ã€‚

è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ã€‚äº¤å‰ç†µæŸå¤±å‡½æ•°é€‚ç”¨äºå¤šåˆ†ç±»é—®é¢˜ï¼Œä¼˜åŒ–å™¨å¯ä»¥é€‰æ‹©Adamæˆ–SGDã€‚å­¦ä¹ ç‡éœ€è¦é€‚å½“è®¾ç½®ï¼Œæ¯”å¦‚0.01æˆ–0.001ã€‚

è®­ç»ƒå¾ªç¯éƒ¨åˆ†ï¼Œéœ€è¦è¿­ä»£å¤šä¸ªepochï¼Œåœ¨æ¯ä¸ªepochä¸­è¿›è¡Œå‰å‘ä¼ æ’­ã€è®¡ç®—æŸå¤±ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°ã€‚åŒæ—¶ï¼Œå¯ä»¥ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å’Œå‡†ç¡®ç‡çš„å˜åŒ–ã€‚

éªŒè¯é˜¶æ®µï¼Œä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œç¡®ä¿æ¨¡å‹æ²¡æœ‰è¿‡æ‹Ÿåˆï¼Œå¹¶ä¸”æ³›åŒ–èƒ½åŠ›è‰¯å¥½ã€‚

å¯èƒ½éœ€è¦æ³¨æ„çš„é—®é¢˜åŒ…æ‹¬æ•°æ®æ ‡å‡†åŒ–åº”è¯¥åœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼Œç„¶ååº”ç”¨åˆ°æµ‹è¯•é›†ï¼Œé¿å…æ•°æ®æ³„æ¼ã€‚å¦å¤–ï¼Œæ ‡ç­¾æ˜¯å¦éœ€è¦è½¬æ¢ä¸ºone-hotç¼–ç ï¼Ÿåœ¨PyTorchä¸­ï¼ŒCrossEntropyLossä¸éœ€è¦ï¼Œå› ä¸ºæ ‡ç­¾æ˜¯ç±»åˆ«çš„ç´¢å¼•ã€‚

ç°åœ¨ï¼Œå°†è¿™äº›æ­¥éª¤æ•´åˆæˆä»£ç ï¼š

1. å¯¼å…¥å¿…è¦çš„åº“ï¼štorch, sklearn.datasetsä¸­çš„load_irisï¼Œsklearn.model_selectionçš„train_test_splitï¼Œä»¥åŠç›¸å…³çš„PyTorchæ¨¡å—ã€‚
2. åŠ è½½æ•°æ®ï¼Œåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ã€‚ä½¿ç”¨StandardScalerå¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼Œå¤„ç†æ—¶æ³¨æ„ä»…æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚
3. è½¬æ¢æ•°æ®ä¸ºPyTorchçš„Tensorï¼Œå¹¶åˆ›å»ºDataLoaderã€‚å¯èƒ½ç”±äºæ•°æ®é‡å°ï¼Œç›´æ¥ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†ä½œä¸ºæ‰¹é‡ã€‚
4. å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹çš„ç»“æ„ï¼Œä½¿ç”¨å…¨è¿æ¥å±‚å’ŒReLUæ¿€æ´»å‡½æ•°ã€‚
5. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ã€‚
6. ç¼–å†™è®­ç»ƒå¾ªç¯ï¼Œè¿­ä»£epochï¼Œè®¡ç®—æŸå¤±ï¼Œåå‘ä¼ æ’­ï¼Œè®°å½•å‡†ç¡®ç‡ã€‚
7. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

å¦å¤–ï¼Œç”±äºé¸¢å°¾èŠ±æ•°æ®é›†æ ·æœ¬è¾ƒå°‘ï¼Œæ¨¡å‹å¯èƒ½ä¼šå¾ˆå¿«è¿‡æ‹Ÿåˆï¼Œæ‰€ä»¥å¯èƒ½éœ€è¦æ—©åœæˆ–è€…è°ƒæ•´ç½‘ç»œç»“æ„ï¼Œæ¯”å¦‚å‡å°‘éšè—å±‚çš„å¤§å°ï¼Œæˆ–è€…æ·»åŠ æ­£åˆ™åŒ–ï¼Œæ¯”å¦‚Dropoutå±‚ã€‚ä¸è¿‡ï¼Œä½œä¸ºç¤ºä¾‹ï¼Œå¯èƒ½ä¿æŒç®€å•å³å¯ã€‚

è¿™æ ·åº”è¯¥èƒ½å®Œæˆä¸€ä¸ªåŸºæœ¬çš„åˆ†ç±»ä»»åŠ¡ã€‚ä¸è¿‡å¯èƒ½éœ€è¦è°ƒæ•´è¶…å‚æ•°ï¼Œæ¯”å¦‚å­¦ä¹ ç‡ã€epochæ¬¡æ•°ï¼Œæˆ–è€…ç½‘ç»œç»“æ„ï¼Œä»¥è¾¾åˆ°æ›´å¥½çš„æ•ˆæœã€‚ä¾‹å¦‚ï¼Œå¦‚æœå‡†ç¡®ç‡ä¸å¤Ÿé«˜ï¼Œå¯ä»¥å°è¯•å¢åŠ éšè—å±‚çš„å¤§å°ï¼Œæˆ–è€…å¢åŠ æ›´å¤šçš„å±‚ã€‚ä½†é¸¢å°¾èŠ±æ•°æ®é›†ç›¸å¯¹ç®€å•ï¼Œå¯èƒ½ç®€å•çš„æ¨¡å‹å°±è¶³å¤Ÿã€‚

å¦å¤–ï¼Œå¯èƒ½éœ€è¦æ³¨æ„æ•°æ®åˆ’åˆ†æ—¶çš„éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°ã€‚æ­¤å¤–ï¼Œæ ‡å‡†åŒ–å¤„ç†æ˜¯å¦æ­£ç¡®åº”ç”¨ï¼Œé¿å…æ•°æ®æ³„æ¼ã€‚åœ¨ç¤ºä¾‹ä¸­ï¼Œå·²ç»æ­£ç¡®ä½¿ç”¨fit_transformåœ¨è®­ç»ƒé›†ï¼Œtransformåœ¨æµ‹è¯•é›†ã€‚



ä»¥ä¸‹æ˜¯ä½¿ç”¨PyTorchå®ç°Fisheré¸¢å°¾èŠ±æ•°æ®é›†åˆ†ç±»çš„Pythonç¨‹åº`iris_neural_network.py`ï¼Œå¼ºè°ƒä½¿ç”¨äº†ç¥ç»ç½‘ç»œè¿›è¡Œåˆ†ç±»ã€‚

å¦‚æœç¼ºå°‘moduleï¼Œå®‰è£…å³å¯ï¼Œä¾‹å¦‚ï¼špip install torchï¼Œpip install sklearnç­‰ã€‚

> ä½¿ç”¨ pip install sklearn å¯èƒ½ä¸ä¼šå¾—åˆ°é¢„æœŸçš„ç»“æœï¼Œå› ä¸ºPyPIï¼ˆPython Package Indexï¼‰ä¸Šçš„å®˜æ–¹åŒ…åç§°æ˜¯ scikit-learnã€‚ä½†æˆ‘ç¡®å®ç”¨çš„ pip install sklearnã€‚è¿™æ˜¯å› ä¸º sklearn æ˜¯ scikit-learn çš„ä¸€ä¸ªåˆ«åï¼ŒæŸäº›æƒ…å†µä¸‹é€šè¿‡ pip å®‰è£…æ—¶èƒ½å¤Ÿè¯†åˆ«è¿™ä¸ªåˆ«åå¹¶æ­£ç¡®å®‰è£…è½¯ä»¶åŒ…ã€‚

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


# å®šä¹‰æ¨¡å‹ç»“æ„
class IrisNet(nn.Module):
    def __init__(self, input_size=4, hidden_size=10, num_classes=3):
        super(IrisNet, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, num_classes)
        )

    def forward(self, x):
        return self.net(x)


# è®­ç»ƒå‡½æ•°
def train(model, dataloader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    for batch_X, batch_y in dataloader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)

        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * batch_X.size(0)

    return running_loss / len(dataloader.dataset)


# æµ‹è¯•å‡½æ•°
def evaluate(model, X, y, device):
    model.eval()
    with torch.no_grad():
        X, y = X.to(device), y.to(device)
        outputs = model(X)
        _, predicted = torch.max(outputs, 1)
        accuracy = (predicted == y).float().mean().item()
    return accuracy, predicted


# ä¸»ç¨‹åº
def main():
    # è®¾ç½®è®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device("mps")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")

    # åŠ è½½æ•°æ®
    iris = load_iris()
    X, y = iris.data, iris.target

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    """
    random_state=42
    è®¾å®šéšæœºæ•°ç§å­ï¼Œä»è€Œç¡®ä¿æ¯æ¬¡è¿è¡Œä»£ç æ—¶æ•°æ®åˆ’åˆ†çš„ç»“æœéƒ½æ˜¯ç›¸åŒçš„ã€‚è¿™æ ·åšå¯ä»¥ä½¿å®éªŒå…·æœ‰å¯é‡å¤æ€§ï¼Œ
    æœ‰åˆ©äºè°ƒè¯•å’Œç»“æœå¯¹æ¯”ã€‚

    stratify=y
    è¿™ä¸ªå‚æ•°è¡¨ç¤ºæŒ‰ç…§ y ä¸­çš„æ ‡ç­¾è¿›è¡Œåˆ†å±‚æŠ½æ ·ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­å„ç±»åˆ«çš„
    æ¯”ä¾‹ä¼šä¸åŸå§‹æ•°æ®ä¸­çš„ç±»åˆ«æ¯”ä¾‹ä¿æŒä¸€è‡´ã€‚è¿™å¯¹äºç±»åˆ«ä¸å¹³è¡¡çš„æ•°æ®é›†å°¤ä¸ºé‡è¦ï¼Œå¯ä»¥
    é¿å…æŸä¸€ç±»åˆ«åœ¨åˆ’åˆ†æ—¶è¢«ä¸¥é‡ä½ä¼°æˆ–è¿‡é‡‡æ ·ã€‚
    """

    # æ ‡å‡†åŒ–ï¼šåªåœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼Œå†å°†ç›¸åŒçš„å˜æ¢åº”ç”¨åˆ°æµ‹è¯•é›†ä¸Š
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # è½¬æ¢ä¸º Tensor
    X_train = torch.tensor(X_train, dtype=torch.float32)
    X_test = torch.tensor(X_test, dtype=torch.float32)
    y_train = torch.tensor(y_train, dtype=torch.long)
    y_test = torch.tensor(y_test, dtype=torch.long)

    # æ„é€  DataLoader
    train_dataset = TensorDataset(X_train, y_train)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

    # æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
    model = IrisNet().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    # è®­ç»ƒ
    num_epochs = 100
    for epoch in range(1, num_epochs + 1):
        loss = train(model, train_loader, criterion, optimizer, device)
        if epoch % 10 == 0:
            print(f"Epoch [{epoch:3d}/{num_epochs}], Loss: {loss:.4f}")

    # è¯„ä¼°
    test_acc, test_pred = evaluate(model, X_test, y_test, device)
    print(f"\nâœ… Test Accuracy: {test_acc * 100:.2f}%")

    # ç¤ºä¾‹é¢„æµ‹
    sample = X_test[0].unsqueeze(0)
    sample_pred = model(sample.to(device))
    pred_class = torch.argmax(sample_pred, dim=1).item()
    print(f"ğŸ” Sample Prediction: True = {y_test[0].item()}, Predicted = {pred_class}")


if __name__ == "__main__":
    main()

```

> äº‘è™šæ‹Ÿæœºè¿è¡Œç»“æœï¼š
>
> <img src="https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250223151816482.png" alt="image-20250223151816482" style="zoom:50%;" />



**ä»£ç è¯´æ˜ï¼š**

1. **æ•°æ®å‡†å¤‡**ï¼š
   - ä½¿ç”¨scikit-learnåŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†
   - å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆ80%ï¼‰å’Œæµ‹è¯•é›†ï¼ˆ20%ï¼‰
   - ä½¿ç”¨æ ‡å‡†åŒ–å¤„ç†ï¼ˆStandardScalerï¼‰å¯¹ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–

2. **ç¥ç»ç½‘ç»œç»“æ„**ï¼š
   - è¾“å…¥å±‚ï¼š4ä¸ªç¥ç»å…ƒï¼ˆå¯¹åº”4ä¸ªç‰¹å¾ï¼‰
   - éšè—å±‚ï¼š10ä¸ªç¥ç»å…ƒï¼ˆä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°ï¼‰
   - è¾“å‡ºå±‚ï¼š3ä¸ªç¥ç»å…ƒï¼ˆå¯¹åº”3ä¸ªç±»åˆ«ï¼‰

3. **è®­ç»ƒé…ç½®**ï¼š
   - ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ˆCrossEntropyLossï¼‰
   - ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼ˆå­¦ä¹ ç‡0.01ï¼‰
   - è®­ç»ƒ100ä¸ªepoch

4. **è®­ç»ƒè¿‡ç¨‹**ï¼š
   - æ¯ä¸ªepochè®°å½•æŸå¤±å’Œå‡†ç¡®ç‡
   - æ¯10ä¸ªepochæ‰“å°è®­ç»ƒè¿›åº¦

5. **è¯„ä¼°ä¸é¢„æµ‹**ï¼š
   - æœ€ç»ˆåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹å‡†ç¡®ç‡
   - åŒ…å«ä¸€ä¸ªé¢„æµ‹ç¤ºä¾‹å±•ç¤º

**è¾“å‡ºç¤ºä¾‹ï¼š**

```
$ python iris_neural_network.py 
Epoch [ 10/100], Loss: 0.2363
Epoch [ 20/100], Loss: 0.0899
Epoch [ 30/100], Loss: 0.0614
Epoch [ 40/100], Loss: 0.0634
Epoch [ 50/100], Loss: 0.0498
Epoch [ 60/100], Loss: 0.0492
Epoch [ 70/100], Loss: 0.0492
Epoch [ 80/100], Loss: 0.0451
Epoch [ 90/100], Loss: 0.0479
Epoch [100/100], Loss: 0.0436

âœ… Test Accuracy: 100.00%
ğŸ” Sample Prediction: True = 0, Predicted = 0
```

**æ³¨æ„äº‹é¡¹ï¼š**

1. ç”±äºæ•°æ®é›†è¾ƒå°ï¼Œæ¨¡å‹å¯èƒ½å¾ˆå¿«è¾¾åˆ°100%è®­ç»ƒå‡†ç¡®ç‡
2. å¯ä»¥è°ƒæ•´ä»¥ä¸‹å‚æ•°ä¼˜åŒ–æ€§èƒ½ï¼š
   - éšè—å±‚å¤§å°ï¼ˆ10ï¼‰
   - å­¦ä¹ ç‡ï¼ˆ0.01ï¼‰
   - epochæ•°é‡ï¼ˆ100ï¼‰
   - ä¼˜åŒ–å™¨ï¼ˆå°è¯•SGDç­‰ï¼‰
3. æ·»åŠ æ­£åˆ™åŒ–ï¼ˆå¦‚Dropoutå±‚ï¼‰å¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
4. å¯ä»¥ä½¿ç”¨GPUåŠ é€Ÿï¼ˆå°†æ•°æ®å’Œæ¨¡å‹ç§»åŠ¨åˆ°`cuda`è®¾å¤‡ï¼‰



## 2.3 MNIST å›¾åƒåˆ†ç±»é¡¹ç›®

é€šè¿‡æ‰‹å†™æ•°å­—è¯†åˆ«çš„å®ä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¥ç»ç½‘ç»œçš„å¼ºå¤§ï¼Œä¹Ÿå¯ä»¥æ›´å¥½åœ°ç†è§£å®ƒæ˜¯å¦‚ä½•è¿è¡Œçš„ã€‚

MNISTæ˜¯ä¸€ä¸ªè‘—åçš„æ‰‹å†™ä½“æ•°å­—è¯†åˆ«æ•°æ®é›†ï¼Œè®­ç»ƒé›†åŒ…å«60,000ä¸ªæ ·æœ¬å’Œæ ‡ç­¾ï¼Œæµ‹è¯•é›†åŒ…å«10,000 ä¸ªæ ·æœ¬å’Œæ ‡ç­¾ã€‚å…¶ä¸­æ ·æœ¬ä¸ºä»£è¡¨0~9ä¸­çš„ä¸€ä¸ªæ•°å­—çš„ç°åº¦å›¾ç‰‡ï¼Œå¯¹åº”ä¸€ä¸ªæ‰€ä»£è¡¨æ•°å­—çš„æ ‡ç­¾ï¼Œå›¾ç‰‡å¤§å°28*28ï¼Œä¸”æ•°å­—å‡ºç°åœ¨å›¾ç‰‡æ­£ä¸­é—´ã€‚

**æ•°æ®é›†**: æ•°æ®é›† MNISTã€‚

**æ•°æ®é¢„å¤„ç†**: å› ä¸º MNIST å›¾åƒæ˜¯å•é€šé“ï¼ˆç°åº¦å›¾ï¼‰ï¼Œè€Œ ResNet18 æ¨¡å‹æ˜¯ä¸º RGB å›¾åƒè®¾è®¡çš„ï¼ŒæœŸæœ›è¾“å…¥çš„é€šé“æ•°ä¸º 3ï¼Œæ‰€ä»¥è°ƒæ•´æ•°æ®é¢„å¤„ç†çš„éƒ¨åˆ†ã€‚

**è¾“å‡ºå±‚**: å°†æ¨¡å‹è¾“å‡ºå±‚ä¸º 10 ç±»ï¼Œå› ä¸º MNIST æœ‰ 10 ä¸ªç±»åˆ«ï¼ˆ0 åˆ° 9ï¼‰ã€‚

**æ¨¡å‹è°ƒæ•´**: ä½¿ç”¨çš„æ¨¡å‹ä»ç„¶æ˜¯ `ResNet18`ï¼Œä½†éœ€è¦è€ƒè™‘åˆ° MNIST æ˜¯å•é€šé“å›¾åƒï¼Œè°ƒæ•´è¾“å…¥å±‚ã€‚

```python
import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import matplotlib.pyplot as plt
import numpy as np
import time

def main():
    # 1. æ•°æ®å¢å¼º + é¢„å¤„ç†
    transform_train = transforms.Compose([
        transforms.RandomRotation(10),  # éšæœºæ—‹è½¬
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))  # MNIST æ˜¯å•é€šé“ï¼Œä½¿ç”¨ (0.5,) æ¥è§„èŒƒåŒ–
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])

    # åŠ è½½ MNIST æ•°æ®é›†
    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)

    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)
    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2, pin_memory=True)

    classes = [str(i) for i in range(10)]  # MNIST ç±»åˆ«æ˜¯ 0 åˆ° 9

    # 2. è®¾ç½®è®¾å¤‡å’Œæ¨¡å‹
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    print("Using device:", device)

    # åŠ è½½é¢„å®šä¹‰çš„ ResNet18 å¹¶ä¿®æ”¹è¾“å…¥å±‚å’Œè¾“å‡ºå±‚
    net = models.resnet18(weights=None)
    # ä¿®æ”¹è¾“å…¥å±‚çš„ç¬¬ä¸€ä¸ªå·ç§¯å±‚ï¼Œä½¿å…¶æ¥å—å•é€šé“ï¼ˆ1é€šé“ç°åº¦å›¾åƒï¼‰
    net.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    net.fc = nn.Linear(net.fc.in_features, 10)  # MNIST 10 ç±»
    net.to(device)

    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

    # 3. è®­ç»ƒè¿‡ç¨‹
    best_loss = float('inf')
    patience = 10  # æé«˜è€å¿ƒ
    patience_counter = 0

    start_time = time.time()
    print("Starting training with early stopping...")
    for epoch in range(800):  # å¯é€‚å½“å¢å¤§ epoch
        net.train()
        epoch_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            if i % 100 == 99:
                print(f"[{epoch + 1}, {i + 1:5d}] loss: {loss.item():.3f}")

        avg_loss = epoch_loss / len(trainloader)
        print(f"[{epoch+1}] Avg Loss: {avg_loss:.3f}")

        if avg_loss < best_loss - 1e-4:
            best_loss = avg_loss
            patience_counter = 0
        else:
            patience_counter += 1
            print(f"No improvement. Patience: {patience_counter}/{patience}")
            if patience_counter >= patience:
                print("Early stopping triggered.")
                break

    end_time = time.time()
    execution_time_minutes = (end_time - start_time) / 60
    print(f"âœ… Training completed in {execution_time_minutes:.2f} minutes.")

    # ä¿å­˜æ¨¡å‹
    torch.save(net.state_dict(), './resnet18_mnist.pth')

    # 4. æµ‹è¯•å‡†ç¡®ç‡
    correct = 0
    total = 0
    net.eval()
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f"Accuracy on test images: {100 * correct / total:.2f}%")

    # æ¯ç±»å‡†ç¡®ç‡
    class_correct = list(0. for _ in range(10))
    class_total = list(0. for _ in range(10))
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            c = (predicted == labels).squeeze()
            for i in range(len(labels)):
                class_correct[labels[i]] += c[i].item()
                class_total[labels[i]] += 1

    for i in range(10):
        print(f'Accuracy of {classes[i]:5s}: {100 * class_correct[i] / class_total[i]:.2f}%')

    # --- å¯è§†åŒ–é¢„æµ‹ ---

    def imshow_grid(images, labels, preds=None, classes=None, rows=8, cols=8):
        images = images.cpu() / 2 + 0.5  # unnormalize
        npimg = images.numpy()
        fig, axes = plt.subplots(rows, cols, figsize=(cols * 1.5, rows * 1.5))
        for i in range(rows * cols):
            r, c = divmod(i, cols)
            ax = axes[r, c]
            img = np.transpose(npimg[i], (1, 2, 0))
            ax.imshow(img.squeeze(), cmap="gray")
            title = f'{classes[labels[i]]}'
            if preds is not None:
                title += f'\nâ†’ {classes[preds[i]]}'
            ax.set_title(title, fontsize=8)
            ax.axis('off')
        plt.tight_layout()
        plt.show()

    # è·å–ä¸€æ‰¹å›¾åƒç”¨äºæ˜¾ç¤º
    dataiter = iter(testloader)
    images, labels = next(dataiter)
    while images.size(0) < 64:
        more_images, more_labels = next(dataiter)
        images = torch.cat([images, more_images], dim=0)
        labels = torch.cat([labels, more_labels], dim=0)
    images = images[:64]
    labels = labels[:64]

    # é¢„æµ‹
    net.eval()
    with torch.no_grad():
        outputs = net(images.to(device))
        _, predicted = torch.max(outputs, 1)

    # æ˜¾ç¤ºå›¾åƒç½‘æ ¼
    imshow_grid(images, labels, predicted.cpu(), classes=classes, rows=8, cols=8)

if __name__ == "__main__":
    import torch.multiprocessing
    torch.multiprocessing.set_start_method('spawn', force=True)
    main()


```



è¿è¡Œæœºå™¨

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/202507261935350.png" alt="b452b39cfb47eb8bf5b640c828b6b71b" style="zoom:50%;" />



è¿è¡Œç»“æœ

```
/Users/hfyan/miniconda3/bin/python /Users/hfyan/git/2025spring-cs201/LLM/mnist_resnet18.py 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [02:52<00:00, 57.6kB/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 97.2kB/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:04<00:00, 374kB/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 6.74kB/s]
Using device: mps
Starting training with early stopping...
/Users/hfyan/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.
  warnings.warn(warn_msg)
[1,   100] loss: 0.136
[1,   200] loss: 0.132
[1,   300] loss: 0.035
[1,   400] loss: 0.098
[1] Avg Loss: 0.150
[2,   100] loss: 0.137
[2,   200] loss: 0.030
[2,   300] loss: 0.030
[2,   400] loss: 0.015
[2] Avg Loss: 0.052
[3,   100] loss: 0.018
[3,   200] loss: 0.105
[3,   300] loss: 0.078
[3,   400] loss: 0.026
[3] Avg Loss: 0.039
[4,   100] loss: 0.032
[4,   200] loss: 0.056
[4,   300] loss: 0.008
[4,   400] loss: 0.013
[4] Avg Loss: 0.031
[5,   100] loss: 0.003
[5,   200] loss: 0.025
[5,   300] loss: 0.029
[5,   400] loss: 0.022
[5] Avg Loss: 0.027
[6,   100] loss: 0.041
[6,   200] loss: 0.022
[6,   300] loss: 0.047
[6,   400] loss: 0.005
[6] Avg Loss: 0.023
[7,   100] loss: 0.039
[7,   200] loss: 0.000
[7,   300] loss: 0.022
[7,   400] loss: 0.014
[7] Avg Loss: 0.018
[8,   100] loss: 0.001
[8,   200] loss: 0.044
[8,   300] loss: 0.021
[8,   400] loss: 0.002
[8] Avg Loss: 0.019
No improvement. Patience: 1/10
[9,   100] loss: 0.002
[9,   200] loss: 0.020
[9,   300] loss: 0.002
[9,   400] loss: 0.007
[9] Avg Loss: 0.017
[10,   100] loss: 0.027
[10,   200] loss: 0.034
[10,   300] loss: 0.031
[10,   400] loss: 0.004
[10] Avg Loss: 0.016
[11,   100] loss: 0.003
[11,   200] loss: 0.004
[11,   300] loss: 0.005
[11,   400] loss: 0.003
[11] Avg Loss: 0.015
[12,   100] loss: 0.011
[12,   200] loss: 0.000
[12,   300] loss: 0.031
[12,   400] loss: 0.003
[12] Avg Loss: 0.015
No improvement. Patience: 1/10
[13,   100] loss: 0.002
[13,   200] loss: 0.002
[13,   300] loss: 0.002
[13,   400] loss: 0.019
[13] Avg Loss: 0.013
[14,   100] loss: 0.019
[14,   200] loss: 0.004
[14,   300] loss: 0.025
[14,   400] loss: 0.003
[14] Avg Loss: 0.013
No improvement. Patience: 1/10
[15,   100] loss: 0.003
[15,   200] loss: 0.001
[15,   300] loss: 0.011
[15,   400] loss: 0.056
[15] Avg Loss: 0.013
[16,   100] loss: 0.034
[16,   200] loss: 0.008
[16,   300] loss: 0.001
[16,   400] loss: 0.003
[16] Avg Loss: 0.011
[17,   100] loss: 0.008
[17,   200] loss: 0.001
[17,   300] loss: 0.001
[17,   400] loss: 0.001
[17] Avg Loss: 0.011
[18,   100] loss: 0.009
[18,   200] loss: 0.015
[18,   300] loss: 0.002
[18,   400] loss: 0.036
[18] Avg Loss: 0.013
No improvement. Patience: 1/10
[19,   100] loss: 0.019
[19,   200] loss: 0.001
[19,   300] loss: 0.023
[19,   400] loss: 0.005
[19] Avg Loss: 0.011
[20,   100] loss: 0.002
[20,   200] loss: 0.007
[20,   300] loss: 0.007
[20,   400] loss: 0.005
[20] Avg Loss: 0.011
No improvement. Patience: 1/10
[21,   100] loss: 0.001
[21,   200] loss: 0.008
[21,   300] loss: 0.012
[21,   400] loss: 0.005
[21] Avg Loss: 0.011
[22,   100] loss: 0.007
[22,   200] loss: 0.001
[22,   300] loss: 0.001
[22,   400] loss: 0.002
[22] Avg Loss: 0.011
No improvement. Patience: 1/10
[23,   100] loss: 0.003
[23,   200] loss: 0.002
[23,   300] loss: 0.001
[23,   400] loss: 0.014
[23] Avg Loss: 0.011
No improvement. Patience: 2/10
[24,   100] loss: 0.003
[24,   200] loss: 0.001
[24,   300] loss: 0.003
[24,   400] loss: 0.002
[24] Avg Loss: 0.010
[25,   100] loss: 0.016
[25,   200] loss: 0.002
[25,   300] loss: 0.010
[25,   400] loss: 0.000
[25] Avg Loss: 0.009
[26,   100] loss: 0.001
[26,   200] loss: 0.002
[26,   300] loss: 0.006
[26,   400] loss: 0.021
[26] Avg Loss: 0.008
[27,   100] loss: 0.002
[27,   200] loss: 0.002
[27,   300] loss: 0.017
[27,   400] loss: 0.000
[27] Avg Loss: 0.010
No improvement. Patience: 1/10
[28,   100] loss: 0.001
[28,   200] loss: 0.012
[28,   300] loss: 0.009
[28,   400] loss: 0.000
[28] Avg Loss: 0.008
No improvement. Patience: 2/10
[29,   100] loss: 0.001
[29,   200] loss: 0.008
[29,   300] loss: 0.009
[29,   400] loss: 0.031
[29] Avg Loss: 0.010
No improvement. Patience: 3/10
[30,   100] loss: 0.038
[30,   200] loss: 0.001
[30,   300] loss: 0.031
[30,   400] loss: 0.001
[30] Avg Loss: 0.011
No improvement. Patience: 4/10
[31,   100] loss: 0.017
[31,   200] loss: 0.013
[31,   300] loss: 0.029
[31,   400] loss: 0.032
[31] Avg Loss: 0.010
No improvement. Patience: 5/10
[32,   100] loss: 0.002
[32,   200] loss: 0.000
[32,   300] loss: 0.003
[32,   400] loss: 0.001
[32] Avg Loss: 0.009
No improvement. Patience: 6/10
[33,   100] loss: 0.009
[33,   200] loss: 0.018
[33,   300] loss: 0.001
[33,   400] loss: 0.007
[33] Avg Loss: 0.010
No improvement. Patience: 7/10
[34,   100] loss: 0.001
[34,   200] loss: 0.001
[34,   300] loss: 0.001
[34,   400] loss: 0.011
[34] Avg Loss: 0.010
No improvement. Patience: 8/10
[35,   100] loss: 0.004
[35,   200] loss: 0.005
[35,   300] loss: 0.009
[35,   400] loss: 0.010
[35] Avg Loss: 0.011
No improvement. Patience: 9/10
[36,   100] loss: 0.001
[36,   200] loss: 0.004
[36,   300] loss: 0.013
[36,   400] loss: 0.007
[36] Avg Loss: 0.008
[37,   100] loss: 0.008
[37,   200] loss: 0.003
[37,   300] loss: 0.007
[37,   400] loss: 0.002
[37] Avg Loss: 0.010
No improvement. Patience: 1/10
[38,   100] loss: 0.002
[38,   200] loss: 0.002
[38,   300] loss: 0.011
[38,   400] loss: 0.004
[38] Avg Loss: 0.009
No improvement. Patience: 2/10
[39,   100] loss: 0.006
[39,   200] loss: 0.003
[39,   300] loss: 0.002
[39,   400] loss: 0.001
[39] Avg Loss: 0.008
No improvement. Patience: 3/10
[40,   100] loss: 0.000
[40,   200] loss: 0.012
[40,   300] loss: 0.011
[40,   400] loss: 0.001
[40] Avg Loss: 0.009
No improvement. Patience: 4/10
[41,   100] loss: 0.010
[41,   200] loss: 0.008
[41,   300] loss: 0.006
[41,   400] loss: 0.002
[41] Avg Loss: 0.008
No improvement. Patience: 5/10
[42,   100] loss: 0.005
[42,   200] loss: 0.003
[42,   300] loss: 0.014
[42,   400] loss: 0.005
[42] Avg Loss: 0.010
No improvement. Patience: 6/10
[43,   100] loss: 0.010
[43,   200] loss: 0.000
[43,   300] loss: 0.012
[43,   400] loss: 0.002
[43] Avg Loss: 0.008
No improvement. Patience: 7/10
[44,   100] loss: 0.001
[44,   200] loss: 0.004
[44,   300] loss: 0.035
[44,   400] loss: 0.000
[44] Avg Loss: 0.011
No improvement. Patience: 8/10
[45,   100] loss: 0.002
[45,   200] loss: 0.014
[45,   300] loss: 0.010
[45,   400] loss: 0.014
[45] Avg Loss: 0.010
No improvement. Patience: 9/10
[46,   100] loss: 0.001
[46,   200] loss: 0.076
[46,   300] loss: 0.001
[46,   400] loss: 0.004
[46] Avg Loss: 0.009
No improvement. Patience: 10/10
Early stopping triggered.
âœ… Training completed in 27.72 minutes.
Accuracy on test images: 99.57%
Accuracy of 0    : 99.59%
Accuracy of 1    : 99.91%
Accuracy of 2    : 99.71%
Accuracy of 3    : 99.80%
Accuracy of 4    : 99.49%
Accuracy of 5    : 99.33%
Accuracy of 6    : 99.37%
Accuracy of 7    : 99.22%
Accuracy of 8    : 99.90%
Accuracy of 9    : 99.31%

Process finished with exit code 0

```



<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/202507261936331.png" alt="22485e1e277b7dfea954fe0cd8a1af4f" style="zoom:50%;" />



**Q: net.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) ?**

> æ˜¯çš„ï¼Œ`net.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)`è¿™ä¸€è¡Œä»£ç æ˜¯ç”¨äºä¿®æ”¹ ResNet18 çš„è¾“å…¥å±‚ï¼Œä½¿å…¶é€‚åº” MNIST æ•°æ®é›†ã€‚
>
> **è§£é‡Šï¼š**
>
> - **MNIST å›¾åƒ** æ˜¯ç°åº¦å›¾ï¼ˆå•é€šé“ï¼‰ï¼Œæ‰€ä»¥è¾“å…¥å›¾åƒçš„é€šé“æ•°æ˜¯ 1ï¼Œè€ŒåŸå§‹çš„ ResNet18 æ¨¡å‹æ˜¯ä¸º RGB å›¾åƒè®¾è®¡çš„ï¼ŒæœŸæœ›è¾“å…¥çš„é€šé“æ•°ä¸º 3ã€‚
> - **`nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)`** è¿™ä¸€è¡Œå°±æ˜¯ä¿®æ”¹åŸæ¥ `ResNet18` ä¸­ç¬¬ä¸€ä¸ªå·ç§¯å±‚ï¼ˆ`conv1`ï¼‰çš„å®šä¹‰ï¼Œä»¥ä½¿å…¶èƒ½æ¥æ”¶å•é€šé“çš„ç°åº¦å›¾åƒï¼ˆ`1` é€šé“ï¼‰ã€‚
>   - **`1`**: è¾“å…¥å›¾åƒçš„é€šé“æ•°ï¼ˆå³ MNIST å›¾åƒçš„ç°åº¦é€šé“æ•°ï¼‰ã€‚
>   - **`64`**: è¾“å‡ºçš„å·ç§¯é€šé“æ•°ï¼ˆResNet18 ä¸­é€šå¸¸æ˜¯ 64ï¼‰ã€‚
>   - **`kernel_size=(7, 7)`**: å·ç§¯æ ¸çš„å¤§å°ä¸º 7x7ã€‚è¿™ä¸ªå€¼ä¸åŸå§‹ ResNet18 ä¸­çš„è®¾ç½®ä¸€è‡´ã€‚
>   - **`stride=(2, 2)`**: å·ç§¯çš„æ­¥é•¿ä¸º 2ï¼Œè¿™æ„å‘³ç€æ¯æ¬¡å·ç§¯åï¼Œå›¾åƒå°ºå¯¸ä¼šå‡å°‘ä¸€åŠã€‚
>   - **`padding=(3, 3)`**: å¡«å……ä¸º 3ï¼Œä¿æŒè¾“å…¥å›¾åƒçš„å°ºå¯¸åœ¨å·ç§¯åä¸è‡³äºå˜åŒ–å¤ªå¤§ï¼ˆç¡®ä¿å·ç§¯åè¾“å‡ºçš„ç©ºé—´ç»´åº¦é€‚å½“ï¼‰ã€‚
>   - **`bias=False`**: é€šå¸¸åœ¨æ·±åº¦ç½‘ç»œä¸­ï¼Œå¦‚æœä½¿ç”¨äº†æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatchNormï¼‰ç­‰å±‚ï¼Œå·ç§¯å±‚å¯ä»¥å»æ‰åç½®é¡¹ã€‚
>
> **ä¸ºä»€ä¹ˆè¦åšè¿™ä¸€æ­¥ï¼š**
>
> - **è¾“å…¥å±‚é€‚é…**: MNIST å›¾åƒæ˜¯å•é€šé“çš„ï¼ˆç°åº¦å›¾ï¼‰ï¼Œæ‰€ä»¥éœ€è¦å°† `ResNet18` çš„è¾“å…¥å±‚ `conv1` çš„è¾“å…¥é€šé“æ•°ç”± 3 è°ƒæ•´ä¸º 1ã€‚å¦åˆ™ï¼Œå¦‚æœä¿æŒåŸæ ·ï¼Œç½‘ç»œæ— æ³•æ­£ç¡®å¤„ç†å•é€šé“è¾“å…¥å›¾åƒã€‚
> - **å·ç§¯æ ¸å’Œæ­¥é•¿é€‰æ‹©**: `kernel_size`, `stride`, `padding` éƒ½æ˜¯ä¿æŒä¸åŸå§‹ ResNet18 æ¨¡å‹ä¸€è‡´çš„è¶…å‚æ•°ï¼Œç›®çš„æ˜¯å°½é‡ä¿æŒæ¨¡å‹ç»“æ„ä¸å˜ï¼Œä»è€Œç¡®ä¿é¢„æœŸçš„æ•ˆæœã€‚
>
> **æ€»ç»“ï¼š**
>
> è¿™ä¸€ä¿®æ”¹æ˜¯å¿…é¡»çš„ï¼Œå› ä¸º MNIST æ˜¯ç°åº¦å›¾åƒï¼Œè€Œ ResNet18 æ˜¯ä¸º RGB å›¾åƒè®¾è®¡çš„ï¼Œå¿…é¡»ä¿®æ”¹è¾“å…¥é€šé“æ•°æ‰èƒ½æ­£ç¡®å¤„ç† MNIST æ•°æ®é›†ã€‚å¦‚æœä½ ç”¨å…¶ä»–æ•°æ®é›†ï¼ˆæ¯”å¦‚ CIFAR-10 æˆ–æ›´å¤§å°ºå¯¸çš„å›¾åƒï¼‰ï¼Œè¿™éƒ¨åˆ†çš„ä¿®æ”¹å°±ä¸éœ€è¦ã€‚



## 2.4 CIFAR-10 å›¾åƒåˆ†ç±»é¡¹ç›®

ç”¨åˆ°çš„æ•°æ®é›†æ˜¯ The CIFAR-10 datasetï¼Œhttps://www.cs.toronto.edu/~kriz/cifar.html

> The CIFAR-10 and CIFAR-100 datasets are labeled subsets of the [80 million tiny images](http://people.csail.mit.edu/torralba/tinyimages/) dataset. CIFAR-10 and CIFAR-100 were created by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
>
> **The CIFAR-10 dataset**
>
> The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.
>
> The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.
>
> Here are the classes in the dataset, as well as 10 random images from each:
>
> | airplane   | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane10.png) |
> | ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
> | automobile | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile10.png) |
> | bird       | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird10.png) |
> | cat        | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat10.png) |
> | deer       | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer10.png) |
> | dog        | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog10.png) |
> | frog       | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog10.png) |
> | horse      | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse10.png) |
> | ship       | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship10.png) |
> | truck      | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck10.png) |
>
> The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. "Automobile" includes sedans, SUVs, things of that sort. "Truck" includes only big trucks. Neither includes pickup trucks.
>
> ### Download
>
> If you're going to use this dataset, please cite the tech report at the bottom of this page. 
>
> | Version                                                      | Size   | md5sum                           |
> | ------------------------------------------------------------ | ------ | -------------------------------- |
> | [CIFAR-10 python version](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz) | 163 MB | c58f30108f718f92721af3b95e74349a |
> | [CIFAR-10 Matlab version](https://www.cs.toronto.edu/~kriz/cifar-10-matlab.tar.gz) | 175 MB | 70270af85842c9e89bb428ec9976c926 |
> | [CIFAR-10 binary version (suitable for C programs)](https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz) | 162 MB | c32a1d4ab5d03f1284b67883e8d87530 |
>
> ### Baseline results
>
> You can find some baseline replicable results on this dataset [on the project page for cuda-convnet](http://code.google.com/p/cuda-convnet/). These results were obtained with a convolutional neural network. Briefly, they are 18% test error without data augmentation and 11% with. Additionally, [Jasper Snoek](http://www.cs.toronto.edu/~jasper/) has a [new paper](http://hips.seas.harvard.edu/content/practical-bayesian-optimization-machine-learning-algorithms) in which he used Bayesian hyperparameter optimization to find nice settings of the weight decay and other hyperparameters, which allowed him to obtain a test error rate of 15% (without data augmentation) using the architecture of the net that got 18%.



æœ¬æ„æ˜¯è¿è¡Œ https://www.runoob.com/pytorch/pytorch-image-classification.html

ä½†æ˜¯å‡†ç¡®ç‡æ²¡æœ‰è¾¾åˆ°baselineçš„82%ï¼Œæ‰€ä»¥åšäº†æ”¹è¿›ã€‚

### ç”¨ResNet18ç»“æ„ï¼Œé‡æ–°è®­ç»ƒï¼Œåšäº†æ•°æ®å¢å¼º

```python
import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import matplotlib.pyplot as plt
import numpy as np
import time

def main():
    # 1. æ•°æ®å¢å¼º + é¢„å¤„ç†
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),  # éšæœºæ—‹è½¬
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # è‰²å½©è°ƒæ•´
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)

    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2, pin_memory=True)

    classes = ('plane', 'car', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck')

    # 2. è®¾ç½®è®¾å¤‡å’Œæ¨¡å‹
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    print("Using device:", device)

    # åŠ è½½é¢„å®šä¹‰çš„ ResNet18 å¹¶ä¿®æ”¹è¾“å‡ºå±‚
    net = models.resnet18(weights=None)
    net.fc = nn.Linear(net.fc.in_features, 10)  # CIFAR10 10 ç±»
    net.to(device)

    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

    # 3. è®­ç»ƒè¿‡ç¨‹
    best_loss = float('inf')
    patience = 10 # æé«˜è€å¿ƒ
    patience_counter = 0

    start_time = time.time()
    print("Starting training with early stopping...")
    for epoch in range(800):  # å¯é€‚å½“å¢å¤§ epoch
        net.train()
        epoch_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            if i % 100 == 99:
                print(f"[{epoch + 1}, {i + 1:5d}] loss: {loss.item():.3f}")

        avg_loss = epoch_loss / len(trainloader)
        print(f"[{epoch+1}] Avg Loss: {avg_loss:.3f}")

        if avg_loss < best_loss - 1e-4:
            best_loss = avg_loss
            patience_counter = 0
        else:
            patience_counter += 1
            print(f"No improvement. Patience: {patience_counter}/{patience}")
            if patience_counter >= patience:
                print("Early stopping triggered.")
                break



    end_time = time.time()
    execution_time_minutes = (end_time - start_time) / 60
    print(f"âœ… Training completed in {execution_time_minutes:.2f} minutes.")


    # ä¿å­˜æ¨¡å‹
    torch.save(net.state_dict(), './resnet18_cifar10_data_augument.pth')

    # 4. æµ‹è¯•å‡†ç¡®ç‡
    correct = 0
    total = 0
    net.eval()
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f"Accuracy on test images: {100 * correct / total:.2f}%")

    # æ¯ç±»å‡†ç¡®ç‡
    class_correct = list(0. for _ in range(10))
    class_total = list(0. for _ in range(10))
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            c = (predicted == labels).squeeze()
            for i in range(len(labels)):
                class_correct[labels[i]] += c[i].item()
                class_total[labels[i]] += 1

    for i in range(10):
        print(f'Accuracy of {classes[i]:5s}: {100 * class_correct[i] / class_total[i]:.2f}%')

    # --- å¯è§†åŒ–é¢„æµ‹ ---

    def imshow_grid(images, labels, preds=None, classes=None, rows=8, cols=8):
        images = images.cpu() / 2 + 0.5  # unnormalize
        npimg = images.numpy()
        fig, axes = plt.subplots(rows, cols, figsize=(cols * 1.5, rows * 1.5))
        for i in range(rows * cols):
            r, c = divmod(i, cols)
            ax = axes[r, c]
            img = np.transpose(npimg[i], (1, 2, 0))
            ax.imshow(img)
            title = f'{classes[labels[i]]}'
            if preds is not None:
                title += f'\nâ†’ {classes[preds[i]]}'
            ax.set_title(title, fontsize=8)
            ax.axis('off')
        plt.tight_layout()
        plt.show()

    # è·å–ä¸€æ‰¹å›¾åƒç”¨äºæ˜¾ç¤º
    dataiter = iter(testloader)
    images, labels = next(dataiter)
    while images.size(0) < 64:
        more_images, more_labels = next(dataiter)
        images = torch.cat([images, more_images], dim=0)
        labels = torch.cat([labels, more_labels], dim=0)
    images = images[:64]
    labels = labels[:64]

    # é¢„æµ‹
    net.eval()
    with torch.no_grad():
        outputs = net(images.to(device))
        _, predicted = torch.max(outputs, 1)

    # æ˜¾ç¤ºå›¾åƒç½‘æ ¼
    imshow_grid(images, labels, predicted.cpu(), classes=classes, rows=8, cols=8)

if __name__ == "__main__":
    import torch.multiprocessing
    torch.multiprocessing.set_start_method('spawn', force=True)
    main()

```



è¿è¡Œç»“æœå¦‚ä¸‹ï¼š

```
/Users/hfyan/miniconda3/bin/python /Users/hfyan/Desktop/LLMs-from-scratch-main/runoob/pytorch-image-classification/image_classification-ResNet18-RandomCropFlipLR_Cosine.py 
Using device: mps
Starting training with early stopping...
[1,   100] loss: 1.752
[1,   200] loss: 1.675
[1,   300] loss: 1.654
[1] Avg Loss: 1.806
[2,   100] loss: 1.497
[2,   200] loss: 1.459
[2,   300] loss: 1.453
[2] Avg Loss: 1.520
[3,   100] loss: 1.534
[3,   200] loss: 1.383
[3,   300] loss: 1.167
[3] Avg Loss: 1.372
[4,   100] loss: 1.390
[4,   200] loss: 1.221
[4,   300] loss: 1.238
[4] Avg Loss: 1.244
[5,   100] loss: 1.089
[5,   200] loss: 1.020
[5,   300] loss: 1.133
[5] Avg Loss: 1.159
......
No improvement. Patience: 1/10
[192,   100] loss: 0.187
[192,   200] loss: 0.293
[192,   300] loss: 0.356
[192] Avg Loss: 0.302
[193,   100] loss: 0.223
[193,   200] loss: 0.348
[193,   300] loss: 0.309
[193] Avg Loss: 0.301
[194,   100] loss: 0.303
[194,   200] loss: 0.219
[194,   300] loss: 0.280
[194] Avg Loss: 0.304
No improvement. Patience: 1/10
[195,   100] loss: 0.279
[195,   200] loss: 0.296
[195,   300] loss: 0.313
[195] Avg Loss: 0.296
[196,   100] loss: 0.254
[196,   200] loss: 0.385
[196,   300] loss: 0.280
[196] Avg Loss: 0.300
No improvement. Patience: 1/10
[197,   100] loss: 0.216
[197,   200] loss: 0.298
[197,   300] loss: 0.290
[197] Avg Loss: 0.298
No improvement. Patience: 2/10
[198,   100] loss: 0.267
[198,   200] loss: 0.218
[198,   300] loss: 0.367
[198] Avg Loss: 0.290
[199,   100] loss: 0.270
[199,   200] loss: 0.240
[199,   300] loss: 0.351
[199] Avg Loss: 0.301
No improvement. Patience: 1/10
[200,   100] loss: 0.251
[200,   200] loss: 0.227
[200,   300] loss: 0.302
[200] Avg Loss: 0.299
No improvement. Patience: 2/10
[201,   100] loss: 0.348
[201,   200] loss: 0.301
[201,   300] loss: 0.193
[201] Avg Loss: 0.299
No improvement. Patience: 3/10
[202,   100] loss: 0.313
[202,   200] loss: 0.329
[202,   300] loss: 0.305
[202] Avg Loss: 0.295
No improvement. Patience: 4/10
[203,   100] loss: 0.266
[203,   200] loss: 0.254
[203,   300] loss: 0.307
[203] Avg Loss: 0.294
No improvement. Patience: 5/10
[204,   100] loss: 0.372
[204,   200] loss: 0.295
[204,   300] loss: 0.348
[204] Avg Loss: 0.300
No improvement. Patience: 6/10
[205,   100] loss: 0.392
[205,   200] loss: 0.353
[205,   300] loss: 0.306
[205] Avg Loss: 0.296
No improvement. Patience: 7/10
[206,   100] loss: 0.262
[206,   200] loss: 0.213
[206,   300] loss: 0.396
[206] Avg Loss: 0.293
No improvement. Patience: 8/10
[207,   100] loss: 0.293
[207,   200] loss: 0.204
[207,   300] loss: 0.337
[207] Avg Loss: 0.291
No improvement. Patience: 9/10
[208,   100] loss: 0.413
[208,   200] loss: 0.294
[208,   300] loss: 0.315
[208] Avg Loss: 0.295
No improvement. Patience: 10/10
Early stopping triggered.
âœ… Training completed in 79.91 minutes.
Accuracy on test images: 83.57%
Accuracy of plane: 83.70%
Accuracy of car  : 92.20%
Accuracy of bird : 78.70%
Accuracy of cat  : 60.40%
Accuracy of deer : 79.30%
Accuracy of dog  : 77.40%
Accuracy of frog : 90.30%
Accuracy of horse: 92.50%
Accuracy of ship : 88.50%
Accuracy of truck: 92.70%

Process finished with exit code 0
```



### æœ¬éƒ¨åˆ†QA

Q: ä¹‹å‰ç‰ˆæœ¬ `image_classification-ResNet18.py` æ²¡æœ‰baselineå¥½ï¼Ÿ

> ä½ ç°åœ¨çš„ç»“æœæ˜¯ï¼š
>
> - **Test Accuracy:** 76.77%
> - **Test Error Rate:** 100% - 76.77% = **23.23%**
>
> è€Œä½ å¼•ç”¨çš„ **baselineï¼ˆæ¥è‡ª cuda-convnet é¡¹ç›®ï¼‰**æä¾›çš„æ˜¯ï¼š
>
> | æ–¹æ³•                       | Test Error (%) | Test Accuracy (%) |
> | -------------------------- | -------------- | ----------------- |
> | Baseline CNNï¼ˆæ— æ•°æ®å¢å¼ºï¼‰ | **18.0%**      | **82.0%**         |
> | Bayesian ä¼˜åŒ–åï¼ˆæ— å¢å¼ºï¼‰  | **15.0%**      | **85.0%**         |
> | CNNï¼ˆå¸¦æ•°æ®å¢å¼ºï¼‰          | **11.0%**      | **89.0%**         |
>
> ------
>
> ğŸ†š å¯¹æ¯”åˆ†æï¼š
>
> | æŒ‡æ ‡              | ä½ çš„æ¨¡å‹ï¼ˆResNet18ï¼‰ | baseline æœ€å¥½  | å·®è·                |
> | ----------------- | -------------------- | -------------- | ------------------- |
> | Test Accuracy (%) | **76.77%**           | **89.0%**      | â†“ ~12.2%            |
> | ç½‘ç»œç»“æ„          | ResNet18             | è‡ªå®šä¹‰ CNN     | ResNet18 æ˜¯æ›´å¼ºæ¨¡å‹ |
> | æ•°æ®å¢å¼º          | å¦                   | æ˜¯             | âœ… å¯æå‡            |
> | è¶…å‚è°ƒä¼˜          | é»˜è®¤                 | æœ‰ï¼ˆBayesianï¼‰ | âœ… å¯æå‡            |
>
> ------
>
> ğŸ§  ç»“è®ºï¼š
>
> ä½ å½“å‰çš„ç»“æœæ˜¯ **å¾ˆä¸é”™çš„èµ·ç‚¹**ï¼Œè€ƒè™‘åˆ°ï¼š
>
> - ä½¿ç”¨çš„æ˜¯æ ‡å‡† `ResNet18`ï¼Œä¸”æ²¡æœ‰åšä»»ä½• **æ•°æ®å¢å¼º**ï¼›
> - æ²¡æœ‰ç‰¹åˆ«è°ƒ `lr`ã€`weight_decay`ã€`batch_size` ç­‰è¶…å‚æ•°ï¼›
> - æ²¡æœ‰ç”¨é¢å¤– trickï¼Œå¦‚ CosineLRã€Mixupã€Label Smoothing ç­‰ï¼›
> - ç”¨çš„æ˜¯ **MPS åç«¯**ï¼ˆé€Ÿåº¦å¿«ï¼Œä½†å¯èƒ½ç¨³å®šæ€§å’Œ cuda è¿˜å·®ç‚¹ï¼‰ï¼›
>
> **å› æ­¤ï¼Œä½ çš„ç»“æœåŸºæœ¬å’Œâ€œæœªè°ƒä¼˜çš„ ResNet18 åŸºçº¿â€æŒå¹³ã€‚**
>
> ------
>
> âœ… å¦‚ä½•è¿½å¹³ç”šè‡³è¶…è¶Š baselineï¼Ÿ
>
> ä½ çš„ ResNet18 å¾ˆå¼ºï¼Œä½†ä½ å¯ä»¥ç»§ç»­æå‡å‡†ç¡®ç‡ï¼š
>
> | æ–¹æ³•                    | è¯´æ˜                                        |
> | ----------------------- | ------------------------------------------- |
> | âœ… åŠ æ•°æ®å¢å¼º            | `transforms.RandomCrop + HorizontalFlip`    |
> | âœ… è°ƒä¼˜å­¦ä¹ ç‡/è°ƒåº¦å™¨     | ä½¿ç”¨ `CosineAnnealingLR`ã€warmup ç­‰         |
> | âœ… å¼•å…¥æ­£åˆ™åŒ–æŠ€å·§        | åŠ  weight_decayï¼Œdropout æˆ– label smoothing |
> | âœ… æ¢æ›´é«˜æ•ˆæ¨¡å‹ï¼ˆå¯é€‰ï¼‰  | MobileNetV2 æˆ– EfficientNet                 |
> | âœ… åŠ å…¥éªŒè¯é›† early stop | é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæå‡æ³›åŒ–                        |
> | âœ… å¤šè½®è®­ç»ƒ+å†»ç»“å¾®è°ƒ     | å¾®è°ƒ ImageNet æƒé‡éƒ¨åˆ†                      |
>
> ------
>
> æ˜¯å¦éœ€è¦æˆ‘å¸®ä½ **åŠ æ•°æ®å¢å¼ºç‰ˆæœ¬**ï¼Ÿå‡ è¡Œä»£ç å°±å¯ä»¥åœ¨ `transform` ä¸­æ·»åŠ ï¼Œè®© ResNet18 å‘æŒ¥æ›´å¤§æ½œåŠ›ã€‚
>
> ```python
> transform_train = transforms.Compose([
>     transforms.RandomCrop(32, padding=4),
>     transforms.RandomHorizontalFlip(),
>     transforms.ToTensor(),
>     transforms.Normalize(mean, std),
> ])
> ```
>
> æ˜¯å¦éœ€è¦æˆ‘ç›´æ¥ç»™å‡ºå¯¹æ¯”å®Œæ•´è®­ç»ƒè„šæœ¬ï¼Ÿå¯ä»¥ä¸€é”®æ›¿æ¢ä½ å½“å‰ç‰ˆæœ¬ã€‚



Q: transforms.Resize(224), # ResNet è¾“å…¥è¦æ±‚è‡³å°‘224x224ï¼Œç°åœ¨ä¸éœ€è¦äº†ï¼Ÿ

> ä½ é—®å¾—éå¸¸å¥½ï¼Œè¿™æ¶‰åŠåˆ° **ResNet18 ä¸ CIFAR-10 è¾“å…¥å›¾åƒå°ºå¯¸çš„å…¼å®¹æ€§é—®é¢˜**ï¼š
>
> ------
>
> ğŸ” ResNet18 çš„åŸå§‹è®¾è®¡è¾“å…¥å°ºå¯¸
>
> - åŸå§‹ ImageNet é¢„è®­ç»ƒçš„ ResNet18 è¾“å…¥å°ºå¯¸æ˜¯ **224Ã—224**ã€‚
>
> - å› æ­¤å¸¸è§ç”¨æ³•ä¸­é€šå¸¸åŒ…å«ï¼š
>
>   ```python
>   transforms.Resize(256),
>   transforms.CenterCrop(224),
>   ```
>
> ------
>
> âœ… åœ¨ CIFAR-10 ä¸Šä½¿ç”¨ ResNet18 ä¸éœ€è¦ `Resize(224)` çš„æƒ…å†µ
>
> å› ä¸ºä½ åœ¨è®­ç»ƒä¸­ä½¿ç”¨çš„æ˜¯ï¼š
>
> ```python
> models.resnet18(weights=None)
> ```
>
> è¿™è¡¨ç¤ºï¼š
>
> - **æ²¡æœ‰åŠ è½½é¢„è®­ç»ƒæƒé‡**
> - ç½‘ç»œä»å¤´å¼€å§‹è®­ç»ƒ
> - æ‰€ä»¥ä½ å¯ä»¥**è‡ªç”±åœ°æ›¿æ¢ç¬¬ä¸€å±‚å·ç§¯**æˆ–ç›´æ¥ç”¨é»˜è®¤çš„ï¼Œ**ä¸éœ€è¦æ”¾å¤§åˆ° 224Ã—224**
>
> è€Œä¸”ä½ çš„æ•°æ®ï¼ˆCIFAR-10ï¼‰æ˜¯ `32x32`ï¼Œæ‰€ä»¥ torchvision ä¸­å¸¸è§çš„å¤„ç†æ–¹å¼æ˜¯ï¼š
>
> - **ä¿ç•™åŸå§‹å°ºå¯¸**
> - åªä½¿ç”¨ï¼š`RandomCrop(32, padding=4)` + `RandomHorizontalFlip()` å°±è¶³å¤Ÿäº†
>
> ------
>
> ğŸ§  å¦‚æœä½ æƒ³ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
>
> å¦‚æœä½ æ‰“ç®—ä½¿ç”¨ï¼š
>
> ```python
> models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
> ```
>
> åˆ™ä½ å¿…é¡»ï¼š
>
> 1. **è°ƒæ•´è¾“å…¥å›¾åƒå¤§å°åˆ° â‰¥224x224**ï¼š
>
>    ```python
>    transforms.Resize(256),
>    transforms.CenterCrop(224),
>    ```
>
> 2. æˆ–è€…ä¿®æ”¹æ¨¡å‹ç¬¬ä¸€å±‚ï¼š
>
>    å°†ï¼š
>
>    ```python
>    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
>    model.maxpool = nn.Identity()
>    ```
>
> ------
>
> âœ… ç»“è®º
>
> | ç”¨æ³•                                           | æ˜¯å¦éœ€è¦ Resize(224) | è¯´æ˜                |
> | ---------------------------------------------- | -------------------- | ------------------- |
> | `weights=None`ï¼ˆä»å¤´è®­ç»ƒï¼‰                     | âŒ ä¸éœ€è¦             | å¯ç›´æ¥ç”¨ 32Ã—32 å›¾åƒ |
> | `weights=ResNet18_Weights.DEFAULT`ï¼ˆç”¨é¢„è®­ç»ƒï¼‰ | âœ… éœ€è¦               | éœ€Resizeåˆ°224x224   |
>
> ------
>
> æ˜¯å¦éœ€è¦æˆ‘å¸®ä½ å†™ä¸€ä»½ä½¿ç”¨ **é¢„è®­ç»ƒResNet18** + è‡ªåŠ¨è°ƒæ•´å°ºå¯¸çš„å®Œæ•´ä»£ç ç‰ˆæœ¬ï¼Ÿ



Q:  **ç²¾åº¦ (Accuracy)** ä»ç„¶æ¯” **baseline (82%)** ç¨ä½ï¼Œå¯èƒ½çš„åŸå› å’Œæ”¹è¿›å»ºè®®ï¼š

> ä½ çš„è®­ç»ƒç»“æœå·²ç»ç›¸å½“ä¸é”™ï¼Œä½† **ç²¾åº¦ (Accuracy)** ä»ç„¶æ¯” **baseline (82%)** ç¨ä½ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½çš„åŸå› å’Œæ”¹è¿›å»ºè®®ï¼š
>
> ------
>
> âœ… å¯èƒ½çš„åŸå› ï¼š
>
> 1. **æ²¡æœ‰æ•°æ®å¢å¼º**ï¼š
>    - ä½ çš„å½“å‰æ•°æ®å¢å¼ºè®¾ç½®ä¸­ä»…åŒ…å«äº† `RandomCrop` å’Œ `RandomHorizontalFlip`ã€‚è¿™å¯¹ ResNet18 æ¥è¯´å¯èƒ½ä¸å¤Ÿï¼Œå°¤å…¶æ˜¯åœ¨ CIFAR-10 è¿™ç§å°è§„æ¨¡æ•°æ®é›†ä¸Šã€‚Baseline çš„ 82% å‡†ç¡®ç‡æ˜¯ä½¿ç”¨äº† **æ›´å¤šçš„æ•°æ®å¢å¼º**ã€‚
> 2. **å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰è®¾ç½®é—®é¢˜**ï¼š
>    - ä½ çš„å­¦ä¹ ç‡è®¾ç½®æ˜¯ `lr=0.1`ï¼Œè¿™å¯èƒ½ç¨å¾®æœ‰äº›é«˜ï¼Œå¯¼è‡´ç½‘ç»œåœ¨è®­ç»ƒåˆæœŸå°±å‘ç”Ÿäº†éœ‡è¡ï¼Œæ— æ³•æœ‰æ•ˆæ”¶æ•›ã€‚
>    - ä½ å¯ä»¥å°è¯• **é€æ­¥é™ä½å­¦ä¹ ç‡**ï¼Œæ¯”å¦‚ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆ`lr_scheduler`ï¼‰æˆ–è€… **æ›´ä½çš„åˆå§‹å­¦ä¹ ç‡**ï¼ˆå¦‚ 0.01 æˆ– 0.001ï¼‰ã€‚
> 3. **è®­ç»ƒè½®æ•°ï¼ˆEpochsï¼‰ä¸è¶³**ï¼š
>    - è™½ç„¶ä½ è®¾ç½®äº† **100 ä¸ª epoch**ï¼Œä½†æ ¹æ®è®­ç»ƒè¿‡ç¨‹æ¥çœ‹ï¼Œæ¨¡å‹åœ¨ 15 åˆ†é’Ÿå†…åªè®­ç»ƒäº†å¤§çº¦ **24 ä¸ª epoch**ã€‚è¿™æ„å‘³ç€å¯èƒ½è¿˜éœ€è¦æ›´å¤šçš„è®­ç»ƒæ—¶é—´æ¥è¿›ä¸€æ­¥é™ä½æŸå¤±ã€‚
> 4. **æ—©æœŸåœæ­¢ï¼ˆEarly Stoppingï¼‰**ï¼š
>    - æ—©æœŸåœæ­¢å¯èƒ½åœ¨æŸå¤±å°šæœªå®Œå…¨é™ä½æ—¶å°±åœæ­¢äº†è®­ç»ƒã€‚ä½ å¯èƒ½ä¼šå¸Œæœ› **æ”¾å®½æ—©æœŸåœæ­¢çš„æ¡ä»¶**ï¼Œè®©è®­ç»ƒç»§ç»­æ›´å¤š epochï¼Œç›´åˆ°ç²¾åº¦è¾¾åˆ°æ›´é«˜æ°´å¹³ã€‚
>
> ------
>
> âœ… æ”¹è¿›å»ºè®®ï¼š
>
> 1. **å¢åŠ æ•°æ®å¢å¼º**ï¼š
>
> - ä¸ºäº†æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå»ºè®®æ·»åŠ æ›´å¤šçš„æ•°æ®å¢å¼ºæ“ä½œã€‚å¯ä»¥åŠ å…¥ï¼š
>   - `RandomRotation(10)`ï¼šéšæœºæ—‹è½¬å›¾åƒ
>   - `ColorJitter`ï¼šè°ƒæ•´å›¾åƒçš„äº®åº¦ã€å¯¹æ¯”åº¦ç­‰
>   - `RandomAffine`ï¼šéšæœºä»¿å°„å˜æ¢
>
> ä¿®æ”¹åçš„æ•°æ®å¢å¼ºä»£ç ï¼š
>
> ```python
> transform_train = transforms.Compose([
>     transforms.RandomCrop(32, padding=4),
>     transforms.RandomHorizontalFlip(),
>     transforms.RandomRotation(10),  # éšæœºæ—‹è½¬
>     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # è‰²å½©è°ƒæ•´
>     transforms.ToTensor(),
>     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
> ])
> ```
>
> 2. **è°ƒæ•´å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰**ï¼š
>
> - **é€æ­¥é™ä½å­¦ä¹ ç‡**ï¼šå»ºè®®åœ¨è®­ç»ƒæ—¶ **ä½¿ç”¨åŠ¨æ€å­¦ä¹ ç‡**ï¼Œå¯ä»¥å°è¯•æ›´ä½çš„åˆå§‹å­¦ä¹ ç‡ï¼ˆå¦‚ 0.01ï¼‰ï¼Œå¹¶è®¾ç½®ä¸€ä¸ªå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆ`lr_scheduler`ï¼‰æ¥é€æ­¥é™ä½å­¦ä¹ ç‡ã€‚
>
> ```python
> optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)
> scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)  # æ¯20ä¸ªepochå°†å­¦ä¹ ç‡é™ä½50%
> ```
>
> 3. **æ”¾å®½ Early Stopping çš„æ¡ä»¶**ï¼š
>
> - æé«˜ `patience`ï¼ˆä¾‹å¦‚ä» 5 æ”¹ä¸º 10ï¼‰ï¼Œæˆ–è€…å°†åœæ­¢æ¡ä»¶è®¾ä¸ºæ›´å®½æ¾çš„æ ‡å‡†ï¼Œæ¯”å¦‚ **å¢åŠ éªŒè¯é›†å‡†ç¡®ç‡çš„æŒç»­æå‡**ã€‚
>
> ```python
> patience = 10  # æé«˜è€å¿ƒ
> ```
>
> 4. **å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆEpochsï¼‰**ï¼š
>
> - å°è¯•å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆä¾‹å¦‚ 200-300ï¼‰ï¼Œå³ä½¿ä½¿ç”¨æ—©æœŸåœæ­¢ï¼Œå¢åŠ è®­ç»ƒæ—¶é—´ä»ç„¶æœ‰åŠ©äºæ¨¡å‹çš„æ”¶æ•›ã€‚
>
> 5. **æ£€æŸ¥æ•°æ®é›†çš„åˆ’åˆ†**ï¼š
>
> - ç¡®ä¿ä½ åœ¨è®­ç»ƒå’ŒéªŒè¯é›†ä¸Šæ²¡æœ‰æ•°æ®æ³„æ¼ï¼Œå¹¶ä¸”ç¡®ä¿ **æ•°æ®åŠ è½½æ—¶æ²¡æœ‰éšæœºæ€§** å½±å“æ¨¡å‹è¯„ä¼°ï¼ˆæ¯”å¦‚ä½¿ç”¨å›ºå®šçš„éšæœºç§å­ï¼‰ã€‚
>
> 6. **ä½¿ç”¨æ•°æ®å¢å¼ºï¼ˆå¦‚ MixUp æˆ– Cutoutï¼‰**ï¼š
>
> - ä½ å¯ä»¥è¿›ä¸€æ­¥å°è¯• **MixUp**ï¼ˆé€šè¿‡çº¿æ€§æ’å€¼è¿›è¡Œæ•°æ®å¢å¼ºï¼‰æˆ– **Cutout**ï¼ˆéšæœºé®æŒ¡ä¸€éƒ¨åˆ†å›¾åƒï¼‰ç­‰æ–¹æ³•ï¼Œå·²çŸ¥åœ¨ CIFAR-10 ä¸Šè¿™äº›æ–¹æ³•æœ‰åŠ©äºæé«˜å‡†ç¡®ç‡ã€‚
>
> ------
>
> âœ… å°ç»“ï¼š
>
> ä½ çš„æ¨¡å‹è®­ç»ƒå·²ç»éå¸¸æ¥è¿‘ baseline çš„ç»“æœï¼Œç»è¿‡æ•°æ®å¢å¼ºã€é€‚å½“çš„å­¦ä¹ ç‡è°ƒèŠ‚ã€æ›´å¤šè®­ç»ƒè½®æ¬¡ç­‰ä¼˜åŒ–åï¼Œåº”è¯¥èƒ½è¿›ä¸€æ­¥æå‡å‡†ç¡®ç‡ã€‚ä½ è¿˜å¯ä»¥å°è¯• **ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹**ï¼Œæˆ–è€… **è¿ç§»å­¦ä¹ **ï¼Œè¿™äº›æ–¹æ³•èƒ½å¤Ÿè®©æ¨¡å‹æ›´å¿«æ”¶æ•›ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒå°æ•°æ®é›†ä¸Šã€‚
>
> ------
>
> æ˜¯å¦éœ€è¦æˆ‘è¿›ä¸€æ­¥å¸®åŠ©ä½ é›†æˆ **å­¦ä¹ ç‡è°ƒåº¦å™¨**ã€**æ›´å¤šæ•°æ®å¢å¼º**ã€**éªŒè¯é›†è¯„ä¼°**ç­‰åŠŸèƒ½ï¼Œæˆ–è€…ç›´æ¥ç»™å‡ºä¸€ä¸ªå…¨æ–°çš„ç‰ˆæœ¬è¿›è¡Œæ”¹è¿›ï¼Ÿ



> ğŸ”§ æ–°æ”¹è¿›ç‚¹ï¼š
>
> 1. **æ•°æ®å¢å¼º**ï¼š
>    - å¢åŠ äº† `RandomRotation` å’Œ `ColorJitter`ï¼Œå¯ä»¥è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
> 2. **å­¦ä¹ ç‡è°ƒåº¦å™¨**ï¼š
>    - ä½¿ç”¨ `CosineAnnealingLR` æ¥åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼Œå¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒåæœŸæ›´å¹³æ»‘åœ°æ”¶æ•›ã€‚
> 3. **æ—©æœŸåœæ­¢**ï¼š
>    - è°ƒæ•´äº† `patience` ä¸º 10 è½®ï¼Œå…è®¸æ¨¡å‹æœ‰æ›´å¤šæ—¶é—´æ¥æ”¶æ•›ï¼Œé˜²æ­¢è¿‡æ—©åœæ­¢ã€‚
> 4. **è®­ç»ƒæ—¶é—´ç»Ÿè®¡**ï¼š
>    - è¾“å‡ºäº†æ€»è®­ç»ƒæ—¶é—´ï¼Œæ–¹ä¾¿äº†è§£è®­ç»ƒæ¶ˆè€—ã€‚
> 5. **æ¨¡å‹ä¿å­˜**ï¼š
>    - ä¿å­˜äº†è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæ–¹ä¾¿ä»¥ååŠ è½½å’Œæ¨ç†ã€‚
>
> ------
>
> ä¸‹ä¸€æ­¥ï¼š
>
> ä½ å¯ä»¥è¿è¡Œè¿™ä¸ªè„šæœ¬ï¼Œçœ‹æ˜¯å¦èƒ½ **è¶…è¶Š baseline çš„ 82%** å‡†ç¡®ç‡ã€‚å¦‚æœç»“æœä¸ç†æƒ³ï¼Œå¯ä»¥è¿›ä¸€æ­¥è°ƒæ•´è¶…å‚æ•°ã€æ·»åŠ æ›´å¤šçš„æŠ€å·§ï¼ˆå¦‚ MixUpã€Cutoutï¼‰ã€‚



**Q: è§£è¯»ä»£ç **

```
è§£è¯»ï¼š    # æ¯ç±»å‡†ç¡®ç‡
    class_correct = list(0. for _ in range(10))
    class_total = list(0. for _ in range(10))
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            c = (predicted == labels).squeeze()
            for i in range(len(labels)):
                class_correct[labels[i]] += c[i].item()
                class_total[labels[i]] += 1
```

> è¿™æ®µä»£ç ç”¨äºè®¡ç®— **æ¯ä¸€ç±»çš„å‡†ç¡®ç‡**ï¼Œå…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š
>
> ------
>
> ğŸ§© ä»£ç è§£è¯»
>
> ```python
> # åˆ›å»ºä¸¤ä¸ªåˆ—è¡¨ï¼Œåˆ†åˆ«ç”¨äºå­˜å‚¨æ¯ä¸€ç±»çš„æ­£ç¡®é¢„æµ‹æ•°å’Œæ€»æ ·æœ¬æ•°
> class_correct = list(0. for _ in range(10))  # å­˜å‚¨æ¯ä¸ªç±»åˆ«æ­£ç¡®é¢„æµ‹çš„æ¬¡æ•°
> class_total = list(0. for _ in range(10))    # å­˜å‚¨æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
> ```
>
> - `class_correct[i]` ç”¨äºè®°å½•ç±»åˆ« `i` çš„æ­£ç¡®é¢„æµ‹æ¬¡æ•°ã€‚
> - `class_total[i]` ç”¨äºè®°å½•ç±»åˆ« `i` çš„æ€»æ ·æœ¬æ•°ã€‚
>
> `10` æ˜¯ CIFAR-10 æ•°æ®é›†ä¸­çš„ç±»åˆ«æ•°é‡ï¼ˆ10 ç±»ï¼‰ã€‚
>
> ------
>
> ```python
> # ä¸è®¡ç®—æ¢¯åº¦ï¼ŒåŠ é€Ÿæ¨ç†
> with torch.no_grad():
>     for data in testloader:
>         images, labels = data
>         images, labels = images.to(device), labels.to(device)
>         outputs = net(images)
>         _, predicted = torch.max(outputs.data, 1)
> ```
>
> - ä½¿ç”¨ `torch.no_grad()` æ¥ **å…³é—­æ¢¯åº¦è®¡ç®—**ï¼Œè¿™æ ·åœ¨æ¨ç†æ—¶èŠ‚çœå†…å­˜å’Œè®¡ç®—ã€‚
> - `testloader` æ˜¯åŠ è½½æµ‹è¯•é›†çš„æ•°æ®è¿­ä»£å™¨ï¼Œ`images` æ˜¯è¾“å…¥å›¾åƒï¼Œ`labels` æ˜¯å¯¹åº”çš„æ ‡ç­¾ã€‚
> - `outputs = net(images)`ï¼šç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ `net` å¯¹å›¾åƒè¿›è¡Œæ¨ç†ï¼Œå¾—åˆ°æ¯ä¸ªç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡ã€‚
> - `_, predicted = torch.max(outputs.data, 1)`ï¼šé€šè¿‡ `torch.max` è·å– **æ¯å¼ å›¾åƒçš„æœ€å¤§æ¦‚ç‡ç±»åˆ«**ï¼ˆå³æ¨¡å‹çš„é¢„æµ‹æ ‡ç­¾ï¼‰ã€‚
>
> ------
>
> ```python
> # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ­£ç¡®é¢„æµ‹æ•°é‡å’Œæ€»æ ·æœ¬æ•°é‡
> c = (predicted == labels).squeeze()
> for i in range(len(labels)):
>     class_correct[labels[i]] += c[i].item()  # è®°å½•æ­£ç¡®é¢„æµ‹çš„æ•°é‡
>     class_total[labels[i]] += 1              # è®°å½•æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡
> ```
>
> - `(predicted == labels)`ï¼šæ£€æŸ¥é¢„æµ‹æ ‡ç­¾ä¸å®é™…æ ‡ç­¾æ˜¯å¦ä¸€è‡´ï¼Œè¿”å›å¸ƒå°”å€¼ï¼ˆTrue æˆ– Falseï¼‰ã€‚`True` ä»£è¡¨é¢„æµ‹æ­£ç¡®ï¼Œ`False` ä»£è¡¨é¢„æµ‹é”™è¯¯ã€‚
> - `c[i].item()`ï¼šè½¬æ¢å¸ƒå°”å€¼ä¸º 1 æˆ– 0ï¼Œè¡¨ç¤ºè¯¥æ ·æœ¬æ˜¯å¦è¢«æ­£ç¡®åˆ†ç±»ã€‚
> - `class_correct[labels[i]] += c[i].item()`ï¼šå°†æ¯ä¸ªæ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ•°é‡ç´¯åŠ åˆ°å¯¹åº”ç±»åˆ«çš„ `class_correct` ä¸­ã€‚
> - `class_total[labels[i]] += 1`ï¼šæ¯é‡åˆ°ä¸€ä¸ªæ ·æœ¬ï¼Œå°±ç´¯åŠ åˆ°è¯¥ç±»åˆ«çš„æ€»æ ·æœ¬æ•° `class_total`ã€‚
>
> ------
>
> ğŸ§  æ€»ç»“ï¼š
>
> è¿™æ®µä»£ç çš„ç›®çš„æ˜¯é€šè¿‡éå†æµ‹è¯•é›†ï¼Œè®¡ç®—æ¯ä¸ªç±»åˆ«çš„**æ­£ç¡®é¢„æµ‹æ•°**å’Œ**æ€»æ ·æœ¬æ•°**ã€‚æœ€åï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¡ç®— **æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡**ï¼š
>
> ```python
> for i in range(10):
>     accuracy = 100 * class_correct[i] / class_total[i]
>     print(f'Accuracy of {classes[i]:5s}: {accuracy:.2f}%')
> ```
>
> `classes[i]` æ˜¯ CIFAR-10 ä¸­çš„ç±»åˆ«åç§°ï¼ˆå¦‚ `plane`, `car` ç­‰ï¼‰ã€‚
>
> ------
>
> ğŸ”„ å…³é”®ç‚¹ï¼š
>
> 1. **`torch.max(outputs.data, 1)`**: è·å–æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç±»åˆ«ã€‚
> 2. **`class_correct[labels[i]] += c[i].item()`**: ç´¯åŠ æ­£ç¡®é¢„æµ‹çš„æ•°é‡ã€‚
> 3. **`class_total[labels[i]] += 1`**: ç´¯åŠ æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡ã€‚
>
> é€šè¿‡è¿™ä¸ªè¿‡ç¨‹ï¼Œä½ èƒ½å¾—åˆ°æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡ï¼Œæœ‰åŠ©äºäº†è§£æ¨¡å‹åœ¨å“ªäº›ç±»åˆ«ä¸Šè¡¨ç°è¾ƒå¥½æˆ–è¾ƒå·®ã€‚



**Q: å…·ä½“ç¤ºä¾‹å‘¢ï¼Ÿæ˜¯ä¸€ä¸ªä¸€ä¸ªå›¾åƒæ£€æµ‹å§**

> æ˜¯çš„ï¼Œ**è¿™æ®µä»£ç ç¡®å®æ˜¯åœ¨ä¸€ä¸ªä¸ªå›¾åƒä¸Šæ£€æµ‹é¢„æµ‹æ˜¯å¦æ­£ç¡®**ï¼Œå¹¶æŒ‰ç±»åˆ«ç»Ÿè®¡æ¯ç±»çš„é¢„æµ‹å‡†ç¡®ç‡ã€‚ä¸‹é¢æˆ‘ä»¬ç»“åˆä¸€ä¸ª**å…·ä½“ç¤ºä¾‹**æ¥æ¼”ç¤ºè¿™æ®µä»£ç åœ¨å®é™…è¿è¡Œä¸­åšäº†ä»€ä¹ˆã€‚
>
> ------
>
> ğŸ¯ å‡è®¾åœºæ™¯
>
> æˆ‘ä»¬å‡è®¾æµ‹è¯•é›†ä¸­æŸä¸ª batchï¼ˆå°æ‰¹é‡ï¼‰é‡Œæœ‰ 5 å¼ å›¾åƒï¼ŒçœŸå®æ ‡ç­¾ä¸ºï¼š
>
> ```python
> labels = [1, 3, 3, 5, 1]  # å®é™…ç±»åˆ«ç¼–å·
> ```
>
> æ¨¡å‹é¢„æµ‹ç»“æœä¸ºï¼š
>
> ```python
> predicted = [1, 2, 3, 5, 0]  # æ¨¡å‹é¢„æµ‹çš„ç±»åˆ«ç¼–å·
> ```
>
> é‚£ä¹ˆå¯¹æ¯”ï¼š
>
> | å›¾ç‰‡ç¼–å· | å®é™…æ ‡ç­¾ï¼ˆlabels[i]ï¼‰ | æ¨¡å‹é¢„æµ‹ï¼ˆpredicted[i]ï¼‰ | æ˜¯å¦æ­£ç¡® (==) |
> | -------- | --------------------- | ------------------------ | ------------- |
> | 0        | 1                     | 1                        | âœ… True        |
> | 1        | 3                     | 2                        | âŒ False       |
> | 2        | 3                     | 3                        | âœ… True        |
> | 3        | 5                     | 5                        | âœ… True        |
> | 4        | 1                     | 0                        | âŒ False       |
>
> ç”Ÿæˆå¸ƒå°”æ¯”è¾ƒï¼š
>
> ```python
> c = [True, False, True, True, False]
> ```
>
> ------
>
> ğŸ“Š ç´¯åŠ åˆ° `class_correct` å’Œ `class_total`
>
> åˆå§‹åŒ–ï¼š
>
> ```python
> class_correct = [0.0] * 10
> class_total = [0.0] * 10
> ```
>
> éå†æ¯ä¸ªæ ·æœ¬ï¼š
>
> ```python
> for i in range(5):
>  class_correct[labels[i]] += c[i].item()
>  class_total[labels[i]] += 1
> ```
>
> æ¯ä¸€æ­¥çš„å˜åŒ–å¦‚ä¸‹ï¼š
>
> | i    | labels[i] | c[i]  | class_correct         | class_total           |
> | ---- | --------- | ----- | --------------------- | --------------------- |
> | 0    | 1         | True  | [0, 1, 0, 0, 0â€¦]      | [0, 1, 0, 0, 0â€¦]      |
> | 1    | 3         | False | [0, 1, 0, 0, 0â€¦]      | [0, 1, 0, 1, 0â€¦]      |
> | 2    | 3         | True  | [0, 1, 0, 1, 0â€¦]      | [0, 1, 0, 2, 0â€¦]      |
> | 3    | 5         | True  | [0, 1, 0, 1, 0, 1, â€¦] | [0, 1, 0, 2, 0, 1, â€¦] |
> | 4    | 1         | False | [0, 1, 0, 1, 0, 1, â€¦] | [0, 2, 0, 2, 0, 1, â€¦] |
>
> ------
>
> âœ… å‡†ç¡®ç‡ç»“æœï¼ˆæœ€ç»ˆï¼‰
>
> ```python
> Accuracy of class 1: 1 / 2 = 50.00%
> Accuracy of class 3: 1 / 2 = 50.00%
> Accuracy of class 5: 1 / 1 = 100.00%
> ```
>
> å…¶ä»–ç±»åˆ«æ²¡æœ‰å‡ºç°ï¼Œå‡†ç¡®ç‡ä¸º 0%ã€‚
>
> ------
>
> ğŸ” åœ¨çœŸå®ä»£ç ä¸­ï¼š
>
> è¿™æ®µé€»è¾‘åœ¨ **æ•´ä¸ª testloaderï¼ˆæµ‹è¯•é›†ï¼‰** ä¸Šè¿è¡Œï¼Œæ¯æ¬¡å¤„ç†ä¸€ä¸ª batchï¼ˆå¦‚ 100 å¼ å›¾åƒï¼‰ï¼Œæœ€åç»Ÿè®¡å‡ºå®Œæ•´çš„ per-class accuracyã€‚
>
> ------
>
> âœ… æ€»ç»“ï¼š
>
> - æ˜¯ä¸€ä¸ªå›¾åƒä¸€ä¸ªå›¾åƒåœ¨æ¯”å¯¹ `é¢„æµ‹æ˜¯å¦æ­£ç¡®`ã€‚
> - ç´¯åŠ æ¯ä¸ªç±»åˆ«çš„æ­£ç¡®é¢„æµ‹æ¬¡æ•°å’Œæ ·æœ¬æ€»æ•°ã€‚
> - æœ€åè®¡ç®— `æ¯ç±»å‡†ç¡®ç‡ = æ­£ç¡®é¢„æµ‹æ•° / æ ·æœ¬æ€»æ•°`ã€‚
>
> ä½ å¯ä»¥åœ¨è®­ç»ƒå®Œåéå¸¸ç›´è§‚åœ°çœ‹åˆ°ï¼š**æ¨¡å‹åœ¨å“ªäº›ç±»åˆ«ä¸Šè¡¨ç°å¥½ï¼ˆå‡†ç¡®ç‡é«˜ï¼‰ï¼Œå“ªäº›ç±»åˆ«ä¸å¥½ï¼ˆæ¯”å¦‚çŒ«é€šå¸¸è¾ƒéš¾åˆ†ç±»ï¼‰**ã€‚è¿™å¯¹åˆ†ææ¨¡å‹ç“¶é¢ˆå¾ˆæœ‰å¸®åŠ©ã€‚



**Q: _, predicted = torch.max(outputs.data, 1) ï¼Ÿ**

> è¿™è¡Œä»£ç ï¼š
>
> ```python
> _, predicted = torch.max(outputs.data, 1)
> ```
>
> æ˜¯ PyTorch ä¸­çš„å¸¸è§å†™æ³•ï¼Œç”¨äºä»æ¨¡å‹çš„è¾“å‡ºä¸­**è·å–æ¯å¼ å›¾ç‰‡é¢„æµ‹çš„ç±»åˆ«ç¼–å·**ï¼ˆä¹Ÿå°±æ˜¯æœ€å¤§å¾—åˆ†å¯¹åº”çš„ä¸‹æ ‡ï¼‰ã€‚ä¸‹é¢æ˜¯è¯¦ç»†è§£è¯»ã€‚
>
> ------
>
> ğŸ“˜ èƒŒæ™¯ï¼šæ¨¡å‹è¾“å‡ºæ˜¯ä»€ä¹ˆï¼Ÿ
>
> åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæ¯”å¦‚ CIFAR-10ï¼Œæ¨¡å‹å¯¹æ¯å¼ å›¾åƒçš„è¾“å‡ºæ˜¯ä¸€ä¸ª **é•¿åº¦ä¸º10çš„å‘é‡**ï¼Œè¡¨ç¤ºæ¯ä¸ªç±»åˆ«çš„å¾—åˆ†ï¼ˆlogitsï¼‰ã€‚ä¾‹å¦‚å¯¹ä¸€å¼ å›¾ç‰‡ï¼Œè¾“å‡ºå¯èƒ½æ˜¯ï¼š
>
> ```python
> [2.1, 1.5, -0.3, 4.2, 0.8, 3.3, 1.1, 0.0, -1.2, 2.0]
> ```
>
> è¿™è¡¨ç¤ºå¯¹ç±»åˆ« 0ï½9 çš„â€œç½®ä¿¡åº¦â€æˆ–â€œåŒ¹é…ç¨‹åº¦â€ï¼Œè¶Šå¤§è¶Šå¯èƒ½æ˜¯æ­£ç¡®ç±»åˆ«ã€‚
>
> ------
>
> ğŸ§  `torch.max(outputs.data, 1)` è§£é‡Š
>
> ```python
> torch.max(tensor, dim)
> ```
>
> è¿™ä¸ªå‡½æ•°ä¼šåœ¨æŒ‡å®šçš„ç»´åº¦ `dim` ä¸Šå¯»æ‰¾æœ€å¤§å€¼ï¼Œå¹¶è¿”å›ä¸¤ä¸ªç»“æœï¼š
>
> 1. **æœ€å¤§å€¼çš„æ•°å€¼**
> 2. **æœ€å¤§å€¼çš„ä¸‹æ ‡ï¼ˆç´¢å¼•ï¼‰**
>
> ä¾‹å¦‚ï¼š
>
> ```python
> outputs.data = tensor([
>   [2.1, 1.5, -0.3, 4.2, 0.8, 3.3, 1.1, 0.0, -1.2, 2.0],
>   [0.5, 2.8, 1.0, 0.7, 0.1, 0.0, 3.9, 0.2, 0.3, 0.5]
> ])
> ```
>
> è¡¨ç¤º 2 å¼ å›¾ç‰‡çš„åˆ†ç±»è¾“å‡ºã€‚
>
> æ‰§è¡Œï¼š
>
> ```python
> _, predicted = torch.max(outputs.data, 1)
> ```
>
> å°±ä¼šå¾—åˆ°ï¼š
>
> ```python
> predicted = tensor([3, 6])
> ```
>
> è¡¨ç¤ºç¬¬ 1 å¼ å›¾åƒé¢„æµ‹ä¸ºç±»åˆ« 3ï¼Œç¬¬ 2 å¼ ä¸ºç±»åˆ« 6ã€‚
>
> ------
>
> ğŸ“Œ ä¸ºä»€ä¹ˆç”¨ `_` æ¥æ”¶ç¬¬ä¸€ä¸ªè¿”å›å€¼ï¼Ÿ
>
> å› ä¸ºä½ åªå…³å¿ƒâ€œå“ªä¸ªç±»åˆ«æ˜¯æœ€å¤§â€ï¼Œä¸å…³å¿ƒâ€œæœ€å¤§å¾—åˆ†æ˜¯å¤šå°‘â€ï¼Œæ‰€ä»¥ç¬¬ä¸€ä¸ªè¿”å›å€¼ï¼ˆå¾—åˆ†ï¼‰å°±ç”¨ `_` å¿½ç•¥æ‰äº†ã€‚
>
> ------
>
> âœ… æ€»ç»“
>
> ```python
> _, predicted = torch.max(outputs.data, 1)
> ```
>
> å«ä¹‰æ˜¯ï¼š
>
> > åœ¨æ¯å¼ å›¾åƒçš„è¾“å‡ºå‘é‡ä¸­ï¼Œæ‰¾å‡ºæœ€å¤§å€¼æ‰€åœ¨çš„ä¸‹æ ‡ `predicted`ï¼Œä½œä¸ºè¯¥å›¾åƒçš„é¢„æµ‹ç±»åˆ«ç¼–å·ã€‚
>
> è¿™ä¸ª `predicted` å°±å¯ä»¥å’Œ `labels` åšæ¯”è¾ƒæ¥åˆ¤æ–­æ˜¯å¦é¢„æµ‹æ­£ç¡®ã€‚





### é™„å½•

**ImageNet å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„ Top-5 é”™è¯¯ç‡**

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250726164756658.png" alt="image-20250726164756658" style="zoom: 67%;" />



**xè½´ï¼šæ¨¡å‹å’Œå¹´ä»½**

ä»å·¦åˆ°å³åˆ—å‡ºäº†ä¸åŒå¹´ä»½çš„ä»£è¡¨æ€§æ¨¡å‹ï¼š

| å¹´ä»½ | æ¨¡å‹         | æ‰€å±æœºæ„/ä½œè€…            |
| ---- | ------------ | ------------------------ |
| 2011 | XRCE         | éæ·±åº¦å­¦ä¹ æ–¹æ³•           |
| 2012 | AlexNet      | Alex Krizhevsky ç­‰äºº     |
| 2013 | ZFNet        | Zeiler & Fergus          |
| 2014 | VGG          | Visual Geometry Group    |
| 2014 | GoogLeNet    | Googleï¼ˆInceptionï¼‰      |
| 2015 | ResNet       | Microsoft Research       |
| 2016 | GoogLeNet-v4 | æ›´æ·±æ›´å¤æ‚çš„ GoogLeNet   |
| ---  | **Human**    | äººç±»å¹³å‡è¡¨ç°ï¼ˆå¯¹æ¯”åŸºçº¿ï¼‰ |

yè½´ï¼š**Top-5 é”™è¯¯ç‡ï¼ˆ%ï¼‰**

è¡¨ç¤ºæ¨¡å‹åœ¨ ImageNet ä¸Šé”™è¯¯é¢„æµ‹çš„æ¦‚ç‡ï¼ˆTop-5ï¼‰ã€‚é”™è¯¯ç‡è¶Šä½ï¼Œæ¨¡å‹æ€§èƒ½è¶Šå¥½ã€‚

ä»å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼š

- **2011 XRCEï¼šTop-5é”™è¯¯ç‡ä¸º 26.0%**
- **2012 AlexNet** å°†é”™è¯¯ç‡é™ä½åˆ°äº† **16.4%**ï¼Œå¼€å¯æ·±åº¦å­¦ä¹ æ—¶ä»£
- **2015 ResNet** å°†é”™è¯¯ç‡é™è‡³ **3.6%**ï¼Œé¦–æ¬¡è¶…è¿‡äº† **äººç±»**ï¼ˆäººç±»è¯¯å·®ä¸º 5.0%ï¼‰
- **2016 GoogLeNet-v4** é™åˆ°ä»… **3.1%**

------

ğŸ§  ä»€ä¹ˆæ˜¯ Top-5 é”™è¯¯ç‡ï¼Ÿ

åœ¨ ImageNet å›¾åƒåˆ†ç±»ä¸­ï¼Œ**æ¯å¼ å›¾åƒçš„æ ‡ç­¾å±äº 1000 ä¸ªç±»åˆ«ä¹‹ä¸€**ã€‚

- **Top-1 é”™è¯¯ç‡**ï¼šé¢„æµ‹ç¬¬ä¸€åæ˜¯å¦æ­£ç¡®ã€‚
- **Top-5 é”™è¯¯ç‡**ï¼šåªè¦æ¨¡å‹é¢„æµ‹å‡ºçš„å‰ 5 ä¸ªç±»åˆ«ä¸­åŒ…å«æ­£ç¡®ç­”æ¡ˆï¼Œå°±ç®— **é¢„æµ‹æˆåŠŸ**ã€‚

ğŸ‘‰ ä¾‹å¦‚ï¼š
å¦‚æœæ¨¡å‹å¯¹æŸå¼ å›¾ç‰‡çš„è¾“å‡ºæ˜¯ï¼š

```text
1. dog
2. cat
3. horse
4. frog
5. deer
```

è€ŒçœŸå®ç±»åˆ«æ˜¯ **frog**ï¼Œåˆ™ï¼š

- **Top-1** æ˜¯é”™çš„ï¼ˆç¬¬ä¸€åæ˜¯ dogï¼‰
- **Top-5** æ˜¯å¯¹çš„ï¼ˆfrog åœ¨å‰äº”åä¸­ï¼‰

------

âœ… æ€»ç»“

è¿™å¼ å›¾å±•ç¤ºäº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ ImageNet ä¸Šçš„é£è·ƒå¼è¿›å±•ï¼Œå°¤å…¶æ˜¯ï¼š

- AlexNetï¼ˆ2012ï¼‰é¦–æ¬¡ä½¿ç”¨æ·±åº¦ CNNï¼Œå¤§å¹…æ”¹è¿›ç»“æœï¼›
- ResNetï¼ˆ2015ï¼‰æå‡ºæ®‹å·®ç½‘ç»œï¼Œé¦–æ¬¡è¶…è¿‡äººç±»ï¼›
- GoogLeNet å’Œ VGG ä¹Ÿéƒ½æ˜¯ä»£è¡¨æ€§çš„ç»å…¸ CNN æ¶æ„ã€‚

**Top-5 é”™è¯¯ç‡ä½ï¼Œæ„å‘³ç€æ¨¡å‹æ›´ç¨³å®šã€é²æ£’æ€§æ›´å¼º**ã€‚åœ¨å·¥ä¸šç•Œã€æ¯”èµ›ä¸­ç»å¸¸ä½œä¸ºé‡è¦è¯„ä»·æŒ‡æ ‡ã€‚

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥è¿›ä¸€æ­¥è§£é‡Šæ¯ä¸ªæ¨¡å‹æ¶æ„çš„æ”¹è¿›è¦ç‚¹ã€‚



## 2.5 Tiny ImageNetå›¾åƒåˆ†ç±»é¡¹ç›®

ç°åœ¨ç”¨çš„chatgptæ¶æ„éƒ½æ˜¯transformeræ¨¡å‹, TransformeråŒ…æ‹¬ç¼–ç å™¨å’Œè§£ç å™¨ä¸¤å¤§éƒ¨åˆ†ï¼Œå…¶ä¸­ç¼–ç å™¨è´Ÿè´£å°†è¾“å…¥åºåˆ—æ˜ å°„ä¸ºéšè—è¡¨ç¤ºï¼Œè€Œè§£ç å™¨åˆ™åˆ©ç”¨è¿™äº›éšè—è¡¨ç¤ºæ¥ç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚<mark>transformerçœ‹èµ·æ¥æ˜¯å¦ä¸ASCIIç¼–ç ç›¸ä¼¼</mark>ã€‚

![image-20250224172229492](https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250224172229492.png)



åŸºäº PyTorch å’Œ torchvision åº“çš„ç¤ºä¾‹ä»£ç ï¼Œæˆ‘ä»¬æ¼”ç¤ºå¦‚ä½•åŠ è½½Tiny ImageNet æ•°æ®é›†ã€æ„å»ºåŸºäºé¢„è®­ç»ƒ ResNet50 æ¨¡å‹çš„ç¥ç»ç½‘ç»œï¼Œå¹¶è¿›è¡Œå¾®è°ƒè®­ç»ƒå®ç°å›¾åƒåˆ†ç±»ã€‚ImageNet æ•°æ®é›†ä½“ç§¯è¾ƒå¤§ï¼Œæˆ‘ä»¬ä½¿ç”¨Tiny ImageNetï¼Œå³æ˜¯è¿™æ ·ï¼Œè®­ç»ƒæ—¶è¿˜éœ€è¦å……è¶³çš„å­˜å‚¨å’Œè®¡ç®—èµ„æºï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ­£ç¡®è®¾ç½®æ•°æ®è·¯å¾„ã€batch size å’Œå…¶ä»–è¶…å‚æ•°ã€‚

èµ·å› æ˜¯ä¹¦ä¸­æåˆ°

![image-20250224172024784](https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250224172024784.png)



![image-20250224172055496](https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250224172055496.png)

**Q1. ResNet50?**

> ResNet-50æ˜¯ä¸€ç§æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼Œå±äºResNetï¼ˆResidual Networkï¼‰ç³»åˆ—æ¨¡å‹ä¹‹ä¸€ã€‚å®ƒç”±å¾®è½¯ç ”ç©¶é™¢çš„Kaiming Heç­‰äººåœ¨2015å¹´æå‡ºï¼Œæ—¨åœ¨è§£å†³è®­ç»ƒéå¸¸æ·±çš„ç¥ç»ç½‘ç»œæ—¶é‡åˆ°çš„æ¢¯åº¦æ¶ˆå¤±æˆ–æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚ResNeté€šè¿‡å¼•å…¥â€œæ®‹å·®å­¦ä¹ å•å…ƒâ€ï¼ˆresidual learning units æˆ–è€… shortcut connections/è·³è·ƒè¿æ¥ï¼‰ï¼Œä½¿å¾—ç½‘ç»œå¯ä»¥æ›´æ·±ä¸”æ›´å®¹æ˜“è®­ç»ƒã€‚
>
> **ResNet-50ç»“æ„ç‰¹ç‚¹**
>
> - **æ·±åº¦**: ResNet-50æ˜¯ä¸€ä¸ªåŒ…å«50å±‚çš„æ·±å±‚ç½‘ç»œï¼ˆåŒ…æ‹¬å·ç§¯å±‚ã€æ¿€æ´»å‡½æ•°å±‚ã€æ± åŒ–å±‚ç­‰ï¼‰ï¼Œå› æ­¤å¾—åResNet-50ã€‚
> - **æ®‹å·®å—(Residual Block)**: æ ¸å¿ƒæ¦‚å¿µæ˜¯æ®‹å·®å­¦ä¹ ï¼Œé€šè¿‡æ·»åŠ ç›´è¿è¾¹ï¼ˆshortcut connectionsï¼‰ï¼Œè®©è¾“å…¥ç›´æ¥è·³è¿‡ä¸€å±‚æˆ–å¤šå±‚ä¼ åˆ°åé¢å±‚ï¼Œè¿™æœ‰åŠ©äºè§£å†³æ·±å±‚ç½‘ç»œä¸­çš„é€€åŒ–é—®é¢˜ï¼ˆå³éšç€ç½‘ç»œæ·±åº¦å¢åŠ ï¼Œè®­ç»ƒé”™è¯¯ç‡åè€Œä¸Šå‡çš„é—®é¢˜ï¼‰ã€‚å…¬å¼è¡¨è¾¾ä¸ºï¼š\[y = F(x, \{W_i\}) + x\]ï¼Œå…¶ä¸­\(x\)å’Œ\(y\)åˆ†åˆ«æ˜¯å±‚çš„è¾“å…¥å’Œè¾“å‡ºï¼Œ\(F(x, \{W_i\})\)è¡¨ç¤ºå­¦ä¹ åˆ°çš„æ®‹å·®æ˜ å°„ã€‚
> - **å…·ä½“æ¶æ„**:
>   - è¾“å…¥å›¾åƒå°ºå¯¸é€šå¸¸æ˜¯224x224ã€‚
>   - ç½‘ç»œå¼€å§‹äºä¸€ä¸ª7x7çš„å·ç§¯å±‚ï¼Œæ¥ç€æ˜¯æœ€å¤§æ± åŒ–å±‚ã€‚
>   - æ¥ä¸‹æ¥æ˜¯4ä¸ªä¸»è¦çš„æ®‹å·®é˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µåˆ†åˆ«åŒ…å«äº†3ã€4ã€6å’Œ3ä¸ªæ®‹å·®å—ï¼ˆå¯¹äºResNet-50ï¼‰ï¼Œæ¯ä¸ªæ®‹å·®å—å†…éƒ¨æœ‰ä¸¤ä¸ª3x3çš„å·ç§¯å±‚ã€‚
>   - åœ¨æœ€åä¸€ä¸ªæ®‹å·®é˜¶æ®µä¹‹åï¼Œä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–å±‚å’Œä¸€ä¸ªå…¨è¿æ¥å±‚è¿›è¡Œåˆ†ç±»ä»»åŠ¡ï¼Œé»˜è®¤è®¾ç½®ä¸‹ç”¨äºImageNetæ•°æ®é›†çš„åˆ†ç±»ï¼Œæœ‰1000ä¸ªè¾“å‡ºèŠ‚ç‚¹ã€‚
>
> **åº”ç”¨åœºæ™¯**
>
> ç”±äºå…¶ä¼˜ç§€çš„æ€§èƒ½å’Œç›¸å¯¹åˆç†çš„è®¡ç®—éœ€æ±‚ï¼ŒResNet-50å¹¿æ³›åº”ç”¨äºå„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­ï¼Œå¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€äººè„¸è¯†åˆ«ç­‰ã€‚æ­¤å¤–ï¼Œå®ƒä¹Ÿå¸¸è¢«ç”¨ä½œè¿ç§»å­¦ä¹ çš„åŸºç¡€æ¨¡å‹ï¼Œåœ¨ç‰¹å®šé¢†åŸŸçš„å°å‹æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒä»¥è¾¾åˆ°è¾ƒå¥½çš„æ•ˆæœã€‚
>
> ResNetçš„æˆåŠŸæ¨åŠ¨äº†æ·±åº¦å­¦ä¹ ç‰¹åˆ«æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œåç»­è¿˜å‡ºç°äº†æ›´æ·±çš„ç‰ˆæœ¬å¦‚ResNet-101å’ŒResNet-152ï¼Œä»¥åŠé’ˆå¯¹æ•ˆç‡ä¼˜åŒ–çš„å˜ä½“å¦‚ResNeXtã€‚



### 1.å‡†å¤‡Tiny ImageNetæ•°æ®é›†

Tiny ImageNetã€‚å®ƒåŒ…å« 200 ä¸ªç±»åˆ«ï¼Œæ¯ä¸ªç±»åˆ« 500 å¼ è®­ç»ƒå›¾ç‰‡ï¼Œæ€»æ•°æ®é‡å¤§çº¦ 500MBï¼Œéå¸¸é€‚åˆå®éªŒå’Œè°ƒè¯•ã€‚

ä¸‹è½½ `wget http://cs231n.stanford.edu/tiny-imagenet-200.zip`ï¼Œè®°237MBã€‚

éªŒè¯é›†é€šå¸¸è§£å‹åæ‰€æœ‰å›¾ç‰‡ä¼šåœ¨åŒä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œè€Œ ImageFolder è¦æ±‚æ¯ä¸ªç±»åˆ«æœ‰ç‹¬ç«‹å­æ–‡ä»¶å¤¹ã€‚ä½ éœ€è¦æ ¹æ®å®˜æ–¹æä¾›çš„ éªŒè¯é›†æ ‡ç­¾æ–‡ä»¶ï¼Œå¦‚ val_annotations.txtï¼Œå¯¹å›¾ç‰‡è¿›è¡Œåˆ†ç±»æ•´ç†ã€‚å¸¸è§çš„åšæ³•æ˜¯ç¼–å†™ä¸€ä¸ªè„šæœ¬ï¼Œæ ¹æ®æ–‡ä»¶ä¸­çš„ç±»åˆ«ä¿¡æ¯å°†å›¾ç‰‡ç§»åŠ¨åˆ°å¯¹åº”çš„å­æ–‡ä»¶å¤¹ä¸­ã€‚

`tinyimagenet.sh`

```sh
#!/bin/bash

# download and unzip dataset
#wget http://cs231n.stanford.edu/tiny-imagenet-200.zip
unzip tiny-imagenet-200.zip

current="$(pwd)/tiny-imagenet-200"

# training data
cd $current/train
for DIR in $(ls); do
   cd $DIR
   rm *.txt
   mv images/* .
   rm -r images
   cd ..
done

# validation data
cd $current/val
annotate_file="val_annotations.txt"
length=$(cat $annotate_file | wc -l)
for i in $(seq 1 $length); do
    # fetch i th line
    line=$(sed -n ${i}p $annotate_file)
    # get file name and directory name
    file=$(echo $line | cut -f1 -d" " )
    directory=$(echo $line | cut -f2 -d" ")
    mkdir -p $directory
    mv images/$file $directory
done
rm -r images
echo "done"

```



è¿è¡Œ`sh tinyimagenet.sh`ï¼Œæ•°æ®è§£å‹å¹¶åˆ†ç±»å‡†å¤‡å¥½ï¼Œè®°472MBã€‚

```
% ls -l
total 5200
drwxrwxr-x    3 hfyan  staff       96 Dec 12  2014 test
drwxrwxr-x  202 hfyan  staff     6464 Dec 12  2014 train
drwxrwxr-x  203 hfyan  staff     6496 Feb 24 11:09 val
-rw-rw-r--    1 hfyan  staff     2000 Feb  9  2015 wnids.txt
-rw-------    1 hfyan  staff  2655750 Feb  9  2015 words.txt
(base) hfyan@HongfeideMac-Studio tiny-imagenet-200 % pwd
/Users/hfyan/data/tiny-imagenet-200

```



### 2.æŸ¥çœ‹æœ¬åœ°æœºå™¨Mac Studioçš„é…ç½®

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/202502160845440.png" alt="image-20250216084507879" style="zoom:50%;" />

Apple M1 Ultra æ˜¯ Apple èŠ¯ç‰‡ç³»åˆ—ä¸­çš„ä¸€å‘˜ï¼Œä¸“ä¸ºé«˜æ€§èƒ½éœ€æ±‚è®¾è®¡ï¼Œç‰¹åˆ«æ˜¯åœ¨ Mac Studio ç­‰è®¾å¤‡ä¸­ä½¿ç”¨ã€‚M1 Ultra çš„é…ç½®åŒ…æ‹¬äº†ä¸­å¤®å¤„ç†å™¨ï¼ˆCPUï¼‰ã€å›¾å½¢å¤„ç†å™¨ï¼ˆGPUï¼‰ä»¥åŠç»Ÿä¸€å†…å­˜æ¶æ„ï¼ˆUnified Memory Architecture, UMAï¼‰ï¼Œå…¶ä¸­ç»Ÿä¸€å†…å­˜å¯ä¾› CPUã€GPU ä»¥åŠå…¶ä»–ç»„ä»¶å…±äº«ã€‚

**å…³äº GPU å†…å­˜**

åœ¨ M1 Ultra ä¸­ï¼Œ 64GB å†…å­˜å®é™…ä¸Šæ˜¯æ•´ä¸ªç³»ç»Ÿå…±äº«çš„ç»Ÿä¸€å†…å­˜å®¹é‡ï¼Œè¿™æ„å‘³ç€è¿™64GBå†…å­˜æ˜¯ç”±CPUã€GPUåŠå…¶ä»–ç»„ä»¶å…±åŒä½¿ç”¨çš„ï¼Œè€Œä¸æ˜¯ä¸“é—¨åˆ†é…ç»™GPUçš„ç‹¬ç«‹å†…å­˜ã€‚ è¿™ç§è®¾è®¡æå¤§åœ°æé«˜äº†çµæ´»æ€§å’Œæ€§èƒ½è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚å›¾å½¢ä»»åŠ¡æˆ–å¤šä»»åŠ¡å¤„ç†åœºæ™¯ä¸‹ã€‚ 

- **ç»Ÿä¸€å†…å­˜æ¶æ„**ï¼šApple çš„è®¾è®¡ç†å¿µæ˜¯é€šè¿‡ç»Ÿä¸€å†…å­˜æ¶æ„æ¥æå‡æ€§èƒ½å’Œæ•ˆç‡ã€‚è¿™ç§æ¶æ„å…è®¸ GPU å’Œ CPU è®¿é—®ç›¸åŒçš„å†…å­˜æ± ï¼Œå‡å°‘äº†æ•°æ®å¤åˆ¶çš„éœ€æ±‚ï¼Œå¹¶ä¸”å¯ä»¥æ›´çµæ´»åœ°æ ¹æ®éœ€è¦åˆ†é…å†…å­˜èµ„æºã€‚

- **M1 Ultra çš„ GPU èµ„æº**ï¼šM1 Ultra é…å¤‡äº†ä¸€ä¸ªå¼ºå¤§çš„ 48 æ ¸å¿ƒ GPUã€‚å°½ç®¡æ²¡æœ‰â€œä¸“ç”¨â€çš„ GPU æ˜¾å­˜ï¼Œä½†å…¶å¯ä»¥ä»æ•´ä¸ª 64GB ç»Ÿä¸€å†…å­˜ä¸­è·å–æ‰€éœ€çš„å·¥ä½œå†…å­˜ã€‚è¿™å¯¹äºè®¸å¤šå›¾å½¢å¯†é›†å‹åº”ç”¨æ¥è¯´æ˜¯éå¸¸æœ‰åˆ©çš„ï¼Œå› ä¸ºå®ƒé¿å…äº†ä¼ ç»Ÿæ˜¾å­˜ä¸ä¸»å­˜ä¹‹é—´å¯èƒ½å­˜åœ¨çš„ç“¶é¢ˆã€‚

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/202502160845741.png" alt="image-20250216084526735" style="zoom:50%;" />



### 3. è®­ç»ƒæ¨¡å‹

åŸºäº PyTorch å’Œ torchvision åº“çš„ç¤ºä¾‹ä»£ç ï¼Œè¯¥ä»£ç æ¼”ç¤ºäº†å¦‚ä½•åŠ è½½ ImageNet æ•°æ®é›†ã€æ„å»ºåŸºäºé¢„è®­ç»ƒ ResNet æ¨¡å‹çš„ç¥ç»ç½‘ç»œï¼Œå¹¶è¿›è¡Œå¾®è°ƒè®­ç»ƒå®ç°å›¾åƒåˆ†ç±»ã€‚

åœ¨clab.pkuäº‘ç«¯è™šæ‹Ÿæœºï¼Œå†…å­˜åªæœ‰4GBï¼Œæ— GPUï¼Œè·‘ä¸èµ·æ¥ã€‚ä»¥ä¸‹æ˜¯åœ¨æˆ‘æœ¬åœ°macæœºå™¨è¿è¡Œçš„ã€‚

> â€œKilledâ€ é€šå¸¸æ˜¯æ“ä½œç³»ç»Ÿï¼ˆLinux å†…æ ¸ï¼‰å‡ºäºå†…å­˜ä¸è¶³ï¼ˆOOMï¼‰çš„åŸå› ç»ˆæ­¢äº†è¿›ç¨‹ã€‚è¿™å¯èƒ½æ˜¯ç”±äºä»¥ä¸‹åŸå› å¼•èµ·çš„ï¼š
>
> - **å†…å­˜ä¸è¶³**ï¼šç¨‹åºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å ç”¨äº†è¿‡å¤šå†…å­˜ï¼Œè¶…å‡ºäº†ç³»ç»Ÿå¯ç”¨å†…å­˜æˆ–äº¤æ¢ç©ºé—´ï¼ˆswapï¼‰çš„é™åˆ¶ã€‚
> - **GPU å†…å­˜ä¸è¶³**ï¼šå¦‚æœä½¿ç”¨ GPU è®­ç»ƒï¼Œå¯èƒ½ä¹Ÿä¼šå‡ºç° GPU å†…å­˜æº¢å‡ºçš„é—®é¢˜ã€‚

`tiny_imagenet_resnet50_epoch25.py`

```python
import os
import copy
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, models, transforms

# è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
def train_model(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, num_epochs=25, device='cpu'):
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch+1, num_epochs))
        print('-' * 10)

        # æ¯ä¸ª epoch åˆ†ä¸ºè®­ç»ƒå’ŒéªŒè¯é˜¶æ®µ
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
            else:
                model.eval()   # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

            running_loss = 0.0
            running_corrects = 0

            # éå†æ•°æ®
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                optimizer.zero_grad()  # æ¢¯åº¦æ¸…é›¶

                # å‰å‘ä¼ æ’­
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    # ä»…åœ¨è®­ç»ƒé˜¶æ®µåå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–°
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            if phase == 'train':
                scheduler.step()

            epoch_loss = running_loss / dataset_sizes[phase]

            # MPS åç«¯ä¸æ”¯æŒ float64 è¿ç®—ã€‚è§£å†³æ–¹æ³•æ˜¯ä½¿ç”¨ float32ï¼Œå³è°ƒç”¨ .float()ã€‚
            #epoch_acc = running_corrects.double() / dataset_sizes[phase]
            epoch_acc = running_corrects.float() / dataset_sizes[phase]

            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))

            # ä¿å­˜æœ€ä½³æ¨¡å‹å‚æ•°
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
        print()

    print('Best val Acc: {:.4f}'.format(best_acc))
    model.load_state_dict(best_model_wts)
    return model

def main():
    # 1. æ•°æ®é¢„å¤„ç†ä¸åŠ è½½
    # æ³¨æ„ï¼šæ­¤å¤„å‡å®šImageNetæ•°æ®é›†æŒ‰ç…§train/valæ–‡ä»¶å¤¹åˆ†åˆ«å­˜æ”¾å„ç±»åˆ«å›¾ç‰‡ï¼Œ
    # ä¸”æ¯ä¸ªç±»åˆ«ä½œä¸ºä¸€ä¸ªå­æ–‡ä»¶å¤¹å­˜åœ¨
    data_transforms = {
        'train': transforms.Compose([
            transforms.RandomResizedCrop(224),           # éšæœºè£å‰ªä¸º224Ã—224
            transforms.RandomHorizontalFlip(),           # éšæœºæ°´å¹³ç¿»è½¬
            transforms.ToTensor(),                         # è½¬ä¸ºTensor
            transforms.Normalize([0.485, 0.456, 0.406],    # ImageNetå‡å€¼
                                 [0.229, 0.224, 0.225])      # ImageNetæ ‡å‡†å·®
        ]),
        'val': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),                           # ä¸­å¿ƒè£å‰ª
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406],
                                 [0.229, 0.224, 0.225])
        ]),
    }

    # Tiny ImageNet æ•°æ®è·¯å¾„
    data_dir = '/Users/hfyan/data/tiny-imagenet-200'
    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                                data_transforms[x])
                      for x in ['train', 'val']}

    # è®¾ç½® num_workers ä¸º 4 ä»¥åˆ©ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½
    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],
                                                    batch_size=128,    # å¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                                                    shuffle=True,
                                                    num_workers=8)
                   for x in ['train', 'val']}

    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
    class_names = image_datasets['train'].classes

    #device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # ä½¿ç”¨ MPS ä½œä¸º GPU åç«¯ï¼ˆé€‚ç”¨äº Apple Siliconï¼‰
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("Using MPS device for GPU acceleration")
    else:
        device = torch.device("cpu")
        print("MPS device not available, using CPU")

    #2. æ„å»ºæ¨¡å‹ï¼ˆä½¿ç”¨é¢„è®­ç»ƒ ResNet50ï¼‰
    # è¿™é‡Œæˆ‘ä»¬åŠ è½½é¢„è®­ç»ƒçš„ ResNet50 æ¨¡å‹ï¼Œå¹¶ä¿®æ”¹æœ€åçš„å…¨è¿æ¥å±‚ä»¥é€‚åº”Tiny ImageNetçš„ç±»åˆ«æ•°ï¼ˆ200ç±»ï¼‰
    model_ft = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
    num_ftrs = model_ft.fc.in_features
    model_ft.fc = nn.Linear(num_ftrs, len(class_names))
    model_ft = model_ft.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)

    # å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥ï¼Œæ¯7ä¸ªepoché™ä½ä¸€æ¬¡å­¦ä¹ ç‡
    exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

    #3. è®­ç»ƒæ¨¡å‹
    num_epochs = 25  # å¯æ ¹æ®éœ€è¦è°ƒæ•´epochæ•°é‡
    model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,
                           dataloaders, dataset_sizes, num_epochs=num_epochs, device=device)

    #4. ä¿å­˜æ¨¡å‹ï¼Œæ–‡ä»¶åå»ºè®®ä¸º tiny_imagenet_resnet50_epoch25.pth
    torch.save(model_ft.state_dict(), 'tiny_imagenet_resnet50_epoch25.pth')
    print("Model saved as tiny_imagenet_resnet50_epoch25.pth")

if __name__ == '__main__':
    main()

```

> **è¯´æ˜**
>
> - **æ•°æ®é¢„å¤„ç†**
>   ä½¿ç”¨äº† `transforms` å¯¹æ•°æ®è¿›è¡Œäº†æ•°æ®å¢å¼ºï¼ˆå¦‚éšæœºè£å‰ªã€æ°´å¹³ç¿»è½¬ï¼‰ä»¥åŠå½’ä¸€åŒ–ï¼ˆImageNetå¸¸ç”¨çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼‰ã€‚æ•°æ®æ–‡ä»¶å¤¹éœ€è¦ç¬¦åˆ `ImageFolder` çš„è¦æ±‚ï¼Œæ¯ä¸ªç±»åˆ«å­˜æ”¾åœ¨ç‹¬ç«‹çš„å­æ–‡ä»¶å¤¹ä¸­ã€‚
> - **æ¨¡å‹æ„å»º**
>   æœ¬ç¤ºä¾‹ä¸­é‡‡ç”¨é¢„è®­ç»ƒçš„ ResNet50 æ¨¡å‹ï¼Œå¹¶ä¿®æ”¹äº†æœ€åä¸€å±‚å…¨è¿æ¥å±‚ä»¥è¾“å‡ºä¸ç±»åˆ«æ•°åŒ¹é…çš„æ¦‚ç‡åˆ†å¸ƒã€‚
> - **è®­ç»ƒè¿‡ç¨‹**
>   ä»£ç ä¸­å®šä¹‰äº† `train_model` å‡½æ•°ï¼Œå¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ï¼Œå¹¶åœ¨éªŒè¯é›†ä¸Šé€‰å–å‡†ç¡®ç‡æœ€é«˜çš„æ¨¡å‹å‚æ•°ã€‚å­¦ä¹ ç‡è°ƒåº¦å™¨ç”¨äºé€æ­¥é™ä½å­¦ä¹ ç‡ä»¥ä¾¿æ›´å¥½åœ°æ”¶æ•›ã€‚
> - **æ³¨æ„äº‹é¡¹**
>   - ImageNet æ•°æ®é›†è¾ƒå¤§ï¼Œå»ºè®®åœ¨ä½¿ç”¨æ—¶æ³¨æ„æ•°æ®åŠ è½½ã€å†…å­˜ç®¡ç†å’Œè®­ç»ƒæ—¶é•¿ã€‚
>   - å¦‚éœ€æ›´æ·±å…¥çš„æ¨¡å‹è°ƒä¼˜æˆ–ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·å‚è€ƒ PyTorch å®˜æ–¹æ–‡æ¡£å’Œç›¸å…³èµ„æ–™ã€‚
>
> è¯¥ç¤ºä¾‹ä»£ç ä¸ºå…¥é—¨çº§ç¤ºä¾‹ï¼Œå®é™…é¡¹ç›®ä¸­å¯èƒ½éœ€è¦æ›´å¤šçš„ä¼˜åŒ–å’Œé…ç½®ã€‚
>
> 
>
> **ä¸»å…¥å£ä¿æŠ¤**ï¼šæ‰€æœ‰æ¶‰åŠå¤šè¿›ç¨‹æˆ–å¤šçº¿ç¨‹çš„ä»£ç éƒ½å°è£…åœ¨ `if __name__ == '__main__':` ä¸‹ï¼Œé¿å… macOS ä¸‹çš„å¯åŠ¨é—®é¢˜ã€‚



**Q. ä¸ºä»€ä¹ˆæ¯æ¬¡è¿è¡Œéƒ½é‡æ–°ä¸‹è½½resnet50?**

ä¸ºä»€ä¹ˆæ¯æ¬¡è¿è¡Œéƒ½é‡æ–°ä¸‹è½½ Downloading: "https://download.pytorch.org/models/resnet50-11ad3fa6.pth" to /Users/hfyan/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth ï¼Œä¸‹è½½è¿‡äº†ä¸€æ¬¡äº†ã€‚

> PyTorch ä¼šå°†é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡ä¸‹è½½åˆ°é»˜è®¤ç¼“å­˜ç›®å½•ï¼ˆé€šå¸¸æ˜¯ `~/.cache/torch/hub/checkpoints`ï¼‰ï¼Œå¦‚æœæ¯æ¬¡è¿è¡Œéƒ½é‡æ–°ä¸‹è½½ï¼Œå¯èƒ½æœ‰ä»¥ä¸‹å‡ ä¸ªåŸå› ï¼š
>
> 1. **ç¼“å­˜ç›®å½•æƒé™æˆ–ç¯å¢ƒå˜é‡é—®é¢˜**  
>    è¯·æ£€æŸ¥ `/Users/hfyan/.cache/torch/hub/checkpoints/` ç›®å½•æ˜¯å¦å­˜åœ¨ã€æ˜¯å¦æœ‰å†™å…¥æƒé™ä»¥åŠæ–‡ä»¶æ˜¯å¦å®Œæ•´ã€‚å¦‚æœè¯¥ç›®å½•ä¸å¯å†™æˆ–æ–‡ä»¶è¢«åˆ é™¤ï¼ŒPyTorch å°±æ— æ³•ä½¿ç”¨ç¼“å­˜æ–‡ä»¶ã€‚ä½ å¯ä»¥å°è¯•è®¾ç½®ç¯å¢ƒå˜é‡ `TORCH_HOME`ï¼Œå°†ç¼“å­˜ç›®å½•æŒ‡å®šåˆ°ä¸€ä¸ªä½ ç¡®å®šæœ‰æƒé™å¹¶ä¸”æŒä¹…çš„ç›®å½•ï¼š
>    ```bash
>    export TORCH_HOME=/Users/hfyan/torch_cache
>    ```
>    ç„¶åå†è¿è¡Œç¨‹åºã€‚
>
> 2. **è¿è¡Œç¯å¢ƒé‡ç½®**  
>    å¦‚æœä½ æ¯æ¬¡è¿è¡Œç¨‹åºæ—¶ä½¿ç”¨çš„æ˜¯ä¸´æ—¶ç¯å¢ƒï¼ˆä¾‹å¦‚æŸäº› IDE çš„æ²™ç›’æ¨¡å¼ã€å®¹å™¨æˆ–è™šæ‹Ÿç¯å¢ƒåœ¨é€€å‡ºåæ¸…é™¤ç¼“å­˜ï¼‰ï¼Œé‚£ä¹ˆæ¯æ¬¡å¯åŠ¨æ—¶ç¼“å­˜æ–‡ä»¶å¯èƒ½éƒ½ä¼šè¢«æ¸…ç©ºï¼Œä»è€Œå¯¼è‡´é‡æ–°ä¸‹è½½ã€‚
>
> 3. **ä¸‹è½½æ–‡ä»¶æ ¡éªŒé—®é¢˜**  
>    PyTorch åœ¨åŠ è½½æƒé‡æ—¶ä¼šæ£€æŸ¥æ–‡ä»¶çš„å®Œæ•´æ€§ã€‚å¦‚æœæ–‡ä»¶æŸåæˆ–æ ¡éªŒå¤±è´¥ï¼Œä¹Ÿä¼šé‡æ–°ä¸‹è½½ã€‚
>
> å»ºè®®å…ˆç¡®è®¤ç¼“å­˜ç›®å½•ä¸­æ˜¯å¦å·²ç»å­˜åœ¨ `resnet50-11ad3fa6.pth` æ–‡ä»¶ï¼Œä¸”è¯¥æ–‡ä»¶å¤§å°å’Œæ ¡éªŒå€¼æ˜¯å¦æ­£ç¡®ã€‚å¦‚æœæœ‰æƒé™é—®é¢˜æˆ–ç¼“å­˜ç›®å½•è¢«é‡ç½®ï¼Œå¯ä»¥é€šè¿‡è®¾ç½® `TORCH_HOME` ç¯å¢ƒå˜é‡æ¥æŒ‡å®šä¸€ä¸ªç¨³å®šçš„ç¼“å­˜ç›®å½•ï¼Œä»è€Œé¿å…æ¯æ¬¡è¿è¡Œæ—¶é‡æ–°ä¸‹è½½ã€‚



#### 2025/2/24 11:30å¼€å§‹è¿è¡Œï¼Œ16:00ç»“æŸ

>  File "/Users/hfyan/data/tiny_imagenet_resnet50_epoch25.py", line 47, in train_model
>
>  epoch_acc = running_corrects.double() / dataset_sizes[phase]
>
>  TypeError: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.
>
>  (base) hfyan@HongfeideMac-Studio data % python tiny_imagenet_resnet50_epoch25.py 

```
(base) hfyan@HongfeideMac-Studio data % python tiny_imagenet_resnet50_epoch25.py 
Using MPS device for GPU acceleration
Epoch 1/25
----------
train Loss: 5.0366 Acc: 0.0720
val Loss: 4.1348 Acc: 0.2819

Epoch 2/25
----------
train Loss: 3.2563 Acc: 0.3406
val Loss: 1.7006 Acc: 0.6197

Epoch 3/25
----------
train Loss: 2.1834 Acc: 0.5065
val Loss: 1.2068 Acc: 0.7062

Epoch 4/25
----------
train Loss: 1.8635 Acc: 0.5663
val Loss: 1.0010 Acc: 0.7498

Epoch 5/25
----------
train Loss: 1.6788 Acc: 0.6029
val Loss: 0.8927 Acc: 0.7702

Epoch 6/25
----------
train Loss: 1.5723 Acc: 0.6268
val Loss: 0.8407 Acc: 0.7808

Epoch 7/25
----------
train Loss: 1.5044 Acc: 0.6390
val Loss: 0.7990 Acc: 0.7907

Epoch 8/25
----------
train Loss: 1.4324 Acc: 0.6567
val Loss: 0.7788 Acc: 0.7939

Epoch 9/25
----------
train Loss: 1.4212 Acc: 0.6571
val Loss: 0.7701 Acc: 0.7981

Epoch 10/25
----------
train Loss: 1.4054 Acc: 0.6614
val Loss: 0.7669 Acc: 0.7966

Epoch 11/25
----------
train Loss: 1.4035 Acc: 0.6615
val Loss: 0.7634 Acc: 0.7980

Epoch 12/25
----------
train Loss: 1.3995 Acc: 0.6626
val Loss: 0.7595 Acc: 0.7990

Epoch 13/25
----------
train Loss: 1.3882 Acc: 0.6647
val Loss: 0.7558 Acc: 0.7988

Epoch 14/25
----------
train Loss: 1.3747 Acc: 0.6680
val Loss: 0.7517 Acc: 0.7997

Epoch 15/25
----------
train Loss: 1.3754 Acc: 0.6683
val Loss: 0.7490 Acc: 0.8006

Epoch 16/25
----------
train Loss: 1.3685 Acc: 0.6689
val Loss: 0.7592 Acc: 0.7970

Epoch 17/25
----------
train Loss: 1.3771 Acc: 0.6681
val Loss: 0.7567 Acc: 0.8009

Epoch 18/25
----------
train Loss: 1.3690 Acc: 0.6688
val Loss: 0.7508 Acc: 0.8011

Epoch 19/25
----------
train Loss: 1.3716 Acc: 0.6694
val Loss: 0.7521 Acc: 0.8008

Epoch 20/25
----------
train Loss: 1.3729 Acc: 0.6687
val Loss: 0.7527 Acc: 0.8002

Epoch 21/25
----------
train Loss: 1.3709 Acc: 0.6689
val Loss: 0.7501 Acc: 0.8014

Epoch 22/25
----------
train Loss: 1.3706 Acc: 0.6708
val Loss: 0.7516 Acc: 0.8008

Epoch 23/25
----------
train Loss: 1.3681 Acc: 0.6696
val Loss: 0.7502 Acc: 0.8002

Epoch 24/25
----------
train Loss: 1.3725 Acc: 0.6698
val Loss: 0.7508 Acc: 0.8003

Epoch 25/25
----------
train Loss: 1.3708 Acc: 0.6696
val Loss: 0.7480 Acc: 0.8004

Best val Acc: 0.8014
Model saved as tiny_imagenet_resnet50_epoch25.pth

```

è·‘äº†4å°æ—¶30åˆ†é’Ÿã€‚

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250224171542842.png" alt="image-20250224171542842" style="zoom:50%;" />



```
% ls -lh *.pth
-rw-r--r--  1 hfyan  staff    92M Feb 24 16:02 tiny_imagenet_resnet50_epoch25.pth
```





**Q.è¿™ä¸ªç¨‹åºä¿å­˜ä¸ºä»€ä¹ˆæ–‡ä»¶åå­—å¥½ï¼Ÿ**

é€šå¸¸å»ºè®®æ–‡ä»¶åèƒ½å¤Ÿåæ˜ å‡ºæ•°æ®é›†ã€æ¨¡å‹ç»“æ„ä»¥åŠè®­ç»ƒè½®æ•°ç­‰å…³é”®ä¿¡æ¯ï¼Œè¿™æ ·ä¾¿äºåç»­ç®¡ç†å’Œè°ƒè¯•ã€‚æ¯”å¦‚ä½ å¯ä»¥ä¿å­˜ä¸ºï¼š

```python
tiny_imagenet_resnet50_epoch25.pth
```

è¿™é‡Œ "tiny_imagenet" è¡¨ç¤ºæ•°æ®é›†ï¼Œ"resnet50" è¡¨ç¤ºæ¨¡å‹ç»“æ„ï¼Œ"epoch25" åˆ™è¯´æ˜è®­ç»ƒäº† 25 ä¸ªè½®æ¬¡ã€‚



### 4.åŠ è½½è®­ç»ƒå¥½çš„çš„æ¨¡å‹å¹¶è¿›è¡ŒéªŒè¯

å‰é¢å·²ç»ä¿å­˜äº†æ¨¡å‹æƒé‡ï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹æ­¥éª¤åŠ è½½æ¨¡å‹å¹¶åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼š

1. **åŠ è½½æ¨¡å‹ç»“æ„å’Œæƒé‡**  
   è¯·ç¡®ä¿ä½ å®šä¹‰çš„æ¨¡å‹ç»“æ„ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ã€‚ä½¿ç”¨ `torch.load` åŠ è½½æƒé‡ï¼Œå¹¶ç”¨ `model.load_state_dict` å¯¼å…¥ã€‚

2. **åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼**  
   è°ƒç”¨ `model.eval()` ç¡®ä¿æ¨¡å‹å…³é—­ BatchNormã€Dropout ç­‰è®­ç»ƒæ—¶ç‰¹æœ‰çš„è¡Œä¸ºã€‚

3. **éå†éªŒè¯æ•°æ®å¹¶è®¡ç®—å‡†ç¡®ç‡**  
   ä½¿ç”¨ `torch.no_grad()` å…³é—­æ¢¯åº¦è®¡ç®—ï¼ŒåŠ å¿«éªŒè¯é€Ÿåº¦ï¼Œå¹¶é˜²æ­¢å†…å­˜æµªè´¹ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ä»£ç ï¼Œ`eval_tiny_imagenet_resnet50_epoch25_pth.py `ï¼š

```python
import os
import torch
import torch.nn as nn
from torchvision import datasets, models, transforms

# æ•°æ®é¢„å¤„ç†
data_transforms = {
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225])
    ]),
}

# ç¡®ä¿å­è¿›ç¨‹å®‰å…¨å¯åŠ¨
if __name__ == '__main__':
    data_dir = '/Users/hfyan/data/tiny-imagenet-200'
    val_dir = os.path.join(data_dir, 'val')
    val_dataset = datasets.ImageFolder(val_dir, data_transforms['val'])
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64,
                                             shuffle=False, num_workers=4)

    # é€‰æ‹©è®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("Using MPS device for GPU acceleration")
    else:
        device = torch.device("cpu")
        print("MPS device not available, using CPU")

    # åŠ è½½æ¨¡å‹
    model_ft = models.resnet50(pretrained=False)
    num_ftrs = model_ft.fc.in_features
    model_ft.fc = nn.Linear(num_ftrs, len(val_dataset.classes))
    model_ft = model_ft.to(device)

    # åŠ è½½æ¨¡å‹æƒé‡
    model_path = 'tiny_imagenet_resnet50_epoch25.pth'
    model_ft.load_state_dict(torch.load(model_path, map_location=device))

    # è¯„ä¼°æ¨¡å¼
    model_ft.eval()

    # æ¨¡å‹è¯„ä¼°
    running_corrects = 0
    total_samples = 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model_ft(inputs)
            _, preds = torch.max(outputs, 1)
            running_corrects += torch.sum(preds == labels.data)
            total_samples += inputs.size(0)

    val_acc = running_corrects.float() / total_samples
    print('Validation Accuracy: {:.4f}'.format(val_acc))

```

---

**æ€»ç»“**

- **æ¨¡å‹ä¿å­˜æ–‡ä»¶åå»ºè®®**ï¼š  
  ä½¿ç”¨æè¿°æ€§æ–‡ä»¶åï¼Œä¾‹å¦‚ `tiny_imagenet_resnet50_epoch25.pth`ï¼Œè¿™æ ·ä¾¿äºè¯†åˆ«æ•°æ®é›†ã€æ¨¡å‹åŠè®­ç»ƒè½®æ¬¡ã€‚

- **éªŒè¯æ­¥éª¤**ï¼š  
  - åŠ è½½ä¸ä½ è®­ç»ƒæ—¶ä¸€è‡´çš„æ¨¡å‹ç»“æ„ã€‚  
  - ä½¿ç”¨ `model.load_state_dict()` åŠ è½½æƒé‡ã€‚  
  - è°ƒç”¨ `model.eval()` è¿›å…¥éªŒè¯æ¨¡å¼ã€‚  
  - éå†éªŒè¯æ•°æ®é›†ï¼Œè®¡ç®—å‡†ç¡®ç‡æˆ–å…¶ä»–æŒ‡æ ‡ã€‚

è¿™æ ·ï¼Œä½ å°±å¯ä»¥åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹å¹¶å¯¹éªŒè¯é›†æ•°æ®è¿›è¡Œæµ‹è¯•ã€‚

![image-20250224171857564](https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250224171857564.png)



```python
(base) hfyan@HongfeideMac-Studio data % python eval_tiny_imagenet_resnet50_epoch25_pth.py 
Using MPS device for GPU acceleration
/Users/hfyan/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/Users/hfyan/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Validation Accuracy: 0.8014
(base) hfyan@HongfeideMac-Studio data % 
```



### æœ¬éƒ¨åˆ†QA

**Q.resnet50 å›¾åƒåˆ†ç±»ï¼Œç²¾åº¦è¾¾åˆ°å¤šå°‘ï¼Ÿ**

ResNet50 çš„ç²¾åº¦æ ¹æ®ä¸åŒçš„è®­ç»ƒè®¾ç½®å’Œæ•°æ®é›†æœ‰æ‰€å˜åŒ–ã€‚ä»¥ä¸‹æ˜¯å‡ ä¸ªä¸åŒæƒ…å†µä¸‹çš„å‡†ç¡®ç‡ï¼š

1. åŸå§‹çš„ ResNet-50 æ¨¡å‹åœ¨ ImageNet æ•°æ®é›†ä¸Šçš„ Top-1 å‡†ç¡®ç‡å¤§çº¦ä¸º 76.5%ï¼ˆæ ¹æ®åŸå§‹è®ºæ–‡ä¸­çš„ç»“æœï¼‰ã€‚

2. åœ¨ä¸€äº›ä¼˜åŒ–å’Œæ”¹è¿›åï¼Œæ¯”å¦‚ä½¿ç”¨ä¸€ç³»åˆ—æŠ€å·§ï¼ˆtricksï¼‰ï¼Œå¯ä»¥å°† ResNet-50 çš„ Top-1 å‡†ç¡®ç‡æå‡åˆ°å¤§çº¦ 79.1%ã€‚

3. ä½¿ç”¨ SSLDï¼ˆSimple Semi-supervised Label Distillationï¼‰çŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œå¹¶ç»“åˆ ImageNet22k æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä½¿å¾— ResNet-50_vd çš„ Top-1 å‡†ç¡®ç‡è¾¾åˆ° 82.4%ã€‚

4. æ›´è¿›ä¸€æ­¥çš„æ”¹è¿›ï¼Œå¦‚é‡‡ç”¨ç‰¹å®šçš„æ•°æ®å¢å¼ºã€åˆ†è¾¨ç‡è°ƒæ•´ä»¥åŠåŸºäº Fix ç­–ç•¥çš„å¾®è°ƒç­‰æ–¹æ³•ï¼Œå¯ä»¥åœ¨ ImageNet éªŒè¯é›†ä¸Šè¾¾åˆ°æ›´é«˜çš„ç²¾åº¦ã€‚ä¾‹å¦‚ï¼ŒPaddleClas æ–‡æ¡£ä¸­æåˆ°ï¼Œé€šè¿‡è¿™äº›ä¼˜åŒ–æªæ–½ï¼ŒResNet50_vd çš„ç²¾åº¦è¾¾åˆ°äº† 83%ï¼Œè€Œåœ¨ 320x320 å°ºåº¦ä¸‹æµ‹è¯•æ—¶ç²¾åº¦å¯è¾¾ 83.7%ã€‚æœ€ç»ˆï¼Œåœ¨é‡‡ç”¨äº†ç‰¹å®šçš„ Fix ç­–ç•¥ä¹‹åï¼Œç²¾åº¦ç”šè‡³å¯ä»¥è¾¾åˆ° 84.0%ã€‚

5. å¦å¤–ï¼Œæœ‰æ–‡çŒ®æŠ¥é“äº†é€šè¿‡çŸ¥è¯†è’¸é¦ç­‰æ–¹æ³•ï¼Œå¯ä»¥è®© ResNet-50 è¾¾åˆ° 82.8% çš„ Top-1 å‡†ç¡®ç‡ã€‚

è¯·æ³¨æ„ï¼Œä¸Šè¿°æ•°å€¼åæ˜ äº†åœ¨æ ‡å‡† ImageNet æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œå®é™…åº”ç”¨ä¸­é’ˆå¯¹ç‰¹å®šä»»åŠ¡æˆ–æ•°æ®é›†çš„ç²¾åº¦å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚å¦‚æœä½ è¦é’ˆå¯¹è‡ªå·±çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¯èƒ½éœ€è¦æ ¹æ®å…·ä½“æƒ…å†µè°ƒæ•´ç½‘ç»œç»“æ„ã€è®­ç»ƒå‚æ•°æˆ–ä½¿ç”¨è¿ç§»å­¦ä¹ ç­‰æŠ€æœ¯æ¥æé«˜åˆ†ç±»ç²¾åº¦ã€‚



**Q.ImageNetå’ŒTiny ImageNetçš„æ•°æ®è§„æ¨¡**

**ImageNet**  

- **ç±»åˆ«æ•°**ï¼šImageNetçš„å¸¸è§„ç‰ˆæœ¬ï¼ˆå¦‚ILSVRCæŒ‘æˆ˜ä½¿ç”¨çš„æ•°æ®ï¼‰åŒ…å« **1000ä¸ªç±»åˆ«**ã€‚  
- **æ¯ç±»å›¾ç‰‡æ•°**ï¼š  
  - **è®­ç»ƒé›†**ï¼šå¤§çº¦ **1200å¼ å›¾ç‰‡/ç±»**ã€‚  
  - **éªŒè¯é›†**ï¼šæ¯ç±» **50å¼ å›¾ç‰‡**ã€‚  
  - **æµ‹è¯•é›†**ï¼šæ¯ç±» **100å¼ å›¾ç‰‡**ã€‚  
- **æ€»å›¾ç‰‡æ•°**ï¼š  
  - è®­ç»ƒé›†çº¦ **128ä¸‡å¼ å›¾ç‰‡**ã€‚  
  - éªŒè¯é›† **50,000å¼ å›¾ç‰‡**ã€‚  
  - æµ‹è¯•é›† **100,000å¼ å›¾ç‰‡**ã€‚  
- **ç¡¬ç›˜ç©ºé—´**ï¼šåŸå§‹ImageNetçš„å®Œæ•´æ•°æ®é›†å¤§çº¦ **150GB**ã€‚è§£å‹åå¯èƒ½æ›´å¤§ã€‚  

---

**Tiny ImageNet**  

- **ç±»åˆ«æ•°**ï¼š**200ä¸ªç±»åˆ«**ã€‚  
- **æ¯ç±»å›¾ç‰‡æ•°**ï¼š  
  - **è®­ç»ƒé›†**ï¼šæ¯ç±» **500å¼ å›¾ç‰‡**ã€‚  
  - **éªŒè¯é›†**ï¼šæ¯ç±» **50å¼ å›¾ç‰‡**ã€‚  
  - **æµ‹è¯•é›†**ï¼šæ¯ç±» **50å¼ å›¾ç‰‡**ã€‚  
- **æ€»å›¾ç‰‡æ•°**ï¼š  
  - è®­ç»ƒé›† **100,000å¼ å›¾ç‰‡**ã€‚  
  - éªŒè¯é›† **10,000å¼ å›¾ç‰‡**ã€‚  
  - æµ‹è¯•é›† **10,000å¼ å›¾ç‰‡**ã€‚  
- **å›¾ç‰‡å°ºå¯¸**ï¼šæ‰€æœ‰å›¾ç‰‡å‡ä¸º **64x64åƒç´ **ã€‚  
- **ç¡¬ç›˜ç©ºé—´**ï¼šå¤§çº¦ **250MB**ã€‚  

---

**æ€»ç»“**ï¼š

- **ImageNet** æ˜¯å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆ150GB+ï¼‰ï¼Œé€‚åˆè®­ç»ƒæ·±å±‚æ¨¡å‹ã€‚  
- **Tiny ImageNet** æ˜¯ç®€åŒ–ç‰ˆï¼Œ250MBï¼Œéå¸¸é€‚åˆå¿«é€Ÿæµ‹è¯•æ¨¡å‹åŸå‹æˆ–è¿›è¡Œå­¦æœ¯å®éªŒã€‚  



### é™„å½•

#### å¤šè¿›ç¨‹ vs. å¤šçº¿ç¨‹

**å¤šè¿›ç¨‹**ï¼š`multiprocessing` æ¨¡å—å…è®¸ä½ åˆ›å»ºå¤šä¸ªç‹¬ç«‹çš„è¿›ç¨‹ã€‚æ¯ä¸ªè¿›ç¨‹éƒ½æœ‰è‡ªå·±çš„Pythonè§£é‡Šå™¨å®ä¾‹å’Œå†…å­˜ç©ºé—´ï¼Œè¿™æ„å‘³ç€å®ƒä»¬ä¸ä¼šå—åˆ°GILï¼ˆGlobal Interpreter Lockï¼Œå…¨å±€è§£é‡Šå™¨é”ï¼‰çš„å½±å“ï¼Œå¹¶ä¸”å¯ä»¥å……åˆ†åˆ©ç”¨å¤šæ ¸å¤„ç†å™¨çš„èƒ½åŠ›è¿›è¡ŒçœŸæ­£çš„å¹¶è¡Œè®¡ç®—ã€‚ç„¶è€Œï¼Œè¿›ç¨‹é—´çš„é€šä¿¡å’Œæ•°æ®äº¤æ¢ï¼ˆå¦‚å…±äº«çŠ¶æ€æˆ–ä¼ é€’æ¶ˆæ¯ï¼‰ç›¸æ¯”çº¿ç¨‹ä¼šæ›´åŠ å¤æ‚å’Œæ¶ˆè€—èµ„æºã€‚

**å¤šçº¿ç¨‹**ï¼šå¦ä¸€æ–¹é¢ï¼ŒPython çš„ `threading` æ¨¡å—æä¾›äº†å¤šçº¿ç¨‹çš„æ”¯æŒã€‚åœ¨åŒä¸€ä¸ªç¨‹åºå†…ï¼Œä½ å¯ä»¥å¯åŠ¨å¤šä¸ªçº¿ç¨‹ï¼Œè¿™äº›çº¿ç¨‹å…±äº«ç›¸åŒçš„å†…å­˜ç©ºé—´ã€‚å°½ç®¡è¿™ä½¿å¾—çº¿ç¨‹é—´é€šä¿¡æ›´åŠ ç®€å•ç›´æ¥ï¼Œä½†ç”±äºGILçš„å­˜åœ¨ï¼Œå¯¹äºCPUå¯†é›†å‹ä»»åŠ¡ï¼Œå¤šçº¿ç¨‹å¹¶ä¸èƒ½å¸¦æ¥çœŸæ­£çš„å¹¶è¡Œæ‰§è¡Œï¼Œå®ƒæ›´é€‚åˆäºI/Oå¯†é›†å‹çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚ç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶è¯»å†™ç­‰ï¼‰ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œçº¿ç¨‹å¯ä»¥åœ¨ç­‰å¾…I/Oæ“ä½œå®Œæˆçš„åŒæ—¶è®©å‡ºæ‰§è¡Œæƒç»™å…¶ä»–çº¿ç¨‹ã€‚

å› æ­¤ï¼Œå¦‚æœä½ æ­£åœ¨å¤„ç†éœ€è¦å¤§é‡CPUè®¡ç®—çš„ä»»åŠ¡ï¼Œå¹¶å¸Œæœ›åˆ©ç”¨å¤šæ ¸å¤„ç†å™¨æé«˜æ€§èƒ½ï¼Œé‚£ä¹ˆä½¿ç”¨ `multiprocessing` æ¨¡å—æ˜¯æ›´åˆé€‚çš„é€‰æ‹©ã€‚è€Œå¯¹äºæ¶‰åŠå¤§é‡ç­‰å¾…å¤–éƒ¨èµ„æºï¼ˆå¦‚æ•°æ®åº“è®¿é—®ã€ç½‘ç»œè¯·æ±‚ç­‰ï¼‰çš„åº”ç”¨åœºæ™¯ï¼Œ`threading` å¯èƒ½æ›´ä¸ºé€‚ç”¨ã€‚





#### ç¤ºä¾‹18161: çŸ©é˜µè¿ç®—

matrices, http://cs101.openjudge.cn/practice/18161



##### å¤šè¿›ç¨‹multiprocessing

ä½¿ç”¨ multiprocessing æ¨¡å—ä»¥åŠ dot_product å‡½æ•°æ¥å®ç°çŸ©é˜µè¿ç®— AÂ·B + Cã€‚å°†åˆ©ç”¨å¤šè¿›ç¨‹å¹¶è¡Œè®¡ç®—çŸ©é˜µä¹˜æ³•éƒ¨åˆ†ï¼Œç„¶åå°†ç»“æœä¸çŸ©é˜µ C ç›¸åŠ ã€‚

å†…å­˜: 29964ï¼Œæ—¶é—´: 520ms

```python
import multiprocessing

def dot_product(row, col):
    """
    è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯ã€‚
    :param row: ç¬¬ä¸€ä¸ªçŸ©é˜µçš„ä¸€è¡Œ
    :param col: ç¬¬äºŒä¸ªçŸ©é˜µçš„ä¸€åˆ—
    :return: ç‚¹ç§¯ç»“æœ
    """
    return sum(a * b for a, b in zip(row, col))

def matrix_multiply_parallel(A, B, num_processes=None):
    """
    ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œè®¡ç®—ä¸¤ä¸ªçŸ©é˜µçš„ä¹˜ç§¯ã€‚
    :param A: ç¬¬ä¸€ä¸ªçŸ©é˜µï¼Œä½œä¸ºåˆ—è¡¨çš„åˆ—è¡¨
    :param B: ç¬¬äºŒä¸ªçŸ©é˜µï¼Œä½œä¸ºåˆ—è¡¨çš„åˆ—è¡¨
    :param num_processes: è¦ä½¿ç”¨çš„è¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸º None (è‡ªåŠ¨å†³å®š)
    :return: çŸ©é˜µä¹˜ç§¯çš„ç»“æœ
    """
    if len(A[0]) != len(B):
        raise ValueError("Matrix dimensions do not match for multiplication")

    result = [[None for _ in range(len(B[0]))] for _ in range(len(A))]
    
    with multiprocessing.Pool(processes=num_processes) as pool:
        tasks = []
        for i in range(len(A)):
            for j in range(len(B[0])):
                row = A[i]
                col = [B_element[j] for B_element in B]
                # å¼‚æ­¥è°ƒç”¨dot_productå‡½æ•°
                tasks.append(pool.apply_async(dot_product, (row, col)))
        
        index = 0
        for i in range(len(A)):
            for j in range(len(B[0])):
                #ä»»åŠ¡æ”¶é›†é€»è¾‘ï¼šé€šè¿‡ç›´æ¥ä½¿ç”¨ tasks[index].get() æ¥è·å–æ¯ä¸ªå¼‚æ­¥ä»»åŠ¡çš„ç»“æœ
                result[i][j] = tasks[index].get()
                index += 1
    
    return result

def matrix_add(X, Y):
    if len(X) != len(Y) or len(X[0]) != len(Y[0]):
        raise ValueError("Matrices must have the same dimensions for addition")
    return [[X[i][j] + Y[i][j] for j in range(len(X[0]))] for i in range(len(X))]

def read_matrix():
    import sys
    input = sys.stdin.read
    data = input().strip().split('\n')
    matrices = []
    idx = 0
    while idx < len(data):
        row, col = map(int, data[idx].split())
        matrix = []
        for r in range(row):
            matrix.append(list(map(int, data[idx + 1 + r].split())))
        matrices.append(matrix)
        idx += row + 1
    return matrices

def main():
    matrices = read_matrix()
    A, B, C = matrices
    
    try:
        AB = matrix_multiply_parallel(A, B, 4)
        result = matrix_add(AB, C)
        for row in result:
            print(' '.join(map(str, row)))
    except ValueError:
        print("Error!")

if __name__ == "__main__":
    main()
```



##### å•è¿›ç¨‹

å†…å­˜: 4392kBï¼Œæ—¶é—´: 71ms

```python
def read_matrix():
    """è¯»å–çŸ©é˜µè¾“å…¥"""
    row, col = map(int, input().split())
    matrix = [list(map(int, input().split())) for _ in range(row)]
    return matrix

def matrix_multiply(A, B):
    """è®¡ç®—ä¸¤ä¸ªçŸ©é˜µçš„ä¹˜ç§¯"""
    # ç¡®ä¿Açš„åˆ—æ•°ç­‰äºBçš„è¡Œæ•°
    if len(A[0]) != len(B):
        raise ValueError("Matrix dimensions do not match for multiplication")
    
    result = []
    for i in range(len(A)):
        row = []
        for j in range(len(B[0])):
            sum = 0
            for k in range(len(A[0])):
                sum += A[i][k] * B[k][j]
            row.append(sum)
        result.append(row)
    return result

def matrix_add(X, Y):
    """è®¡ç®—ä¸¤ä¸ªçŸ©é˜µçš„å’Œ"""
    # ç¡®ä¿Xå’ŒYçš„ç»´åº¦ç›¸åŒ
    if len(X) != len(Y) or len(X[0]) != len(Y[0]):
        raise ValueError("Matrices must have the same dimensions for addition")
    
    result = [[X[i][j] + Y[i][j] for j in range(len(X[0]))] for i in range(len(X))]
    return result

def main():
    # è¯»å–çŸ©é˜µAã€Bã€C
    A = read_matrix()
    B = read_matrix()
    C = read_matrix()

    try:
        # è®¡ç®—çŸ©é˜µä¹˜æ³•AÂ·B
        AB = matrix_multiply(A, B)

        # æ£€æŸ¥çŸ©é˜µABå’ŒCçš„ç»´åº¦æ˜¯å¦é€‚åˆç›¸åŠ 
        if len(AB) != len(C) or len(AB[0]) != len(C[0]):
            raise ValueError("Resultant matrix AB and C must have the same dimensions for addition")

        # è®¡ç®—çŸ©é˜µåŠ æ³•AB+C
        result = matrix_add(AB, C)

        # è¾“å‡ºç»“æœ
        for row in result:
            print(' '.join(map(str, row)))
    except ValueError as e:
        print("Error!")

if __name__ == "__main__":
    main()
```



> Q. å¼€ multiprocessingï¼Œ4çº¿ç¨‹ï¼Œè¿è¡Œæ—¶é—´æ¯”æ™®é€šæ–¹æ³•è®¡ç®—åå€’æ…¢äº†ä¸€ä¸ªæ•°é‡çº§ã€‚æ˜¯æµ‹è¯•æ•°æ®å¤ªå°‘ï¼Ÿ
>
> ä½¿ç”¨ `multiprocessing` æ¨¡å—è¿›è¡Œå¹¶è¡Œè®¡ç®—æ—¶ï¼Œç¡®å®æœ‰å¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹ï¼ˆç‰¹åˆ«æ˜¯å¤„ç†å°è§„æ¨¡æ•°æ®æˆ–ç®€å•ä»»åŠ¡ï¼‰å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºï¼š
>
> 1. **è¿›ç¨‹å¯åŠ¨å’Œé€šä¿¡å¼€é”€**ï¼šåˆ›å»ºæ–°è¿›ç¨‹ä»¥åŠè·¨è¿›ç¨‹è¾¹ç•Œä¼ é€’æ•°æ®ï¼ˆå¦‚å°†ä»»åŠ¡åˆ†é…ç»™ä¸åŒçš„å·¥ä½œè¿›ç¨‹ï¼‰éœ€è¦æ—¶é—´ã€‚å¦‚æœä»»åŠ¡æœ¬èº«éå¸¸è½»é‡çº§ï¼ˆæ¯”å¦‚ç®€å•çš„çŸ©é˜µä¹˜æ³•å…ƒç´ è®¡ç®—ï¼‰ï¼Œè¿™äº›é¢å¤–çš„å¼€é”€å¯èƒ½ä¼šè¶…è¿‡å¹¶è¡ŒåŒ–å¸¦æ¥çš„æ€§èƒ½å¢ç›Šã€‚
>
> 2. **æ•°æ®è§„æ¨¡ä¸è¶³**ï¼šå¯¹äºè¾ƒå°çš„æ•°æ®é›†æˆ–ç®€å•çš„è®¡ç®—ä»»åŠ¡ï¼ŒCPUå¯èƒ½å¾ˆå¿«å°±èƒ½å®Œæˆè®¡ç®—ï¼Œæ­¤æ—¶å¤šçº¿ç¨‹æˆ–å¤šè¿›ç¨‹çš„ä¼˜åŠ¿æ— æ³•ä½“ç°å‡ºæ¥ã€‚å®é™…ä¸Šï¼Œç”±äºä¸Šè¿°æåˆ°çš„è¿›ç¨‹ç®¡ç†å’Œé€šä¿¡æˆæœ¬ï¼Œåè€Œå¯èƒ½å¯¼è‡´æ€»ä½“æ‰§è¡Œæ—¶é—´å¢åŠ ã€‚
>
> 3. **Pythonçš„å…¨å±€è§£é‡Šå™¨é”ï¼ˆGILï¼‰**ï¼šå°½ç®¡ä½¿ç”¨äº† `multiprocessing` å¯ä»¥ç»•è¿‡ GILï¼Œä½†å¯¹äºI/Oå¯†é›†å‹è€Œéè®¡ç®—å¯†é›†å‹ä»»åŠ¡ï¼Œå…¶ä¼˜åŠ¿å¹¶ä¸æ˜æ˜¾ã€‚æ­¤å¤–ï¼Œåœ¨è®¡ç®—å¯†é›†å‹ä»»åŠ¡ä¸­ï¼Œå¦‚æœä»»åŠ¡ç²’åº¦å¤ªç»†ï¼Œé¢‘ç¹çš„ä¸Šä¸‹æ–‡åˆ‡æ¢ä¹Ÿå¯èƒ½å½±å“æ€§èƒ½ã€‚
>
> **æµ‹è¯•æ•°æ®å¤§å°çš„å½±å“**
>
> å¦‚æœä½ ä½¿ç”¨çš„æµ‹è¯•æ•°æ®å¾ˆå°ï¼ˆä¾‹å¦‚æ ·ä¾‹è¾“å…¥ä¸­çš„3x1ã€1x2ç­‰å°çŸ©é˜µï¼‰ï¼Œé‚£ä¹ˆå¹¶è¡ŒåŒ–çš„å¼€é”€ä¼šæ˜¾å¾—å°¤ä¸ºçªå‡ºã€‚è¿™æ˜¯å› ä¸ºï¼š
>
> - å¹¶è¡ŒåŒ–å¸¦æ¥çš„åŠ é€Ÿæ¯”ä¾èµ–äºä»»åŠ¡å¯ä»¥è¢«æœ‰æ•ˆåˆ†å‰²çš„ç¨‹åº¦ä»¥åŠæ¯ä¸ªå­ä»»åŠ¡çš„è®¡ç®—é‡ã€‚
> - å¯¹äºéå¸¸å°çš„çŸ©é˜µï¼Œå•ä¸ªç‚¹ç§¯æ“ä½œçš„è®¡ç®—é‡ä¸è¶³ä»¥æŠµæ¶ˆè¿›ç¨‹é—´é€šä¿¡å’Œç®¡ç†çš„å¼€é”€ã€‚
>
> **æ”¹è¿›å»ºè®®**
>
> å¦‚æœä½ æƒ³éªŒè¯å¹¶è¡ŒåŒ–æ˜¯å¦èƒ½åœ¨æ›´å¤§è§„æ¨¡çš„æ•°æ®ä¸Šå¸¦æ¥æ€§èƒ½æå‡ï¼Œå»ºè®®å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š
>
> 1. **å¢åŠ æ•°æ®è§„æ¨¡**ï¼šä½¿ç”¨æ›´å¤§çš„çŸ©é˜µè¿›è¡Œæµ‹è¯•ï¼Œè¿™æ ·æ¯ä¸ªå­ä»»åŠ¡ï¼ˆå³ä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯ï¼‰ä¼šæœ‰æ›´å¤šçš„è®¡ç®—é‡ï¼Œä»è€Œæ›´æœ‰å¯èƒ½ä»å¹¶è¡ŒåŒ–ä¸­å—ç›Šã€‚
>
> 2. **è°ƒæ•´çº¿ç¨‹/è¿›ç¨‹æ•°**ï¼šæ ¹æ®ä½ çš„CPUæ ¸å¿ƒæ•°åˆç†è®¾ç½®è¿›ç¨‹æ± çš„å¤§å°ã€‚è¿‡å¤šçš„è¿›ç¨‹ä¼šå¯¼è‡´ä¸å¿…è¦çš„ä¸Šä¸‹æ–‡åˆ‡æ¢ï¼Œè€Œè¿‡å°‘åˆ™ä¸èƒ½å……åˆ†åˆ©ç”¨å¤šæ ¸å¤„ç†å™¨çš„èƒ½åŠ›ã€‚
>
> 3. **ä¼˜åŒ–ä»»åŠ¡ç²’åº¦**ï¼šç¡®ä¿ä½ åˆ’åˆ†çš„ä»»åŠ¡è¶³å¤Ÿå¤§ï¼Œä»¥ä¾¿èƒ½å¤Ÿæ©ç›–è¿›ç¨‹é—´çš„é€šä¿¡å’Œè°ƒåº¦å¼€é”€ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥è€ƒè™‘å°†æ•´ä¸ªè¡Œçš„è®¡ç®—ä½œä¸ºä¸€ä¸ªä»»åŠ¡ï¼Œè€Œä¸æ˜¯å•ç‹¬è®¡ç®—æ¯ä¸ªå…ƒç´ ã€‚
>
> 4. **æµ‹é‡æ€§èƒ½**ï¼šä½¿ç”¨è®¡æ—¶å·¥å…·ï¼ˆå¦‚ `time` æ¨¡å—ï¼‰æ¥ç²¾ç¡®æµ‹é‡ä¸åŒå®ç°æ–¹å¼ä¸‹çš„è¿è¡Œæ—¶é—´ï¼Œå¹¶å¯¹æ¯”åˆ†æç»“æœã€‚
>
> æ€»ä¹‹ï¼Œå¯¹äºå°è§„æ¨¡æ•°æ®æˆ–ç®€å•è®¡ç®—ä»»åŠ¡ï¼Œç›´æ¥ä½¿ç”¨å•çº¿ç¨‹å®ç°é€šå¸¸æ›´ä¸ºé«˜æ•ˆã€‚å½“å¤„ç†å¤§è§„æ¨¡æ•°æ®æˆ–å¤æ‚è®¡ç®—æ—¶ï¼Œé€‚å½“åˆ©ç”¨å¹¶è¡ŒåŒ–æŠ€æœ¯æ‰èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚





#### Torch çš„å¤šè¿›ç¨‹è®¡ç®—ä¸å— GIL é™åˆ¶

https://github.com/GMyhf/2025spring-cs201/blob/main/LLM/nn_4_tiny_imagenet_classify.md

```python
...
    # è®¾ç½® num_workers ä¸º 4 ä»¥åˆ©ç”¨å¤šçº¿ç¨‹æ•°æ®åŠ è½½
    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],
                                                    batch_size=128,    
                                                    shuffle=True,
                                                    num_workers=8)
...
```

GIL çš„å…¨ç§°æ˜¯ **Global Interpreter Lock**ï¼ˆå…¨å±€è§£é‡Šå™¨é”ï¼‰ã€‚è¿™æ˜¯ Python è§£é‡Šå™¨ï¼ˆCPythonï¼‰ä¸­çš„ä¸€ä¸ªæœºåˆ¶ï¼Œç”¨äºç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªçº¿ç¨‹æ‰§è¡Œ Python å­—èŠ‚ç ï¼Œå³ä¾¿åœ¨å¤šæ ¸ CPU ä¸Šï¼Œä¹Ÿæ— æ³•å®ç°çœŸæ­£çš„å¹¶è¡Œè®¡ç®—ã€‚

ä¸è¿‡ï¼Œ**Torch çš„å¤šè¿‘ç¨‹è®¡ç®—** é€šå¸¸ **ä¸å— GIL é™åˆ¶**ï¼ŒåŸå› å¦‚ä¸‹ï¼š

1. **è®¡ç®—å¯†é›†å‹ä»»åŠ¡åœ¨ C/C++ åç«¯æ‰§è¡Œ**  
   PyTorch çš„åº•å±‚è®¡ç®—æ˜¯åŸºäº C++ å’Œ CUDA å®ç°çš„ã€‚ä¸€æ—¦è®¡ç®—ä»»åŠ¡è¿›å…¥åº•å±‚åº“ï¼ˆå¦‚è°ƒç”¨ `torch.matmul()`ã€`torch.nn` ç­‰æ“ä½œï¼‰ï¼Œå®ƒå°±è„±ç¦»äº† Python çš„è§£é‡Šå±‚ï¼Œå› æ­¤ä¸ä¼šè¢« GIL é”ä½ã€‚è®¡ç®—ä¸»è¦åœ¨ C++ å±‚é¢å¹¶è¡Œæ‰§è¡Œï¼Œæˆ–è€…äº¤ç»™ GPU åŠ é€Ÿã€‚

2. **DataLoader ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡ŒåŠ è½½æ•°æ®**  
   ä½ è®¾ç½®çš„ `num_workers=8` å®é™…ä¸Šä¼šå¯åŠ¨ 8 ä¸ª **å­è¿›ç¨‹** æ¥åŠ è½½æ•°æ®ï¼Œå› ä¸º PyTorch çš„ `DataLoader` é»˜è®¤ä½¿ç”¨ **multiprocessing** åº“ï¼Œè€Œä¸æ˜¯çº¿ç¨‹æ± ã€‚è¿™ç§æ–¹å¼æ˜¯è·¨è¿›ç¨‹çš„ï¼Œä¸ä¼šè§¦å‘ GILã€‚

3. **å¼‚æ­¥è®¡ç®—ï¼ˆCUDAï¼‰**  
   å¦‚æœä½ åœ¨ä½¿ç”¨ GPU è®¡ç®—ï¼ˆå¦‚ `.to('cuda')`ï¼‰ï¼Œå¤§éƒ¨åˆ†æ“ä½œæ˜¯å¼‚æ­¥çš„ã€‚Python çº¿ç¨‹è§¦å‘è®¡ç®—ä»»åŠ¡åï¼ŒCUDA å†…æ ¸åœ¨ GPU ä¸Šæ‰§è¡Œï¼Œçº¿ç¨‹ä¼šç«‹åˆ»è¿”å›ï¼Œç»§ç»­å¤„ç†å…¶ä»–ä»»åŠ¡ã€‚å› æ­¤ä¸ä¼šå¡åœ¨ GIL ä¸Šã€‚

**æ€»ç»“ï¼š**

- **CPU-boundä»»åŠ¡**ï¼ˆçº¯ Python ä»£ç è®¡ç®—ï¼‰ä¼šå—åˆ° GIL é™åˆ¶ã€‚
- **I/O-boundä»»åŠ¡**ï¼ˆå¦‚æ•°æ®åŠ è½½ã€æ–‡ä»¶è¯»å–ï¼‰åœ¨ PyTorch ä¸­ä¸»è¦é å¤šè¿›ç¨‹ï¼Œä¸å— GIL å½±å“ã€‚
- **GPU-boundä»»åŠ¡** å®Œå…¨åœ¨ CUDA ä¸Šæ‰§è¡Œï¼Œä¸ GIL æ— å…³ã€‚

æ‰€ä»¥ `num_workers=8` æ˜¯åœ¨å¼€ 8 ä¸ªè¿›ç¨‹æ¥å¹¶è¡ŒåŠ è½½æ•°æ®ï¼Œè®¡ç®—éƒ¨åˆ†èµ°çš„åº•å±‚ C++/CUDAï¼Œä¸ä¼šå› ä¸º GIL äº§ç”Ÿæ€§èƒ½ç“¶é¢ˆï¼





# ç¬¬ä¸‰éƒ¨åˆ† æ·±åº¦å­¦ä¹ ä¸å¤§è¯­è¨€æ¨¡å‹

é˜…è¯»ã€Šä»é›¶æ„å»ºå¤§æ¨¡å‹ (Sebastian Raschka, å¡å·´æ–¯è’‚å®‰Â·æ‹‰æ–½å¡, è¦ƒç«‹æ³¢, å†¯æ™“éª‹, åˆ˜ä¹¾)ã€‹ã€‚

ä»£ç å¯ä»¥æœ¬åœ°æˆ–è€…äº‘ç«¯è¿è¡Œï¼Œhttps://github.com/rasbt/LLMs-from-scratch

ç›®å‰2025/7/21ï¼Œgithubè¯¥é¡¹ç›®è·59.5kæ˜Ÿã€‚https://github.com/rasbt/LLMs-from-scratch

ç¬”è¯•å†…å®¹ã€ŠTest_Yourself_On_Build_a_Large_Languageã€‹



## 3.1 æ·±åº¦å­¦ä¹ åŸºç¡€

- æ•°å­—è§’åº¦çœ‹ä¸–ç•Œï¼šä¸€åˆ‡çš†æ•°å­—ï¼Œä¸€åˆ‡æ™ºèƒ½çš†**å‡½æ•°**
  - å¦‚ä½•æ•°å€¼åŒ–ï¼šæ–‡å­— â†’ è¯è¡¨ç¼–ç ï¼Œå›¾ç‰‡ â†’ åƒç´  RGBï¼Œè§†é¢‘ â†’ è¿ç»­çš„å¸§å›¾ç‰‡...
  - å›¾åƒæ£€æµ‹ï¼šè¾“å…¥ä¸€å¼ å›¾ç‰‡ï¼Œè¾“å‡ºå…¶ä¸­æ˜¯å¦åŒ…å«æŸä¸ªç‰©ä½“ã€ç‰©ä½“çš„è¾¹ç¼˜è½®å»“...
  - æ–‡å­—ã€å›¾åƒç”Ÿæˆï¼šè¾“å…¥ä¸€æ®µæ–‡å­—ï¼Œè¾“å‡ºè¿™æ®µæ–‡å­—çš„ç»­å†™/å›ç­”/æ–‡å­—æè¿°çš„å›¾ç‰‡...
- æµç¨‹ï¼šè¾“å…¥ï¼ˆæ–‡/å›¾/è§†é¢‘ï¼‰â†’ ç¼–ç ä¸ºæ•°å€¼åŒ–è¡¨ç¤º â†’ å‡½æ•°è¿›è¡Œä¸€ç³»åˆ—è¿ç®— â†’ å¾—åˆ°æ•°å€¼è¾“å‡º â†’ è§£ç ä¸ºæ‰€éœ€æ ¼å¼
- **æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ ï¼ˆMachine Leaning/Deep Learningï¼‰**ï¼šä½¿ç”¨ï¼ˆæ·±å±‚ç¥ç»ç½‘ç»œï¼‰æ¨¡å‹æ¥å®ç°è¿™äº›å‡½æ•°
- æ—©æœŸçš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼šçº¿æ€§åˆ†ç±»å™¨
  - å…¬å¼ï¼š$y=wx+b$
  - å¯»æ‰¾åˆ’åˆ†ä¸åŒç±»åˆ«æ•°æ®ç‚¹çš„æœ€ä½³è¶…å¹³é¢ï¼ˆ$w, b$ å‚æ•°å–å€¼ï¼‰
  - èƒ½è§£å†³ç®€å•çš„åˆ†ç±»ä»»åŠ¡

- æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼šç¥ç»ç½‘ç»œï¼ˆNeural Networkï¼‰
  - ä¸€ä¸ªäººå·¥å®šä¹‰çš„è®¡ç®—å›¾ï¼Œæ¯ä¸ªèŠ‚ç‚¹è®¾å®š $w, b$ å‚æ•°ï¼ŒèŠ‚ç‚¹è¾“å…¥ä¸è¿™äº›å‚æ•°åšä¹˜æ³•ï¼ŒåŠ æ³•å’Œéçº¿æ€§è¿ç®—å¾—åˆ°è¾“å‡ºï¼ˆå…¬å¼ï¼š$Activation(wx+ b)$ï¼‰ï¼ŒæŒ‰å›¾ç»“æ„å‘å‰ä¼ é€’ç›´è‡³å¾—åˆ°æœ€ç»ˆè¾“å‡ºã€‚
  - ä¸ç”Ÿç‰©å­¦ä¸Šå¤§è„‘çš„ç¥ç»å…ƒè¿æ¥ç»“æ„ç›¸ä¼¼
  - èŠ‚ç‚¹æ›´å¤šã€å±‚æ•°æ›´æ·±ï¼ˆå‚æ•°é‡æ›´å¤§ï¼‰çš„ç¥ç»ç½‘ç»œèƒ½å¤Ÿæ‹Ÿåˆæ›´å¤æ‚çš„å‡½æ•°ï¼ˆè¡¨è¾¾èƒ½åŠ›æ›´å¼ºï¼‰
  - æ•°å­¦ä¸Šå¯è¯æ˜ç¥ç»ç½‘ç»œå¯ä»¥æ‹Ÿåˆä»»ä½•å‡½æ•°
    - https://www.zhihu.com/question/268384579
- æ‹Ÿåˆçš„æ–¹æ³•ï¼šè®­ç»ƒï¼ˆTrainingï¼‰
  - ç”¨å¤§é‡è¦æ‹Ÿåˆçš„å‡½æ•°äº§ç”Ÿçš„è¾“å…¥è¾“å‡ºæ•°æ®ï¼Œä½¿ç”¨ä¼˜åŒ–ç®—æ³•ï¼Œé€æ­¥è°ƒæ•´æ¨¡å‹çš„å‚æ•°ï¼Œç›´è‡³å¯¹ä»»æ„è¾“å…¥èƒ½å¾—åˆ°è¶³å¤Ÿæ¥è¿‘çœŸå®è¾“å‡ºçš„ç»“æœ
  - æŸå¤±å‡½æ•°ï¼šç”¨äºè®¡ç®—æ¨¡å‹è¾“å‡ºä¸çœŸå®è¾“å‡ºçš„å·®è·
  - ä¼˜åŒ–ç®—æ³•ï¼šéšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆSGDï¼‰ï¼ŒAdamsç­‰
  - è®­ç»ƒä¸è¶³/è¿‡åº¦æ—¶ä¼šå‡ºç°æ¨¡å‹æ¬ æ‹Ÿåˆ/è¿‡æ‹Ÿåˆ
  - è¯„æµ‹ï¼šåœ¨ä¸€ç»„æ²¡æœ‰ç”¨æ¥è®­ç»ƒçš„æ•°æ®ä¸Šï¼Œæµ‹è¯•æ¨¡å‹è¾“å‡ºçš„å‡†ç¡®æ€§

## 3.2 ä» Transformer åˆ°ç”Ÿæˆå¼å¤§æ¨¡å‹

- è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processing, NLPï¼‰ï¼šè®¡ç®—æœºç§‘å­¦ä¸­é‡è¦çš„ç ”ç©¶é¢†åŸŸ
  - å¤šç§ä»»åŠ¡ï¼šæƒ…æ„Ÿåˆ†ç±»ï¼Œæœºå™¨ç¿»è¯‘ï¼Œæ–‡æœ¬æ‘˜è¦ï¼Œ...
  - ä¸¤å¤§ç±»ï¼šåˆ¤åˆ«å¼ å’Œ ç”Ÿæˆå¼
- 2017 å¹´ï¼ŒTransformer æ¨¡å‹ç»“æ„è¢«æå‡º
  - Encoder-Decoder ç»“æ„ï¼Œå¯ç‹¬ç«‹/ç»„åˆä½¿ç”¨
  - Encoderï¼štoken åºåˆ—ï¼ˆæ–‡æœ¬ï¼‰è¿›ï¼Œå‘é‡å‡ºï¼Œé€‚åˆåˆ¤åˆ«å¼ä»»åŠ¡ï¼Œä»£è¡¨å·¥ä½œï¼šBERTï¼ˆ2018ï¼‰
  - Decoderï¼štoken åºåˆ—ï¼ˆæ–‡æœ¬ï¼‰è¿›ï¼Œæ¯æ¬¡å‡ºä¸€ä¸ª tokenï¼Œå’Œä¹‹å‰åºåˆ—æ‹¼æ¥ç®—ä¸‹ä¸€ä¸ªtokenï¼Œå¾ªç¯ç›´è‡³ç”Ÿæˆäº†ç‰¹æ®Šçš„ <eos> token ç»“æŸï¼Œé€‚åˆç”Ÿæˆå¼ä»»åŠ¡ï¼Œä»£è¡¨å·¥ä½œï¼šGPTï¼ˆ2018ï¼‰

- è‡ªç„¶è¯­è¨€å¯ä»¥è¡¨ç¤ºä¸ºä¸€ç§**æ¦‚ç‡åˆ†å¸ƒ**ï¼š
  - $P(<bos>åŒ—äº¬<eos>)=P(åŒ—â”‚<bos>)âˆ—P(äº¬â”‚<bos>åŒ—)âˆ—P(<eos>|<bos>åŒ—äº¬)$â€‹
  - è¯å…ƒï¼ˆTokenï¼‰ï¼šè‡ªç„¶è¯­è¨€æŒ‰ è¯/å­è¯/å­— ç²’åº¦åˆ‡åˆ†åçš„å•å…ƒ
  - ä¸€åˆ‡æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ˆæ— è®ºæ˜¯åˆ›ä½œè¯—æ­Œã€è§£å†³æ•°å­¦é¢˜ã€å†™ä»£ç ...ï¼‰éƒ½æ˜¯ **Next Token Prediction** ä»»åŠ¡
  - Transformer Decoder å°±æ˜¯ä¸ºäº†è§£å†³è¿™ç§ä»»åŠ¡çš„æ¨¡å‹
- è¯­è¨€æ¨¡å‹çš„è®­ç»ƒ
  - 2018 ä¹‹å‰ï¼šé’ˆå¯¹æ¯ä¸ªä»»åŠ¡ï¼Œæ„é€ æœ‰ç›‘ç£ï¼ˆSupervisedï¼‰æ•°æ®é›†ï¼Œä»é›¶ä¸“é—¨è®­ç»ƒä¸€ä¸ªæ¨¡å‹
    - ç”Ÿæˆæ¨¡å‹å¸Œæœ›æœ‰å¤šä»»åŠ¡èƒ½åŠ›
  - 2018 å¹´ï¼ŒBERT å’Œ GPT è®ºæ–‡æå‡º Pre-Training + Fine-Tuning æ¨¡å¼ï¼š
  - **é¢„è®­ç»ƒ**ï¼šå…ˆåœ¨å¤§è§„æ¨¡çš„æ— ç›‘ç£è¯­æ–™ä¸Šè®­ç»ƒæ¨¡å‹çš„é€šç”¨è¯­è¨€å»ºæ¨¡èƒ½åŠ›
    - æ–¹æ³•ï¼šä»äº’è”ç½‘çˆ¬å–æµ·é‡æ–‡æœ¬ï¼Œæ¯ä¸ªå¥å­æŒ–ç©ºå¡«è¯ or ç»™å‰åŠå¥é¢„æµ‹ååŠå¥
  - **å¾®è°ƒ**ï¼šç„¶ååœ¨ç‰¹å®šä»»åŠ¡çš„ç›‘ç£æ•°æ®é›†ä¸Šç»§ç»­è®­ç»ƒæ¨¡å‹ï¼Œæå‡æ¨¡å‹åœ¨ä»»åŠ¡ä¸Šçš„æ€§èƒ½
  - åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„åˆ†æ•°å¤§å¹…è¶…è¿‡ SOTAï¼ˆstate-of-the-artï¼‰ç»“æœ

- â€œå¤§â€æ¨¡å‹çš„ä½“ç°
  - å‚æ•°é‡ï¼šBERT (2018): 334Mï¼ˆ3.34äº¿ï¼‰ï¼ŒGPT-3 (2020): 175Bï¼ˆ1750äº¿ï¼‰, DeepSeek-V3 (2025): 671Bï¼ˆ6710äº¿ï¼‰
  - é¢„è®­ç»ƒæ•°æ®é‡ï¼šBERT (2018)ï¼š3.7B tokensï¼ŒGPT-3 (2020)ï¼š500B tokensï¼ŒLlama (2023)ï¼š1.4T tokensï¼ŒDeepSeek-V3 (2025): 14.8T tokens
  - ä» 2018 è‡³ä»Šï¼Œæ¨¡å‹çš„å‚æ•°è§„æ¨¡å’Œè®­ç»ƒæ•°æ®é‡å¢é•¿äº†æ•°åƒå€
- ä¸ºä»€ä¹ˆéœ€è¦å¤§æ¨¡å‹
  - éšç€å‚æ•°é‡å’Œè®­ç»ƒæ•°æ®é‡çš„å¢å¤§ï¼Œæ¨¡å‹åœ¨è¯­è¨€èƒ½åŠ›ã€çŸ¥è¯†è®°å¿†ç­‰æ–¹é¢çš„è¡¨ç°æ²¡æœ‰å‡ºç°åœæ»æˆ–å€’é€€ï¼Œè€Œæ˜¯ä¸æ–­æœ‰æå‡
  - æ¶Œç°èƒ½åŠ›ï¼ˆEmergent Abilitiesï¼‰ï¼šåœ¨æ¨¡å‹å°ºå¯¸è¶³å¤Ÿå¤§/è®­ç»ƒæ•°æ®é‡è¶³å¤Ÿå¤šæ—¶ï¼ŒæŸäº›æ–¹é¢çš„èƒ½åŠ›æœ‰çªç„¶æå‡

- è®¡ç®—ç¡¬ä»¶ï¼šGPU
  - æœ€åˆä¸ºå›¾å½¢è®¡ç®—è®¾è®¡ï¼Œæ‹¥æœ‰å¼ºå¤§çŸ©é˜µç®—åŠ›ï¼Œä¸ç¥ç»çš„ç½‘ç»œè®¡ç®—äº’é€š
  - è‹±ä¼Ÿè¾¾ï¼ˆNVIDIAï¼‰ï¼šè¡Œä¸šé¾™å¤´ï¼Œæœ€å¼ºç¡¬ä»¶æ€§èƒ½ + æ²‰æ·€å¤šå¹´çš„ CUDA ç¼–ç¨‹ç”Ÿæ€
    - RTX ç³»åˆ—ï¼šæ¶ˆè´¹çº§GPU
    - A100/H100 ç³»åˆ—ï¼šä¸“ä¸šGPU
  - Intel Gaudiï¼Œåä¸ºæ˜‡è…¾ç­‰äº§å“æ­£åœ¨è¿½èµ¶

- å¤§æ¨¡å‹çš„â€œé‡åŒ–â€
  - å¤§éƒ¨åˆ†æ¨¡å‹çš„å‚æ•°æ˜¯ float16/bfloat16 æ ¼å¼ï¼Œæ¯ä¸ªå‚æ•° 2 å­—èŠ‚ï¼ˆ16 bitï¼‰
    - DeepSeek-V3/R1 671B æ»¡è¡€ç‰ˆé™¤å¤–ï¼Œçº¯ fp8 è®­ç»ƒ

  - æ¨¡å‹æ¨ç†éœ€è¦çš„æ˜¾å­˜çº¦ä¸º 2.5x~3x å‚æ•°é‡
    - ä¾‹ï¼š1.5B æ¨¡å‹ â‰ˆ 3.7G - 4.5Gï¼Œ7B æ¨¡å‹ â‰ˆ 17.5 - 21G

  - æ¨¡å‹é‡åŒ–ï¼ˆQuantizationï¼‰ï¼šå°†å‚æ•°è¿‘ä¼¼å­˜å‚¨ä¸ºæ›´ä½å­—èŠ‚
    - å°å¹…ç‰ºç‰²æ•ˆæœï¼Œå¤§å¹…èŠ‚çœæ˜¾å­˜
    - INT4 = 4 bit é‡åŒ–ï¼ŒINT8 = 8 bit é‡åŒ–
    - é‡åŒ–åæ¨¡å‹éœ€è¦çš„æ˜¾å­˜ç›¸åº”é™¤å­—èŠ‚å‹ç¼©å€æ•°

- å½“å‰çš„ä¸»æµå¤§æ¨¡å‹ï¼ˆ2025.02ï¼‰
  - å•†ä¸šåŒ–æ¨¡å‹
    - OpenAIï¼šChatGPTï¼ŒGPT-4oï¼ŒGPT-o1/o3
    - Googleï¼šGemini 2.0 Pro
    - Anthropicï¼šClaude 3.5 Sonnet
    - ç™¾åº¦ï¼šæ–‡å¿ƒä¸€è¨€4.0ï¼›
    - æœˆä¹‹æš—é¢ï¼šKimi v1, k1.5
  - å¼€æºæ¨¡å‹
    - Meta AI (Facebook)ï¼šLLaMA-3, 3.1, 3.2, 3.3
    - é˜¿é‡Œï¼šé€šä¹‰åƒé—® Qwen-2.5, 2.5 Coder
    - æ·±åº¦æ±‚ç´¢ï¼šDeepSeek V3/R1, Distill-Qwen2.5, Distill-Llama3
  - ç«äº‰ç‚¹ï¼šè¯„æµ‹æŒ‡æ ‡ï¼ˆæ•°å­¦ï¼Œä»£ç ç­‰èƒ½åŠ›ï¼‰ï¼Œé•¿ä¸Šä¸‹æ–‡ï¼Œå¤šæ¨¡æ€ï¼Œé•¿æ€è€ƒï¼Œä»·æ ¼

## 3.3 å¤§æ¨¡å‹æœ¬åœ°éƒ¨ç½²ã€API è°ƒç”¨

- æœ¬åœ°éƒ¨ç½²å¤§æ¨¡å‹
  - LM Studioï¼šå›¾å½¢åŒ–ç•Œé¢ï¼Œä¸€é”®ä¸‹è½½è¿è¡Œæ¨¡å‹
  - Ollama (+ Open WebUI)
    - æ³¨æ„ tagï¼Œollama é»˜è®¤ä¸‹è½½çš„æ˜¯é‡åŒ–æ¨¡å‹
  - HuggingFaceï¼šæœ€æµè¡Œçš„æ·±åº¦å­¦ä¹ å¼€æºç¤¾åŒºï¼ŒAI åœˆçš„ Github

- é€šè¿‡ API è°ƒç”¨å¤§æ¨¡å‹
  - ä¸ºä»€ä¹ˆéœ€è¦ APIï¼šæ‰¹é‡è°ƒç”¨ï¼ŒåŸºäºå¤§æ¨¡å‹è°ƒç”¨æ„å»ºä¸Šå±‚åº”ç”¨ç­‰
  - OpenAI API è§„èŒƒï¼šåœ¨ ChatGPT çˆ†ç«åè¿…é€Ÿæˆä¸ºå¤§æ¨¡å‹è°ƒç”¨çš„äº‹å®æ ‡å‡†ï¼Œæ‰€æœ‰äº‘å¹³å°/æœ¬åœ°éƒ¨ç½²å·¥å…·éƒ½ä¼šæä¾› OpenAI API æ ¼å¼çš„è°ƒç”¨æœåŠ¡
  - å½“å‰ä¸»æµæ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„è°ƒç”¨å‚è€ƒ Chat Completion æ¥å£çš„è¾“å…¥è¾“å‡ºå³å¯
  - ä¸¤ç§å…·ä½“è°ƒç”¨æ–¹æ³•ï¼š
    - ä½¿ç”¨ OpenAI åº“ï¼Œclient.chat.completions å¯¹è±¡
    - æœ¬è´¨æ˜¯ HTTP æ¥å£ï¼Œé€šè¿‡ requests åº“æˆ– curl è‡ªè¡Œç»„è£… json ç›´æ¥å‘è¯·æ±‚ä¹Ÿå¯ä»¥
  - æ–‡æ¡£ï¼šhttps://platform.openai.com/docs/api-reference/chat/create